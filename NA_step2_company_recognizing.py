# ä¾èµ– / Dependencies / ä¾å­˜é–¢ä¿‚ï¼špython-docx, pandas, openpyxl, unicodedata, tqdm, spacy, fuzzywuzzy, python-Levenshtein
# å®‰è£… / Install / ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼špip install python-docx pandas openpyxl tqdm spacy fuzzywuzzy python-Levenshtein
# æ¨¡å‹ä¸‹è½½ / Model Download / ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼špython -m spacy download en_core_web_sm
# åŸä½œè€…ï¼šæ¨å¤©ä¹@å…³è¥¿å¤§å­¦ / Author: Shiame Yeung@Kansai University / ä½œæˆè€…ï¼šæ¥Šã€€å¤©æ¥½@é–¢è¥¿å¤§å­¦


import os
import re
import time
import pandas as pd
import spacy
from tqdm import tqdm
from fuzzywuzzy import process

# â€”â€” 1. SpaCy NER åªä¿ç•™å¿…è¦ç»„ä»¶ï¼ŒåŠ å¿«é€Ÿåº¦ â€”â€”
nlp = spacy.load(
    "en_core_web_sm",
    disable=["tagger", "parser", "attribute_ruler", "lemmatizer"]
)

# â€”â€” 2. é»˜è®¤å…¬å¸åˆ—è¡¨ï¼Œç”¨äºåˆå§‹åŒ– NA_company_list.csv â€”â€”
DEFAULT_COMPANIES = [
    "IBM","Microsoft","Apple","Google","Amazon","Meta","Facebook",
    "Oracle","SAP","Cisco","Intel","AMD","Nvidia","Dell","HP",
    "Lenovo","Huawei","Samsung","Sony","Panasonic","Fujitsu",
    "Tencent","Alibaba","Baidu","Xiaomi","Didi","Bytedance",
    "Uber","Airbnb","Salesforce","Adobe","Twitter","LinkedIn",
    "Zoom","Slack","Spotify","Netflix","eBay","PayPal",
    "Square","Stripe","OpenAI","TSMC","Qualcomm","LG","SK Hynix"
]

# â€”â€” å·¥å…·ï¼šç”Ÿæˆå”¯ä¸€æ–‡ä»¶å â€”â€”
def get_unique_path(path):
    base, ext = os.path.splitext(path)
    counter = 1
    candidate = path
    while os.path.exists(candidate):
        candidate = f"{base}_{counter}{ext}"
        counter += 1
    return candidate

# â€”â€” 3. åˆæ³• Token åˆ¤æ–­ â€”â€”
def is_valid_token(token):
    token = token.strip()
    if not token or all(c in "-â€“â€”ãƒ».ã€ã€‚ï¼ï¼Ÿï¼ãƒ¼" for c in token):
        return False
    # å«æ•°å­—ä½†ä¸å«è‹±æ–‡ â†’ è·³è¿‡
    if re.search(r"\d", token) and not re.search(r"[A-Za-z]", token):
        return False
    # è¿ç»­ä¸¤ä¸ªç©ºæ ¼ â†’ è·³è¿‡
    if "  " in token:
        return False
    return True

# â€”â€” 4. åŸå§‹ä¼ä¸šåæå– â€”â€”
def extract_companies(text, company_db, ner_model, fuzzy_threshold=95):
    comps = set()
    # å»æ‰æ—¥æœŸåŠå…¶åå†…å®¹
    text_clean = re.sub(r"\s*\d{1,2}/\d{1,2}/\d{2,4}.*$", "", text).strip()

    # spaCy NER æå–
    doc = ner_model(text_clean)
    for ent in doc.ents:
        ent_text = ent.text.strip()
        # è·³è¿‡å™ªéŸ³
        if "  " in ent_text or re.search(r"[\d/%+]|[^\x00-\x7F]", ent_text):
            continue
        valid_ent = True
        for w in ent_text.split():
            if not w[0].isalpha() or w in {"The","And","For","With","From","That","This"}:
                valid_ent = False
                break
            if not is_valid_token(w):
                valid_ent = False
                break
        if valid_ent:
            comps.add(ent_text)

    # IBMers ç±»è§„åˆ™
    for m in re.findall(r"\b([A-Z]{2,})ers\b", text_clean):
        comps.add(m)

    # ä¸¥æ ¼æ¨¡ç³ŠåŒ¹é…è¡¥å……
    STOPWORDS = {"The","And","For","With","From","That","This","Have","Will",
                 "Are","You","Not","But","All","Any","One","Our","Their"}
    tokens = re.findall(r"\b\S+\b", text_clean)
    for pos, token in enumerate(tokens):
        if pos == 0 or token in STOPWORDS:
            continue
        if any(ch in token for ch in "/%+") or "  " in token:
            continue
        if len(token) < 5 or not token[0].isupper() or token.isupper():
            continue
        if re.search(r"\d|[^\x00-\x7F]", token) or not is_valid_token(token):
            continue
        best, score = process.extractOne(token, company_db)
        if score >= fuzzy_threshold:
            comps.add(best)

    return list(comps)

# â€”â€” 5. åˆå§‹åŒ–æˆ–åŠ è½½æ˜ å°„è¡¨ â€”â€”
def prepare_mapping_tables(script_dir):
    comp_list_path = os.path.join(script_dir, "NA_company_list.csv")
    if not os.path.exists(comp_list_path):
        pd.DataFrame({
            "Standard_Name": DEFAULT_COMPANIES,
            "Aliases": ["" for _ in DEFAULT_COMPANIES]
        }).to_csv(comp_list_path, index=False)

    comp_df = pd.read_csv(comp_list_path)
    # ç¡®ä¿ Aliases åˆ—ä¸ºå­—ç¬¦ä¸²
    comp_df['Aliases'] = comp_df['Aliases'].fillna('').astype(str)

    # è®¡ç®— banned åˆ—è¡¨
    banned = set(
        comp_df.loc[
            comp_df['Aliases'].str.strip().str.lower() == 'banned',
            'Standard_Name'
        ]
    )

    # æ„å»ºæ ‡å‡†åä¸åˆ«åæ˜ å°„
    std_set = set()
    alias_map = {}
    for _, r in comp_df.iterrows():
        std = r['Standard_Name'].strip()
        if std in banned:
            continue
        std_set.add(std)
        for a in [x.strip() for x in r['Aliases'].split(',') if x.strip() and x.strip().lower() != 'banned']:
            alias_map[a] = std

    # å§‹ç»ˆæ–°å»º mapping
    map_path = os.path.join(script_dir, "NA_mapping.csv")
    if os.path.exists(map_path):
        backup = get_unique_path(map_path)
        os.rename(map_path, backup)
    # æ–°å¢ Sentence åˆ—ï¼Œå¹¶æ”¾åœ¨æœ€å‰
    mapping_df = pd.DataFrame(columns=["Sentence", "NonStandard", "Standard"])
    mapping_df.to_csv(map_path, index=False)

    return std_set, alias_map, mapping_df, comp_list_path, map_path, banned

# â€”â€” 6. ä¸»æµç¨‹ â€”â€”
def process_all_files():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    std_set, alias_map, mapping_df, comp_list_path, map_path, banned = prepare_mapping_tables(script_dir)

    # unmatched ä» set æ”¹ä¸º dictï¼Œç”¨äºè®°å½•é¦–æ¬¡å‡ºç°çš„å¥å­
    unmatched = {}

    # ç­›é€‰åŸå§‹æ–‡ä»¶
    all_files = [
        f for f in os.listdir(script_dir)
        if (f.lower().endswith('.xlsx') or f.lower().endswith('.csv'))
        and f.lower() not in ('na_company_list.csv','na_mapping.csv')
        and not f.lower().endswith(('_recognized.csv','_log.csv'))
    ]

    for fname in tqdm(all_files, desc="ğŸ“ æ–‡ä»¶æ€»è¿›åº¦", unit="æ–‡ä»¶"):
        t0 = time.time()
        in_path = os.path.join(script_dir, fname)
        base = os.path.splitext(fname)[0] + "_recognized"
        out_csv = get_unique_path(os.path.join(script_dir, base + ".csv"))
        log_csv = get_unique_path(os.path.join(script_dir, base + "_log.csv"))

        print(f"> å¤„ç†æ–‡ä»¶ï¼š{fname}")
        if fname.lower().endswith('.xlsx'):
            df = pd.read_excel(in_path, engine='openpyxl')
        else:
            df = pd.read_csv(in_path)

        if not all(c in df.columns for c in ['Hit_Count','Sentence','Matched_Keywords']):
            print("  âš ï¸ ç¼ºå°‘å…³é”®åˆ—ï¼Œè·³è¿‡ã€‚")
            continue

        hit_df = df[df['Hit_Count']>0].reset_index(drop=True)
        print(f"  - å…± {len(hit_df)} æ¡ç›®æ ‡å¥å­ï¼Œå¼€å§‹è¯†åˆ«â€¦")
        t1 = time.time()

        cache, extracted, logs = {}, {}, []
        for idx, row in tqdm(hit_df.iterrows(), total=len(hit_df), desc="  ğŸ” å¤„ç†ä¸­", unit="è¡Œ"):
            sent = str(row['Sentence'])
            raw = cache.get(sent) or extract_companies(sent, list(std_set)+list(alias_map.keys()), nlp)
            cache[sent] = raw

            std_list = []
            for nm in raw:
                if nm in banned:
                    continue
                if nm in std_set:
                    std_list.append(nm)
                elif nm in alias_map:
                    std_list.append(alias_map[nm])
                else:
                    # è®°å½•é¦–æ¬¡å‡ºç°è¯¥éæ ‡å‡†åæ—¶å¯¹åº”çš„åŸæ–‡å¥å­
                    if nm not in unmatched:
                        unmatched[nm] = sent
                    std_list.append(nm)

            if std_list:
                extracted[idx] = std_list
                logs.append({
                    'Index': idx,
                    'Sentence': sent,
                    'Extracted_Companies': ', '.join(std_list)
                })

        t2 = time.time()

        if not extracted:
            print("  â„¹ï¸ æ— è¯†åˆ«ç»“æœï¼Œè·³è¿‡å†™å…¥ã€‚")
            continue

        # æ„é€  Company_n åˆ—å¹¶ä¿å­˜
        max_n = max(len(v) for v in extracted.values())
        comp_cols = {f'Company_{i+1}':['']*len(df) for i in range(max_n)}
        for idx, names in extracted.items():
            for i, nm in enumerate(names):
                comp_cols[f'Company_{i+1}'][idx] = nm

        out_df = pd.concat([df, pd.DataFrame(comp_cols)], axis=1)
        print("  ğŸ’¾ ä¿å­˜ç»“æœå’Œæ—¥å¿—â€¦")
        out_df.to_csv(out_csv, index=False)
        pd.DataFrame(logs).to_csv(log_csv, index=False)
        print(f"  âœ… å¯¼å‡ºï¼š{out_csv}")
        print(f"  ğŸ“„ æ—¥å¿—ï¼š{log_csv}")

        t3 = time.time()
        print(f"  â± æ€»è€—æ—¶: {t3-t0:.1f}sï¼ŒæŠ½å–è€—æ—¶: {t2-t1:.1f}s")

    # æ›´æ–° mapping
    existing = set(mapping_df['NonStandard'])
    new = [nm for nm in sorted(unmatched) if nm not in existing]
    if new:
        add_df = pd.DataFrame({
            'Sentence': [unmatched[nm] for nm in new],
            'NonStandard': new,
            'Standard': [''] * len(new)
        })[
            ["Sentence", "NonStandard", "Standard"]
        ]
        mapping_df = pd.concat([mapping_df, add_df], ignore_index=True)
        mapping_df.to_csv(map_path, index=False)
        print(f"ğŸ”„ æ›´æ–°æ˜ å°„æ–‡ä»¶ï¼š{map_path}ï¼ˆæ–°å¢ {len(new)} æ¡å¾…äººå·¥æ ‡æ³¨ï¼‰")
    else:
        print("â„¹ï¸ æ— æ–°å¢å¾…æ ‡æ³¨ä¼ä¸šåã€‚")

if __name__ == "__main__":
    process_all_files()

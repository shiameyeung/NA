# åŸä½œè€…ï¼šæ¨å¤©ä¹@å…³è¥¿å¤§å­¦ / Author: Shiame Yeung@Kansai University / ä½œæˆè€…ï¼šæ¥Šã€€å¤©æ¥½@é–¢è¥¿å¤§å­¦
#!/usr/bin/env python3
# coding: utf-8
"""
na_pipeline.py  â€”â€”  å•æ–‡ä»¶ç‰ˆï¼ˆStepâ€‘1 å¯¹é½ + æ‰©å±•å…¬å¸è¯†åˆ«ï¼‰
2025â€‘07â€‘08  revâ€‘C
"""

def cute_box(cn: str, jp: str, icon: str = "ğŸŒ¸") -> None:
    """
    å¤šè¡Œä¹Ÿèƒ½å¯¹é½çš„å¯çˆ±ä¸­/æ—¥åŒè¯­æ¡†
    cn: ä¸­æ–‡æç¤ºï¼ˆå¯ä»¥å¤šè¡Œï¼Œç”¨ '\\n' åˆ†éš”ï¼‰
    jp: æ—¥æ–‡æç¤ºï¼ˆå¯ä»¥å¤šè¡Œï¼‰
    icon: æ¯è¡Œå¼€å¤´å’Œç»“å°¾çš„å°è¡¨æƒ…
    """
    # æŠŠä¸­/æ—¥å„è‡ªçš„å¤šè¡Œæ‹†å¼€ï¼Œæ‹¼æˆç»Ÿä¸€åˆ—è¡¨
    lines = []
    for segment in (cn, jp):
        for ln in segment.split("\n"):
            ln = ln.strip()
            # ç”¨ "icon + ç©ºæ ¼ + æ–‡æœ¬ + ç©ºæ ¼ + icon" æ„é€ æ¯ä¸€è¡Œ
            lines.append(f"{icon} {ln} {icon}")

    # æ‰¾åˆ°æœ€é•¿é‚£è¡Œï¼Œåšä¸ºæ¡†å®½
    width = max(len(ln) for ln in lines)
    border = "â”€" * width

    # æ‰“å°ä¸Šè¾¹æ¡†
    print(f"â•­{border}â•®")
    # æ‰“å°æ¯ä¸€è¡Œï¼Œå³ä¾§å¡«å……ç©ºæ ¼åˆ° width
    for ln in lines:
        print("â”‚" + ln.ljust(width) + "â”‚")
    # æ‰“å°ä¸‹è¾¹æ¡†
    print(f"â•°{border}â•¯")

import sys, subprocess, os

def ensure_env() -> None:
    """
    â¶ åˆ¤æ–­ Python ç‰ˆæœ¬ï¼Œç»™å‡ºâ€œè€ä¾èµ– / æ–°ä¾èµ–â€ä¸¤å¥—æ¸…å•  
    â· å…ˆå‡çº§ pip / setuptools / wheelï¼Œå†ç¡®ä¿ packaging ä¸ requests å­˜åœ¨  
    â¸ å®‰è£…æˆ–å‡çº§å…¶ä½™ä¾èµ–ï¼›ç¼ºä»€ä¹ˆè‡ªåŠ¨è¡¥ä»€ä¹ˆ  
    â¹ è‹¥æœ¬è½®ç¡®å®å®‰è£…è¿‡ä¸œè¥¿ï¼Œåˆ™ cute_box æç¤ºå sys.exit(0)ï¼Œ
       è®©ç”¨æˆ·é‡æ–°æ‰§è¡Œä¸»è„šæœ¬ï¼›å¦åˆ™ç›´æ¥è¿”å›ç»§ç»­è·‘
    """
    import sys, subprocess

    # ---------- 0. å°å·¥å…· ----------
    def pip_install(pkgs: list[str]):
        """ç»Ÿä¸€ pip å®‰è£…å…¥å£ï¼ˆå¸¦ -U å‡çº§ï¼‰"""
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", "-U", *pkgs],
            stdout=subprocess.DEVNULL  # ä¿æŒè¾“å‡ºç®€æ´ï¼Œå¯æŒ‰éœ€å»æ‰
        )

    # ---------- 1. å…ˆå‡çº§ pip / setuptools / wheel ----------
    cute_box(
        "æ­£åœ¨å‡çº§ pip / setuptools / wheel â€¦",
        "pip / setuptools / wheel ã‚’ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ä¸­â€¦",
        "ğŸ”§"
    )
    pip_install(["pip", "setuptools", "wheel"])

    # ---------- 2. ç¡®ä¿ packaging & requests å­˜åœ¨ ----------
    for base_pkg, jp_name in [("packaging", "packaging"), ("requests", "requests")]:
        try:
            __import__(base_pkg)
        except ImportError:
            cute_box(
                f"ç¼ºå°‘ {base_pkg}ï¼Œæ­£åœ¨å®‰è£…â€¦",
                f"{jp_name} ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­â€¦",
                "ğŸ“¦"
            )
            pip_install([base_pkg])

    # ä¹‹åè¦ç”¨ packaging é‡Œçš„ç‰ˆæœ¬æ¯”è¾ƒ
    from importlib.metadata import version, PackageNotFoundError
    from packaging.specifiers import SpecifierSet
    from packaging.requirements import Requirement

    # ---------- 3. æ ¹æ® Python ç‰ˆæœ¬å†³å®šä¾èµ– ----------
    py_major, py_minor = sys.version_info[:2]
    is_new_py = (py_major, py_minor) >= (3, 13)

    if is_new_py:
        core_pkgs = ["numpy>=2.0.0", "spacy>=3.8.7", "thinc>=8.3.6", "blis>=1.0.0"]
        torch_spec = "torch==2.6.0"          # PyTorch ç›®å‰å¯¹ 3.13 ä»…æ­¤ç‰ˆæœ¬
    else:
        core_pkgs = ["numpy<2.0.0", "spacy<3.8.0", "thinc<8.3.0", "blis<0.8.0"]
        torch_spec = "torch>=2,<2.3"

    common_pkgs = [
        "pandas", "tqdm", "sqlalchemy", "pymysql",
        "rapidfuzz", "python-docx", "sentence-transformers",
        torch_spec,
    ]

    wanted = core_pkgs + common_pkgs

    # ---------- 4. åˆ¤æ–­å“ªäº›åŒ…éœ€è¦å®‰è£… / å‡çº§ ----------
    def need_install(spec: str) -> bool:
        req = Requirement(spec)
        try:
            cur_ver = version(req.name)
        except PackageNotFoundError:
            return True
        return cur_ver not in SpecifierSet(str(req.specifier))

    to_install = [spec for spec in wanted if need_install(spec)]

    did_install = False
    if to_install:
        cute_box(
            "å®‰è£… / å‡çº§ä»¥ä¸‹ä¾èµ–ï¼š\n" + "\n".join(to_install),
            "æ¬¡ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« / ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã—ã¾ã™ï¼š\n" + "\n".join(to_install),
            "ğŸ“¦"
        )
        try:
            pip_install(to_install)
            did_install = True
        except subprocess.CalledProcessError:
            cute_box(
                "âŒ è‡ªåŠ¨å®‰è£…å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ ¹æ®æ—¥å¿—è§£å†³ä¾èµ–åé‡è¯•",
                "âŒ è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¤±æ•—ã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã€æ‰‹å‹•ã§ä¾å­˜é–¢ä¿‚ã‚’è§£æ±ºã—ã¦ãã ã•ã„",
                "âš ï¸"
            )
            sys.exit(1)

    # ---------- 5. ç¡®ä¿ spaCy è‹±æ–‡æ¨¡å‹ ----------
    try:
        import spacy
        spacy.load("en_core_web_sm")
    except (ImportError, OSError):
        cute_box(
            "ä¸‹è½½ spaCy æ¨¡å‹ en_core_web_sm â€¦",
            "spaCy ãƒ¢ãƒ‡ãƒ« en_core_web_sm ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­â€¦",
            "ğŸ”„"
        )
        subprocess.check_call([sys.executable, "-m", "spacy", "download", "en_core_web_sm"])
        did_install = True

    # ---------- 6. ç»“æŸè¯­ ----------
    cute_box(
        "ä¾èµ–æ£€æŸ¥å®Œæ¯•ï¼Œè„šæœ¬å¯ä»¥è¿è¡Œï¼",
        "ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯å®Œäº†ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã§ãã¾ã™ï¼",
        "ğŸ‰"
    )
    if did_install:
        cute_box(
            "é¦–æ¬¡/åˆšå‡çº§å®Œï¼Œè¯·é‡æ–°è¿è¡Œä¸»è„šæœ¬ã€‚",
            "åˆå›å®Ÿè¡Œã¾ãŸã¯ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ç›´å¾Œã§ã™ã€‚ã‚‚ã†ä¸€åº¦ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚",
            "ğŸ”"
        )
        sys.exit(0)

# â€”â€”â€”â€”â€”â€” åœ¨è„šæœ¬ä¸€å¯åŠ¨å°±å…ˆç¡®ä¿ç¯å¢ƒ â€”â€”â€”â€”â€”â€”
ensure_env()

import os, re, sys, unicodedata, string
from pathlib import Path
from typing import List, Dict, Set

from datetime import datetime
import random

import itertools

import pandas as pd
from tqdm import tqdm
from sqlalchemy import create_engine, text
from rapidfuzz import fuzz, process
import numpy as np
import torch
from sentence_transformers import SentenceTransformer

try:
    from docx import Document
    import spacy
    nlp = spacy.load("en_core_web_sm", disable=["parser", "lemmatizer"])
    model_emb = SentenceTransformer(
        "sentence-transformers/all-MiniLM-L6-v2",
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
except Exception:
    cute_box(
      "ç¼ºå°‘ä¾èµ–ï¼šè¯·è¿è¡Œ pip install python-docx spacy",
      "ä¾å­˜é–¢ä¿‚ãŒè¶³ã‚Šã¾ã›ã‚“ï¼špip install python-docx spacy ã‚’å®Ÿè¡Œã—ã¦ã­",
      "âš ï¸"
    )
    sys.exit(1)

# ---------------- å¸¸é‡ ----------------
STOPWORDS = {"the","and","for","with","from","that","this","have","will","are","you","not","but","all","any","one","our","their"}
KEYWORD_ROOTS = [
    'partner','alliance','collaborat','cooper','cooperat','join','merger','acquisiti',
    'outsourc','invest','licens','integrat','coordinat','synergiz','associat',
    'confedera','federa','union','unit','amalgamat','conglomerat','combin',
    'buyout','companion','concur','concert','comply','complement','assist',
    'takeover','accession','procure','suppl','conjoint','support','adjust',
    'adjunct','patronag','subsid','affiliat','endors'
]
# ---------------- Bad-Rate è§„åˆ™ ----------------
ORG_SUFFIX  = re.compile(
    r'\b(Inc\.?|Corp\.?|Corporation|Ltd\.?|LLC|PLC|AG|NV|SA|GmbH|S\.p\.A|Co\.?|Company|'
    r'Group|Holdings?|Partners?|Capital|Ventures?|Bank|Trust|Software|'
    r'Technolog(?:y|ies)|Pharma(?:ceuticals)?|Systems?|Services?|'
    r'Industr(?:y|ies)|Foundation|Laborator(?:y|ies)|'
    r'University|College|Institute|School|Hospital|Center|Centre)\b',
    re.I)

TIME_QTY    = re.compile(
    r'\b(year|month|week|day|decade|centur(?:y|ies)|quarter|q[1-4]|'
    r'ago|last|next|few|couple|several|dozen|half|around|approximately)s?\b',
    re.I)

# â”€â”€ é‡‘èæŠ¥è¡¨ / ä¸šç»©å…¬å‘Šç±» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FIN_REPORT = re.compile(
    r'\b(results?|earnings?|revenues?|turnover|profit(?:s)?|loss(?:es)?|guidance|forecast|'
    r'financial statements?|balance sheets?|cash flows?|income statements?)\b',
    re.I)

# â”€â”€ åˆ†å­£ / åˆ†åŠæœŸ / åˆ†å¹´æè¿° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ORDINAL_PERIOD = re.compile(
    r'\b(first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\b.*?\b(quarter|half|year)\b',
    re.I)

# â”€â”€ å…¸å‹â€œå…¬å‘Š/æŠ¥å‘Š/æ›´æ–°â€è§¦å‘è¯ï¼ˆå¤šè§äº ban_listï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ANNOUNCE_VERB = re.compile(
    r'\b(reports?|announces?|updates?|revises?|publishes?|files?|issues?|unveils?)\b',
    re.I)
# ===== åœ¨å¸¸é‡åŒºï¼ˆTIME_QTY ä¹‹åï¼‰æ–°å¢å‡ æ¡é€šç”¨ regex =====
GENERIC_NOUN = re.compile(
    r'\b(services?|solutions?|systems?|platforms?|programs?|projects?|'
    r'statements?|reports?|targets?|technologies?|operations?|activities|'
    r'strategies?|plans?)\b', re.I)

MONTH_NAME = re.compile(
    r'\b(jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|'
    r'jul(?:y)?|aug(?:ust)?|sep(?:t(?:ember)?)?|oct(?:ober)?|nov(?:ember)?|'
    r'dec(?:ember)?)\b', re.I)

NEW_GENERIC_TIME = re.compile(
    r'\b(?:end|beginning|middle|start|first|second|third|fourth|prior|previous|'
    r'current|next)\s+(?:of\s+)?(?:the\s+)?(?:year|quarter|month|week)s?\b',
    re.I)

#  çº¯å¤§å†™ 2â€“4 ä½ç¼©å†™ï¼ˆPBM / ESG â€¦ï¼‰
ALLCAP_SHORT = re.compile(r'^[A-Z]{2,4}$')

#  %ã€ç™¾ä¸‡/åäº¿ã€ç¾å…ƒç¬¦å·ä¹‹ç±»
NUMERIC = re.compile(r'[%\$]\s*\d|\d[\d,\.]+\s*(?:million|billion|thousand)', re.I)


ALL_UPPER  = re.compile(r'^[A-Z]{2,}$')
ALL_LOWER  = re.compile(r'^[a-z]{4,}$')

SHORT_TOKEN = re.compile(r'^[A-Za-z]{1,4}$')
ART_LOWER   = re.compile(r'^\s*(a|an|about|approximately|the|this|that|those)\s+[a-z]')
GENERIC_END = re.compile(
    r'\b(plan|plans?|programs?|systems?|platforms?|services?|solutions?|operations?|'
    r'agreements?|strategies?|reports?|statements?)$', re.I)

def _lower_ratio(text: str) -> float:
    w = text.split()
    return sum(t[0].islower() for t in w) / len(w) if w else 0

def calc_Bad_Score(text: str) -> int:
    """
    è¶Šé«˜è¶Šå¯èƒ½æ˜¯ badï¼ˆéœ€è¦äººå·¥åˆ¤æ–·æˆ–ç›´æ¥ banï¼‰
    è°ƒæ•´ç‚¹ï¼š
      â‘  å…ˆæŒ‰â€œå¥½ç‰¹å¾â€æ¸…é›¶â€”â€”ä¾‹å¦‚åˆæ³•å…¬å¸åç¼€ã€‚
    """
    # === â‘  æ˜æ˜¾å¥½ç‰¹å¾ï¼šç›´æ¥åˆ¤ 0 ===
    if ORG_SUFFIX.search(text):           # Inc., Ltd. ç­‰
        return 0

    score = 0

    # === â‘¡ æ—¶é—´ & æ•°é‡ç±» ===
    if TIME_QTY.search(text) or MONTH_NAME.search(text):
        score += 40                       # å­£åº¦/æœˆä»½/å¹´ï¼Œå‡ ä¹ä¸€å®šæ˜¯å‡å…¬å¸

    # === â‘¢ å¥å¼ & ç»„åˆè¯ ===
    if ' of the ' in text.lower():        # â€œâ€¦of theâ€¦â€ å…¸å‹æŠ¥å‘Šè¯­
        score += 20
    if GENERIC_END.search(text):
        score += 15
    if GENERIC_NOUN.search(text):         # æ–°å¢ï¼šæ³›ç§°åè¯
        score += 15

    # === â‘£ å¤§å°å†™ & é•¿åº¦ ===
    words = text.split()
    if len(words) <= 2:
        score += 20
    if _lower_ratio(text) > 0.30:
        score += 15                       # åŸæ¥æ˜¯ 20ï¼Œç¨å¾®æ”¾å®½
    if ALL_UPPER.match(text) or ALL_LOWER.match(text):
        score += 15
    if any(SHORT_TOKEN.match(w) for w in words):
        score += 10                       # åƒâ€œLLCâ€â€œLPâ€è¿™ç§å¾ˆçŸ­çš„ token
        
    if FIN_REPORT.search(text):        score += 30        
    if ORDINAL_PERIOD.search(text):    score += 25        
    if ANNOUNCE_VERB.search(text):     score += 20    
    if NEW_GENERIC_TIME.search(text):      score += 40  # time ç›¸å…³æ›´ç‹ 
    if ALLCAP_SHORT.match(text):           score += 50  # çº¯ç¼©å†™
    if NUMERIC.search(text):               score += 25  # å«æ•°å€¼/é‡‘é¢    


    return score
# ---------------- å…¨å±€å˜é‡ ----------------
BASE_DIR = Path(__file__).resolve().parent
MAX_COMP_COLS = 50
SENTENCE_RECORDS: List[Dict] = []

# ---------------- å…±ç”¨ ----------------



# ------- æ–°ç‰ˆæœ¬ï¼šé¦–æ¬¡è¾“å…¥åå†™ .db_keyï¼Œåç»­è‡ªåŠ¨è¯»å– -------
def ask_mysql_url() -> str:
    key_file = Path(__file__).with_name(".db_key")   # è„šæœ¬åŒç›®å½• .db_key
    if key_file.exists():
        key = key_file.read_text().strip()
    else:
        key = input("è¯·è¾“å…¥ç§˜é’¥/ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼šuser:pass@host\n>>>>>> ").strip()
        key_file.write_text(key)                     # ç¼“å­˜ä¸‹æ¬¡ç”¨
    return f"mysql+pymysql://{key}.mysql.rds.aliyuncs.com:3306/na_data?charset=utf8mb4"

def choose() -> str:
    # â”€â”€ 1. é€‰é¡¹æ¡† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    cute_box(
        "â‘  åˆæ¬¡è¿è¡Œï¼ˆStep-1 âœ Step-2ï¼‰\nâ‘¡ mappingé€‚ç”¨/é‚»æ¥ï¼ˆStep-3/4ï¼‰\nä½œè€…ï¼šæ¨ å¤©ä¹ æ”¯æŒï¼šæ å®—æ˜Š æ ä½³ç’‡ @é–¢è¥¿å¤§å­¦ã€€ä¼Šä½ç”°ç ”ç©¶å®¤",
        "â‘  åˆå›å®Ÿè¡Œï¼ˆStep-1 âœ Step-2ï¼‰\nâ‘¡ mappingé©ç”¨/éš£æ¥ï¼ˆStep-3/4ï¼‰\nä½œæˆè€…ï¼šæ¥Š å¤©æ¥½ã€€å”åŠ›ï¼šæ å®—æ˜Š æ ä½³ç’‡ @é–¢è¥¿å¤§å­¦ã€€ä¼Šä½ç”°ç ”ç©¶å®¤",
        "ğŸ“‹"
    )
    c = input("è¯·è¾“å…¥ 1 æˆ– 2 / 1 ã‹ 2 ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()

    # â”€â”€ 2. æ ¡éªŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if c not in {"1", "2"}:
        cute_box(
        "æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1 æˆ– 2ï¼",
        "ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚1 ã‹ 2 ã‚’å…¥åŠ›ã—ã¦ã­ï¼",
        "ğŸ”„"
        )
        sys.exit(1)

    return c

def dedup_company_cols(df: pd.DataFrame) -> pd.DataFrame:
    comp_cols = [c for c in df.columns if c.startswith("company_")]
    for ridx in df.index:
        seen: Set[str] = set()
        for col in comp_cols:
            val = str(df.at[ridx, col]).strip()
            if val in seen:
                df.at[ridx, col] = ""
            else:
                seen.add(val)
    return df

# ---------------- Stepâ€‘1 ----------------

def _normalize(text: str) -> str:
    t = re.sub(r"\s+", " ", text.lower().strip())
    t = re.sub(r"^[\-:\"']+|[\-:\"']+$", "", t)
    t = re.sub(r"[,.;/()]+", "", t)
    return t.strip()

def clean_text(t: str) -> str:
    return ''.join(c for c in t if unicodedata.category(c)[0] != 'C' or c in ('\n', '\t'))

def extract_sentences(path: Path) -> List[str]:
    doc = Document(path)
    collecting, current, articles = False, "", []
    for p in doc.paragraphs:
        txt = p.text.strip()
        if not txt: continue
        tag = txt.lower()
        if tag == "body": collecting, current = True, ""; continue
        if tag in ("notes", "classification") and collecting:
            collecting = False; articles.append(current.strip()); continue
        if collecting: current += " " + txt
    sents = []
    for art in articles:
        for s in re.split(r"\.\s*", art):
            s = s.strip();
            if len(s) >= 5: sents.append(s)
    return sents

def extract_index_titles(paragraphs):
    paras_text = [p.text.strip() for p in paragraphs]
    m = re.search(r'Documents?\s*\(\s*(\d+)\s*\)', '\n'.join(paras_text), re.I)
    if not m: return []
    total = int(m.group(1)); pat = re.compile(r'^(\d+)\.\s+(.*)$'); seen, titles = set(), []
    for line in paras_text:
        m2 = pat.match(line)
        if m2:
            raw = m2.group(2).strip(); norm = _normalize(raw)
            if norm in seen: continue
            seen.add(norm); titles.append((int(m2.group(1)), raw, norm))
            if len(titles) >= total: break
    return sorted(titles, key=lambda x: x[0])

def extract_sentences_by_titles(filepath: str) -> List[Dict]:
    doc = Document(filepath); paras = doc.paragraphs
    index_titles = extract_index_titles(paras); recs = []
    if index_titles:
        paras_norm = [_normalize(p.text) for p in paras]
        for _, title_raw, title_norm in index_titles:
            try: match_idx = next(i for i, n in enumerate(paras_norm) if n == title_norm)
            except StopIteration: continue
            pub_idx = match_idx + 1; publisher = paras[pub_idx].text.strip() if pub_idx < len(paras) else ""
            body_start = next((i+1 for i in range(match_idx+1,len(paras)) if paras[i].text.strip().lower()=="body"), None)
            if body_start is None: body_start = pub_idx + 1
            body_end = next((i for i in range(body_start, len(paras)) if paras[i].text.strip().lower() in ("notes","classification")), len(paras))
            article = " ".join(clean_text(paras[i].text) for i in range(body_start, body_end))
            for sent in [s.strip() for s in re.split(r"\.\s*", article) if len(s.strip())>=5]:
                hits = [k for k in KEYWORD_ROOTS if k in sent.lower()]
                recs.append({"Title":title_raw,"Publisher":publisher,"Country":"","Sentence":sent,"Hit_Count":len(hits),"Matched_Keywords":"; ".join(hits)})
        if recs: return recs
    # æ— ç´¢å¼•
    for sent in extract_sentences(Path(filepath)):
        hits=[k for k in KEYWORD_ROOTS if k in sent.lower()]
        recs.append({"Title":"","Publisher":"","Country":"","Sentence":sent,"Hit_Count":len(hits),"Matched_Keywords":"; ".join(hits)})
    return recs

def step1():
    cute_box(
        "Step-1ï¼šæå– Word å¥å­ ä¸­â€¦",
        "Step-1ï¼šæ–‡æŠ½å‡ºä¸­â€¦",
        "ğŸ“„"
    )
    all_recs: List[Dict] = []

    # 1) æ”¶é›†æ‰€æœ‰ .docx è·¯å¾„
    docx_files = []
    for root, _, files in os.walk(BASE_DIR):
        for fname in files:
            if not fname.endswith(".docx") or fname.startswith("~$"):
                continue
            full = Path(root) / fname
            rel = full.relative_to(BASE_DIR).parts
            tier1 = rel[0] if len(rel) >= 1 else ""
            tier2 = rel[1] if len(rel) >= 2 else ""
            docx_files.append((str(full), tier1, tier2, fname))

    # 2) é€æ–‡ä»¶æå–å¥å­
    for fp, t1, t2, fname in tqdm(docx_files, desc="ğŸ“„ å¤„ç† Word æ–‡ä»¶"):
        for r in extract_sentences_by_titles(fp):
            if not r["Title"]:
                r["Title"] = Path(fname).stem
            r.update({"Tier_1": t1, "Tier_2": t2, "Filename": fname})
            all_recs.append(r)

    global SENTENCE_RECORDS
    SENTENCE_RECORDS = all_recs
    cute_box(
        f"Step-1 å®Œæˆï¼Œå…± {len(all_recs)} æ¡è®°å½•",
        f"Step-1 å®Œäº†ã—ã¾ã—ãŸï¼šå…¨{len(all_recs)}ä»¶",
        "âœ…"
    )

# ----------------â€”â€” Stepâ€‘2 â€”â€”----------------

def is_valid_token(token: str) -> bool:
    token = token.strip()
    if "@" in token or token.startswith("http"):    # â‘  å«é‚®ç®± / URL ç‰¹å¾
        return False
    if not token or all(c in "-â€“â€”ãƒ».ã€ã€‚ï¼ï¼Ÿï¼ãƒ¼" for c in token):
        return False
    if re.search(r"\d", token) and not re.search(r"[A-Za-z]", token):
        return False
    if "  " in token:
        return False
    return True


# â€”â€” 4. åŸå§‹ä¼ä¸šåæå– â€”â€”
# â€”â€” 4. åŸå§‹ä¼ä¸šåæå– â€”â€”
def extract_companies(text: str,
                      company_db: List[str],
                      ner_model,
                      fuzzy_threshold: int = 95) -> List[str]:
    """
    Â· **ä»…è´Ÿè´£â€œæŠŠå¥å­é‡Œå¯èƒ½æ˜¯å…¬å¸åçš„ç‰‡æ®µå…¨éƒ¨æŠ“å‡ºæ¥â€**ï¼Œ
      ä¸åšä»»ä½• ban/æ˜ å°„/å»é‡å¤„ç†â€”â€”è¿™äº›ç•™ç»™åç»­æ•°æ®åº“æ¯”å¯¹é˜¶æ®µå®Œæˆã€‚
    Â· è¯†åˆ«é€»è¾‘å®Œå…¨æ²¿ç”¨å•ä½“ç‰ˆï¼ˆspaCy NER + â€œIBMersâ€æ­£åˆ™ + ä¸¥æ ¼æ¨¡ç³ŠåŒ¹é…ï¼‰ã€‚
    """
    comps: Set[str] = set()

    # 1) å»æ‰æ—¥æœŸï¼ˆæ’é™¤ â€˜xx/xx/xxxx ä¹‹åæ•´æ®µâ€™ çš„å™ªéŸ³ï¼‰
    text_clean = re.sub(r"\s*\d{1,2}/\d{1,2}/\d{2,4}.*$", "", text).strip()
    # --- æ–°å¢æ¸…æ´— ---
    # 1) å»æ‰ Â® â„¢ Â©
    text_clean = re.sub(r"[Â®â„¢Â©]", "", text_clean)
    # 2) å»æ‰ç®€å†™å•†æ ‡æ‹¬å·ï¼Œå¦‚ â€œWeight Doctors(R)â€
    text_clean = re.sub(r"\(\s*[A-Z]{1,3}\s*\)", "", text_clean)
    # 3) æ•´å¥é‡Œå¸¦é‚®ç®±çš„ç›´æ¥å‰ªæ‰é‚®ç®±
    text_clean = re.sub(r"\b\S+@\S+\b", "", text_clean)

    # 2) spaCy NER
    doc = ner_model(text_clean)
    for ent in doc.ents:
        ent_text = ent.text.strip()

        # â€”â€” åŸºç¡€å™ªéŸ³è¿‡æ»¤ï¼ˆå’Œå•ä½“ç‰ˆä¸€è‡´ï¼‰
        if "  " in ent_text or re.search(r"[\d/%+]|[^\x00-\x7F]", ent_text):
            continue
        valid_ent = True
        for w in ent_text.split():
            if (not w[0].isalpha()
                or w in {"The","And","For","With","From","That","This"}
                or not is_valid_token(w)):
                valid_ent = False
                break
        if valid_ent:
            comps.add(ent_text)

    # 3) â€œIBMersâ€ ä¸€ç±»å†™æ³•
    for m in re.findall(r"\b([A-Z]{2,})ers\b", text_clean):
        comps.add(m)

    # 4) ä»…ç”¨äºâ€œç¡®è®¤æ˜¯å·²çŸ¥å…¬å¸â€ï¼Œä½†ä¾æ—§è¿”å›åŸè¯
    STOPWORDS = {"The","And","For","With","From","That","This","Have","Will",
                "Are","You","Not","But","All","Any","One","Our","Their"}

    tokens = re.findall(r"\b\S+\b", text_clean)
    for pos, token in enumerate(tokens):
        # â€”â€” å™ªéŸ³ä¸æ ¼å¼è¿‡æ»¤ï¼ˆåŒåŸå…ˆé€»è¾‘ï¼‰ â€”â€”
        if (pos == 0 or token in STOPWORDS
            or any(ch in token for ch in "/%+") or "  " in token
            or len(token) < 5 or not token[0].isupper() or token.isupper()
            or re.search(r"\d|[^\x00-\x7F]", token)
            or not is_valid_token(token)):
            continue

        # è‹¥æ•°æ®åº“é‡Œå­˜åœ¨â€œå®Œå…¨åŒåï¼ˆå¤§å°å†™ä¸åŒè§†åŒï¼‰â€çš„æ¡ç›®ï¼Œå°±ä¿ç•™ï¼›å¦åˆ™å¿½ç•¥
        if any(token.lower() == db.lower() for db in company_db):
            comps.add(token)

    return list(comps)


def step2(mysql_url: str):
    cute_box(
        "Step-2ï¼šå…¬å¸è¯†åˆ«ï¼‹BAN è¿‡æ»¤ ä¸­â€¦",
        "Step-2ï¼šä¼æ¥­åèªè­˜ï¼‹BAN ãƒ•ã‚£ãƒ«ã‚¿ä¸­â€¦",
        "ğŸ·ï¸"
    )
    # å•ç‹¬å¯¼å‡º canonical è¡¨ï¼ˆengine_tmpï¼‰
    engine_tmp = create_engine(mysql_url)            # â† æ–°å»º
    df_canon = pd.read_sql("SELECT id, canonical_name FROM company_canonical", engine_tmp)
    df_canon.to_csv(BASE_DIR / "canonical_list.csv", index=False, encoding="utf-8-sig")
    cute_box(
        f"å·²å†™å‡º canonical_list.csvï¼Œå…± {len(df_canon)} è¡Œ",
        f"canonical_list.csv ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼š{len(df_canon)} è¡Œ",
        "ğŸ—‚ï¸"
    )
    # ---- è¿æ¥æ•°æ®åº“ ----
    engine = create_engine(mysql_url)
    with engine.begin() as conn:
        ban_set = {r[0] for r in conn.execute(text("SELECT alias FROM ban_list"))}
        rows = conn.execute(text("""
            SELECT a.alias, c.canonical_name FROM company_alias a
            JOIN company_canonical c ON a.canonical_id = c.id
        """))
        alias_map = {alias: canon for alias, canon in rows}
        canon_set = {r[0] for r in conn.execute(text("SELECT canonical_name FROM company_canonical"))}
        # â€”â€” é¢„ç¼–ç å…¨éƒ¨ canonicalï¼Œä¸€æ¬¡æå®š â€”â€”
        canon_names = list(canon_set)
        canon_vecs  = model_emb.encode(canon_names, batch_size=64, normalize_embeddings=True)
        # â†“â†“â†“ æ–°å¢ï¼šåå­—â†’ID çš„å­—å…¸ï¼Œç”¨äº Advice å¯¹åº”çš„ ID
        rows2 = conn.execute(text(
            "SELECT id, canonical_name FROM company_canonical"
        ))
        canon_name2id = {name: cid for cid, name in rows2}      # â† æ–°å¢
    
    cute_box(
    f"ban_list={len(ban_set)}ï¼Œalias_map={len(alias_map)}ï¼Œcanon_set={len(canon_set)}",
    f"ban_listï¼š{len(ban_set)}ä»¶ï¼alias_mapï¼š{len(alias_map)}ä»¶ï¼canon_setï¼š{len(canon_set)}ä»¶",
    "ğŸ”"
    )

    df = pd.DataFrame(SENTENCE_RECORDS)
    df_hit = df[df["Hit_Count"].astype(int) >= 1].reset_index(drop=True)
    if df_hit.empty:
        cute_box(
        "Step-1 æ²¡æå–åˆ°ä»»ä½•å¥å­ï¼Œè¯·å…ˆè·‘ Step-1ï¼",
        "Step-1 ã§æ–‡ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã¾ãš Step-1 ã‚’å®Ÿè¡Œã—ã¦ã­",
        "ğŸš«"
        )
        return

    company_db = list(canon_set) + list(alias_map.keys())   # canonical + alias
    comp_cols: List[List[str]] = []
    for sent in tqdm(df_hit["Sentence"].tolist(), desc="å…¬å¸è¯†åˆ«"):
        names_raw = extract_companies(sent, company_db, nlp)
        uniq: List[str] = []
        for alias in names_raw:
            if alias in uniq:     
                continue
            uniq.append(alias)                         # ä¿ç•™å¥é¢åŸè¯ï¼Œä¸åšä»»ä½•æ›¿æ¢
        comp_cols.append(uniq[:MAX_COMP_COLS])

    for i in range(MAX_COMP_COLS):
        df_hit[f"company_{i+1}"] = [lst[i] if i < len(lst) else "" for lst in comp_cols]
        
# === â‘¢ å…ˆæŒ‰æ•°æ®åº“è§„åˆ™å¤„ç†æ¯è¡Œ company_n åˆ— ===
    ban_lower     = {b.lower() for b in ban_set}
    canon_lower   = {c.lower() for c in canon_set}
    alias_lower   = {a.lower(): canon for a, canon in alias_map.items()}
    canon_lower2orig = {c.lower(): c for c in canon_set}
    # â€”â€” æ ‡å‡†åŒ–ï¼šå»æ‰æ‰€æœ‰éå­—æ¯æ•°å­—ï¼Œå†å°å†™
    def _norm_key(s: str) -> str:
        return re.sub(r"[^A-Za-z0-9]", "", s).lower()

    comp_cols = [f"company_{i+1}" for i in range(MAX_COMP_COLS)]

    for ridx in df_hit.index:
        orig_names = [df_hit.at[ridx, c].strip() for c in comp_cols if df_hit.at[ridx, c].strip()]
        new_names  = []
        for nm in orig_names:
            nm_l = nm.lower()
            # â‘  ban â†’ ä¸¢å¼ƒ
            if nm_l in ban_lower:
                continue
            # â‘¡ å·²æ˜¯æ ‡å‡†å â†’ ä¿ç•™åŸæ ·
            if nm_l in canon_lower:
                new_names.append(canon_lower2orig[nm_l])
                continue
            # â‘¢ åˆ«å â†’ æ›¿æ¢ä¸ºå¯¹åº” canonical
            if nm_l in alias_lower:
                new_names.append(alias_lower[nm_l])
                continue
            # â‘£ æœªçŸ¥ â†’ åŸæ ·
            new_names.append(nm)

        # â‘¤ é¡ºä½å·¦ç§» + â€œåŒæ ¹â€ å»é‡
        cleaned = []
        seen_keys = set()
        for nm in sorted(new_names, key=len, reverse=True):           # å…ˆé•¿åçŸ­
            key = _norm_key(nm)
            # 1) ä¸å·²é€‰ä»»ä½•åç§° key å‰ç¼€ / åç¼€ ç›¸åŒ â†’ è§†ä¸ºé‡å¤
            if any(key in k or k in key for k in seen_keys):
                continue
            cleaned.append(nm)
            seen_keys.add(key)
        # â‘¥ å†™å›è¡Œï¼ˆä¸è¶³è¡¥ç©ºï¼Œç”¨ .atï¼‰
        for i, col in enumerate(comp_cols):
            df_hit.at[ridx, col] = cleaned[i] if i < len(cleaned) else ""


    # === â‘£ ç»§ç»­åŸæµç¨‹å†™ result.csvï¼ˆä¸‹æ–¹åŸä»£ç ä¿æŒä¸å˜ï¼‰ ===

    # ---- ç»„è£…ç²¾ç®€ç‰ˆ result.csv ----
    meta_cols = ["Tier_1", "Tier_2", "Filename",
                 "Title", "Publisher", "Sentence",
                 "Hit_Count", "Matched_Keywords"]

    df_final = (df_hit[meta_cols +
                [c for c in df_hit.columns if c.startswith("company_")]]
                .fillna(""))
    df_final = dedup_company_cols(df_final)

    df_final.to_csv(BASE_DIR / "result.csv",
                    index=False, encoding="utf-8-sig")
    cute_box(
        f"å·²ç”Ÿæˆ result.csvï¼Œå…± {len(df_final)} æ¡è®°å½•",
        f"result.csv ã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼šå…¨{len(df_final)}ä»¶",
        "ğŸ“‘"
    )
    
    # ---- ç”Ÿæˆ result_mapping_todo.csv ï¼ˆç©ºè¡¨å®‰å…¨ + ç»Ÿè®¡ï¼‰----
    # 1) nameâ†’id å­—å…¸ï¼ˆAdvice éœ€è¦ï¼‰
    canon_name2id = {row.canonical_name: row.id for row in df_canon.itertuples()}

    todo_rows: List[Dict] = []

    # ç»Ÿè®¡ï¼šå“ªäº›è¢«è¿‡æ»¤/è·³è¿‡
    ban_hits = alias_hits = canon_hits = 0
    rows_skipped_not_enough_companies = 0  # åŒè¡Œå…¬å¸æ€»æ•° < 2 çš„è¡Œ

    comp_cols = [c for c in df_final.columns if c.startswith("company_")]

    for _, row in df_final.iterrows():
        # å–å‡ºè¯¥è¡Œæ‰€æœ‰éç©ºä¼ä¸šåï¼ˆå·²åšè¿‡ban/alias/canonicalä¸€æ¬¡æ¸…æ´—ä¸åŒæ ¹å»é‡ï¼‰
        names = [row[c].strip() for c in comp_cols if row[c].strip()]

        # âœ… è¡Œçº§é—¨æ§›ï¼šæ— è®ºæ˜¯å¦å·²çŸ¥ï¼Œåªè¦åŒè¡Œå…¬å¸æ€»æ•° < 2ï¼Œå°±æ•´è¡Œè·³è¿‡
        if len(names) < 2:
            rows_skipped_not_enough_companies += 1
            continue

        # åªæŠŠâ€œæœªçŸ¥åˆ«åâ€å†™å…¥ todoï¼ˆå·²çŸ¥çš„ä¸éœ€è¦äººå·¥æ˜ å°„ï¼‰
        unknowns: List[str] = []
        for alias in names:
            alias_l = alias.lower()
            if alias_l in ban_lower:
                ban_hits += 1
                continue
            if alias_l in alias_lower:
                alias_hits += 1
                continue
            if alias_l in canon_lower:
                canon_hits += 1
                continue
            unknowns.append(alias)

        # è¯¥è¡Œè¾¾åˆ°â€œæœ‰2+å®¶å…¬å¸â€çš„é—¨æ§›ï¼Œå³ä½¿ unknowns åªæœ‰1ä¸ªæˆ–æ›´å¤šï¼Œéƒ½è¿›å…¥ todo
        for alias in unknowns:
            # --- é¦–è¯å‘½ä¸­ canonical ---
            first_tok = re.split(r'[\s\-]+', alias, maxsplit=1)[0].lower()
            if first_tok in canon_lower:
                advice     = canon_lower2orig[first_tok]
                adviced_id = canon_name2id.get(advice, "")
            else:
                # --- n-gram å®Œå…¨åŒ¹é… ---
                advice = adviced_id = ""
                words = alias.split()
                L = len(words)
                for size in range(L, 0, -1):
                    for i in range(0, L - size + 1):
                        phrase = " ".join(words[i:i+size])
                        key = phrase.lower()
                        if key in canon_lower:
                            advice     = canon_lower2orig[key]
                            adviced_id = canon_name2id.get(advice, "")
                            break
                    if advice:
                        break

            # --- è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆå¯é€‰ï¼‰ ---
            if not advice and canon_vecs.size > 0:
                alias_vec  = model_emb.encode([alias], normalize_embeddings=True)[0]
                sims       = np.dot(canon_vecs, alias_vec)
                best_idx   = int(np.argmax(sims))
                best_score = float(sims[best_idx])
                if best_score >= 0.80:
                    advice     = canon_names[best_idx]
                    adviced_id = canon_name2id.get(advice, "")

            # å†™å…¥ todo
            todo_rows.append({
                "Sentence":        row["Sentence"],
                "Alias":           alias,
                "Bad_Score":       calc_Bad_Score(alias),
                "Advice":          advice or "",
                "Adviced_ID":      adviced_id or "",
                "Canonical_Name":  "",
                "Std_Result":      ""
            })

    # 2) ç»„è£… DataFrameï¼ˆç©ºè¡¨å®‰å…¨ï¼‰
    todo_cols = [
        "Sentence", "Alias", "Bad_Score",
        "Advice", "Adviced_ID",
        "Canonical_Name", "Std_Result"
    ]

    if not todo_rows:
        # â€”â€” æ²¡æœ‰æ–°çš„åˆ«åéœ€è¦æ˜ å°„ï¼šå†™å‡ºåªæœ‰è¡¨å¤´çš„ç©ºè¡¨ï¼Œå¹¶å‹å¥½æç¤º
        todo_df = pd.DataFrame(columns=todo_cols)
        todo_df.to_csv(BASE_DIR / "result_mapping_todo.csv",
                       index=False, encoding="utf-8-sig")

        cute_box(
            "æœ¬æ‰¹æ²¡æœ‰äº§ç”Ÿæ–°çš„åˆ«åéœ€è¦æ˜ å°„ï¼›å·²è¢«è§„åˆ™è¯†åˆ«/è¿‡æ»¤ï¼Œæˆ–å› â€œä»… 1 ä¸ªç–‘ä¼¼ä¼ä¸šåâ€è§„åˆ™è€Œè·³è¿‡ã€‚\n"
            f"ban å‘½ä¸­ï¼š{ban_hits}ï¼Œå·²æœ‰ aliasï¼š{alias_hits}ï¼Œå·²æœ‰ canonicalï¼š{canon_hits}ï¼ŒåŒè¡Œå…¬å¸ä¸è¶³è·³è¿‡ï¼š{rows_skipped_not_enough_companies}",
            "ä»Šå›ã®ãƒãƒƒãƒã§ã¯æ–°ã—ã„åˆ¥åã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ä¸€è‡´ï¼é™¤å¤–ã€ã¾ãŸã¯ã€Œå€™è£œãŒ1ä»¶ã®ã¿ã€è¦å‰‡ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚\n"
            f"ban ä¸€è‡´ï¼š{ban_hits}ï¼æ—¢å­˜ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼š{alias_hits}ï¼æ—¢å­˜ã‚«ãƒãƒ‹ã‚«ãƒ«ï¼š{canon_hits}ï¼åŒä¸€è¡Œã®ä¼æ¥­æ•°ä¸è¶³ã‚¹ã‚­ãƒƒãƒ—ï¼š{rows_skipped_not_enough_companies}",
            "â„¹ï¸"
        )
    else:
        # â€”â€” æ­£å¸¸å»é‡ï¼ˆæŒ‰åˆ«åå°å†™ï¼‰
        todo_df = pd.DataFrame(todo_rows)
        todo_df["__alias_l"] = todo_df["Alias"].str.lower()
        todo_df = todo_df.drop_duplicates("__alias_l").drop(columns="__alias_l")

        # åˆ†ç»„æ’åº
        todo_df["__grp"] = todo_df["Bad_Score"].apply(lambda x: 0 if x >= 50 else (1 if x >= 10 else 2))
        todo_df = (todo_df
                   .sort_values(["__grp", "Sentence"], ascending=[True, True])
                   .drop(columns="__grp"))

        # å›ºå®šåˆ—é¡ºåº
        for col in todo_cols:
            if col not in todo_df.columns:
                todo_df[col] = ""   # å…œåº•ï¼Œä¿è¯åˆ—é½å…¨
        todo_df = todo_df[todo_cols]

        # å†™æ–‡ä»¶
        todo_df["Bad_Score"] = todo_df["Bad_Score"].astype(int).astype(str)
        todo_df['Sentence'] = todo_df['Sentence'].apply(
            lambda s: "'" + s if isinstance(s, str) and s.startswith('=') else s
        )
        todo_df.to_csv(BASE_DIR / "result_mapping_todo.csv",
                       index=False, encoding="utf-8-sig")

        cute_box(
            f"å·²ç”Ÿæˆ result_mapping_todo.csvï¼Œå…± {len(todo_df)} æ¡å¾…å¤„ç†åˆ«åã€‚\n"
            f"ï¼ˆban å‘½ä¸­ï¼š{ban_hits}ï¼Œå·²æœ‰ aliasï¼š{alias_hits}ï¼Œå·²æœ‰ canonicalï¼š{canon_hits}ï¼Œå•ä¸€å€™é€‰è·³è¿‡ï¼š{single_suspect_skipped}ï¼‰",
            f"result_mapping_todo.csv ã‚’ä½œæˆï¼š{len(todo_df)} ä»¶ã®å€™è£œã€‚\n"
            f"ï¼ˆban ä¸€è‡´ï¼š{ban_hits}ï¼æ—¢å­˜ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼š{alias_hits}ï¼æ—¢å­˜ã‚«ãƒãƒ‹ã‚«ãƒ«ï¼š{canon_hits}ï¼å˜ä¸€å€™è£œã‚¹ã‚­ãƒƒãƒ—ï¼š{single_suspect_skipped}ï¼‰",
            "ğŸ“"
        )
        
    cute_box(
    "Step-2 å®Œæˆï¼è¯·ç¼–è¾‘ result_mapping_todo.csv ç„¶åè¿è¡Œ Step-3",
    "Step-2 å®Œäº†ï¼result_mapping_todo.csv ã‚’ç·¨é›†ã—ã¦ã‹ã‚‰ Step-3 ã‚’å®Ÿè¡Œã—ã¦ã­",
    "âœ…"
    )
    cute_box(
        "result_mapping_todo.csv å¿«é€Ÿå¡«å†™æŒ‡å—ï¼š\n"
        "1) ç©ºç™½â†’è·³è¿‡\n"
        "2) 0â†’åŠ  ban_list\n"
        "3) nâ†’è§†ä¸º canonical_id\n"
        "4) å…¶ä»–â†’æ–°/å·²æœ‰æ ‡å‡†å",
        "result_mapping_todo.csv ç°¡æ˜“å…¥åŠ›ã‚¬ã‚¤ãƒ‰ï¼š\n"
        "1) ãƒ–ãƒ©ãƒ³ã‚¯â†’ã‚¹ã‚­ãƒƒãƒ—\n"
        "2) 0â†’ban_listç™»éŒ²\n"
        "3) nâ†’canonical_id ã¨è¦‹ãªã™\n"
        "4) ãã®ä»–â†’æ–°è¦/æ—¢å­˜æ¨™æº–å",
        "ğŸ“‹"
    )

# ================ Step-3 ==============

def step3(mysql_url: str):
    """
    Step-3 æ ‡å‡†åŒ– + å†™åº“ï¼ˆä¸æ—§ NA_step3_standardize.py ç­‰ä»·ï¼‰
    - Canonical_Name == ''  â†’ Std_Result = 'No input'
    - Canonical_Name == '0' â†’ å†™ ban_list,  Std_Result = 'Banned'
    - å…¶å®ƒ:
        â€¢ è‹¥å·²å­˜åœ¨ alias â†’ Std_Result = 'Exists'
        â€¢ å¦åˆ™æ’å…¥/è¡¥å…¨ canonical & alias, Std_Result = 'Added'
    åŒæ—¶æŠŠæœ€æ–°æ˜ å°„åº”ç”¨å› result.csv
    """
    # æœ¬è½®æ‰¹æ¬¡å·ï¼šYYYYMMDD + 8ä½éšæœºæ•°
    process_id = datetime.now().strftime("%Y%m%d") + f"{random.randint(0, 99999999):08d}"
    res_f  = BASE_DIR / "result.csv"
    todo_f = BASE_DIR / "result_mapping_todo.csv"
    if not (res_f.exists() and todo_f.exists()):
        cute_box(
            "æ‰¾ä¸åˆ° result.csv æˆ– result_mapping_todo.csvï¼Œè¯·å…ˆç”Ÿæˆå®ƒä»¬",
            "result.csv ã¾ãŸã¯ result_mapping_todo.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ä½œæˆã—ã¦ã­",
            "â—"
        )
        sys.exit(1)

    # è¯»å–ç°æœ‰æ–‡ä»¶
    df_res = pd.read_csv(res_f,  dtype=str).fillna("")
    df_map = pd.read_csv(todo_f, dtype=str).fillna("")
    if "Process_ID" not in df_map.columns:
        df_map["Process_ID"] = ""

    engine = create_engine(mysql_url)
    with engine.begin() as conn:
        # æ‹‰å–ä¸‰è¡¨åˆ°å†…å­˜
        ban_set    = {r[0] for r in conn.execute(text("SELECT alias FROM ban_list"))}
        canon_map  = {r[0]: r[1] for r in conn.execute(text("SELECT id, canonical_name FROM company_canonical"))}
        alias_map  = {r[0]: r[1] for r in conn.execute(text(
            "SELECT a.alias, c.canonical_name FROM company_alias a "
            "JOIN company_canonical c ON a.canonical_id=c.id"
        ))}
        # æ— è§†å¤§å°å†™çš„é•œåƒ
        ban_lower       = {b.lower() for b in ban_set}
        alias_lower_map = {a.lower(): c for a, c in alias_map.items()}
        canon_lower2id  = {name.lower(): cid for cid, name in canon_map.items()}

    # â€”â€” å¤„ç† todo æ˜ å°„ â€”â€”  
    for idx, row in df_map.iterrows():
        alias_raw   = row["Alias"].strip()
        alias_raw_l = alias_raw.lower()
        canon_input = row["Canonical_Name"].strip()
        if not canon_input:
            df_map.at[idx, "Std_Result"] = "No input"
            continue

        # â‘  Banï¼ˆè¾“å…¥ 0ï¼‰
        if canon_input == "0":
            if alias_raw_l not in ban_lower:
                with engine.begin() as conn:
                    conn.execute(text(
                        "INSERT INTO ban_list(alias, process_id) VALUES (:a, :pid)"
                    ), {"a": alias_raw, "pid": process_id})
                ban_lower.add(alias_raw_l)
            df_map.at[idx, "Std_Result"]   = "Banned"
            df_map.at[idx, "Process_ID"] = f"'{process_id}"
            continue

        # â‘¡ æ•°å­— â†’ è§†ä¸º existing canonical_id
        if canon_input.isdigit():
            cid = int(canon_input)
            if cid not in canon_map:
                df_map.at[idx, "Std_Result"] = "Bad ID"
                continue
            canon_name = canon_map[cid]
        else:
            # æ–° canonical
            ci_l = canon_input.lower()
            if ci_l not in canon_lower2id:
                with engine.begin() as conn:
                    res = conn.execute(text(
                        "INSERT INTO company_canonical(canonical_name, process_id) VALUES (:c, :pid)"
                    ), {"c": canon_input, "pid": process_id})
                new_id = res.lastrowid
                canon_map[new_id]        = canon_input
                canon_lower2id[ci_l] = new_id
                df_map.at[idx, "Process_ID"] = f"'{process_id}"
                canon_name = canon_input
            else:
                canon_name = canon_map[canon_lower2id[ci_l]]
        # â‘¢ å†™ aliasï¼ˆå¿½ç•¥å¤§å°å†™å·²å­˜åœ¨ï¼‰
        if alias_raw_l in alias_lower_map or alias_raw_l in canon_lower2id:
            df_map.at[idx, "Std_Result"] = "Exists"
            continue
        with engine.begin() as conn:
            conn.execute(text(
                "INSERT IGNORE INTO company_alias(alias, canonical_id, process_id) "
                "VALUES (:a, :cid, :pid)"
            ), {"a": alias_raw, "cid": canon_lower2id[canon_name.lower()], "pid": process_id})
        alias_lower_map[alias_raw_l] = canon_name
        df_map.at[idx, "Std_Result"]   = "Added"
        df_map.at[idx, "Process_ID"] = f"'{process_id}"

    # å…ˆå†™å› todoï¼Œå†åšå›å†™ result.csv
    df_map.to_csv(todo_f, index=False, encoding="utf-8-sig")

    # ====== å°†æœ€æ–°æ˜ å°„åº”ç”¨å› result.csv ======
    # é‡æ–°æ‹‰å– ban/alias/canonical å‡†å¤‡æ˜ å°„
    with engine.begin() as conn2:
        ban_set2    = {r[0] for r in conn2.execute(text("SELECT alias FROM ban_list"))}
        rows2       = conn2.execute(text(
            "SELECT a.alias, c.canonical_name FROM company_alias a "
            "JOIN company_canonical c ON a.canonical_id=c.id"
        ))
        alias_map2  = {a: c for a, c in rows2}
        canon_set2  = {r[0] for r in conn2.execute(text("SELECT canonical_name FROM company_canonical"))}

    ban_lower2        = {b.lower() for b in ban_set2}
    alias_lower_map2  = {a.lower(): c for a, c in alias_map2.items()}
    canon_lower2orig2 = {c.lower(): c for c in canon_set2}

    comp_cols = [c for c in df_res.columns if c.startswith("company_")]

    def _norm_key(s: str) -> str:
        return re.sub(r"[^A-Za-z0-9]", "", str(s)).lower()

    changed_cells = 0
    for ridx in df_res.index:
        # è¯»å‡ºåŸå€¼
        orig = df_res.loc[ridx, comp_cols].astype(str).tolist()
        vals_in = [v.strip() for v in orig if v.strip()]
        vals_out = []
        for nm in vals_in:
            key = nm.lower()
            if key in ban_lower2:
                continue
            if key in alias_lower_map2:
                nm = alias_lower_map2[key]
                changed_cells += 1
                key = nm.lower()
            elif key in canon_lower2orig2:
                corrected = canon_lower2orig2[key]
                if corrected != nm:
                    changed_cells += 1
                nm = corrected
            vals_out.append(nm)
        # åŒæ ¹å»é‡ + å·¦ç§»
        cleaned, seen = [], set()
        for nm in sorted(vals_out, key=len, reverse=True):
            k = _norm_key(nm)
            if any(k in kk or kk in k for kk in seen):
                continue
            cleaned.append(nm)
            seen.add(k)
        # å›å†™
        for i, col in enumerate(comp_cols):
            new_val = cleaned[i] if i < len(cleaned) else ""
            if str(df_res.at[ridx, col]) != new_val:
                changed_cells += 1
            df_res.at[ridx, col] = new_val

    # å†æ¸…ä¸€æ¬¡åˆ—å†…é‡å¤
    df_res = dedup_company_cols(df_res)

    cute_box(
        f"å·²å°†æœ€æ–°æ˜ å°„åº”ç”¨åˆ° result.csvï¼ˆå˜æ›´å•å…ƒæ ¼çº¦ {changed_cells} ä¸ªï¼‰",
        f"æœ€æ–°ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ result.csv ã«é©ç”¨ã—ã¾ã—ãŸï¼ˆå¤‰æ›´ã‚»ãƒ«æ•° ç´„ {changed_cells}ï¼‰",
        "ğŸ› ï¸"
    )

    # æœ€åä¿å­˜
    df_res.to_csv(res_f, index=False, encoding="utf-8-sig")

    cute_box(
        f"Step-3 å®Œæˆï¼Œå¤„ç† {len(df_map)} æ¡æ˜ å°„ï¼Œresult.csv å·²æ›´æ–°",
        f"Step-3 å®Œäº†ï¼š{len(df_map)}ä»¶ å‡¦ç†æ¸ˆã¿ï¼Œresult.csv æ›´æ–°å®Œäº†",
        "ğŸš€"
    )
    cute_box(
        f"æœ¬æ‰¹æ¬¡ Process IDï¼š{process_id}",
        f"ä»Šå›ã® Process IDï¼š{process_id}",
        "ğŸ“Œ"
    )
               
            
def step4():
    import pandas as _pd

    # 1) è¯» CSV
    df = _pd.read_csv(BASE_DIR / "result.csv", dtype=str).fillna("")

    # 2) å‡†å¤‡è¾“å‡ºè¡Œï¼šæ³¨æ„è¿™é‡Œç»™æ¯ä¸€æ¡éƒ½åŠ ä¸Š value=1
    rows = []
    for _, r in tqdm(df.iterrows(), desc="ç”Ÿæˆé‚»æ¥è¡¨", total=len(df)):
        comps = [r[f"company_{i}"] 
                 for i in range(1, MAX_COMP_COLS+1) 
                 if r[f"company_{i}"].strip()]
        for a, b in itertools.permutations(comps, 2):
            rows.append({
                "company_a": a,
                "company_b": b,
                "value": 1,
            })

    # 3) æ„å»ºå®Œæ•´ DataFrame
    out = _pd.DataFrame(rows)

    # 4) å†™ adjacency list ï¼ˆåªä¿ç•™ a/b ä¸¤åˆ—ï¼‰
    out[['company_a','company_b']].to_csv(
        BASE_DIR / "result_adjacency_list.csv",
        index=False, encoding="utf-8-sig"
    )
    cute_box(
        "Step4 å·²ç”Ÿæˆé‚»æ¥è¡¨ï¼šresult_adjacency_list.csv",
        "Step4 éš£æ¥ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼šresult_adjacency_list.csv",
        "ğŸ“‹"
    )

    # â€”â€”â€” ç”Ÿæˆå¸¦è¡Œåˆ—æ ‡é¢˜çš„ Pivot Table â€”â€”â€”
    pivot = out.pivot_table(
        index="company_a",      # è¡Œæ ‡ç­¾
        columns="company_b",    # åˆ—æ ‡ç­¾
        values="value",         # èšåˆå­—æ®µ
        aggfunc="sum",          # æŠŠæ‰€æœ‰ value=1 ç´¯åŠ 
        fill_value=""           # 0 æˆ– NaN éƒ½æ˜¾ç¤ºç©ºç™½
    )

    # 5) å¯¼å‡ºå¸¦è¡Œ/åˆ—æ ‡é¢˜çš„çŸ©é˜µ
    pivot.to_csv(
        BASE_DIR / "pivot_table.csv",
        encoding="utf-8-sig"
    )
    cute_box(
        "Step4 å·²ç”Ÿæˆé€è§†è¡¨ï¼špivot_table.csv",
        "Step4 ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼špivot_table.csv",
        "ğŸ“Š"
    )
def main():
    mysql_url = ask_mysql_url()
    try:
        create_engine(mysql_url).connect().close()
        cute_box(
            "æ•°æ®åº“è¿æ¥æˆåŠŸï¼",
            "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š æˆåŠŸï¼",
            "ğŸ”—"
        )
    except Exception as e:
        cute_box(
            f"æ•°æ®åº“è¿æ¥å¤±è´¥ï¼š{e}",
            f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š å¤±æ•—ï¼š{e}",
            "âŒ"
        )
        sys.exit(1)

    choice = choose()

    if choice == "1":
        step1()
        step2(mysql_url)

        # â€”â€” æ–°å¢ï¼šè·‘å®Œ Step-2 åç­‰å¾…ç”¨æˆ·æŒ‡ä»¤ â€”â€”
        while True:
            nxt = input("ğŸ‘‰ è¾“å…¥ 2 ç»§ç»­ Step-3/4ï¼Œæˆ–è¾“å…¥ e é€€å‡º / 2 ã§Step-3/4ã‚’ç¶šè¡Œ, e ã§çµ‚äº†: ").strip().lower()
            if nxt == "2":
                step3(mysql_url)
                step4()
                cute_box(
                "Step-3/4 å…¨éƒ¨å®Œæˆï¼",
                "Step-3/4 å…¨ã¦å®Œäº†ã—ã¾ã—ãŸï¼",
                "ğŸ‰"
                )
                break
            elif nxt == "e":
                cute_box(
                "å·²é€€å‡ºï¼Œæ‹œæ‹œï½",
                "çµ‚äº†ã—ã¾ã—ãŸã€ã¾ãŸã­ï¼",
                "ğŸ‘‹"
                )
                return
            else:
                cute_box(
                "æ— æ•ˆè¾“å…¥ï¼Œè¯·å†è¯•ä¸€æ¬¡ï¼",
                "ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ã€‚ã‚‚ã†ä¸€åº¦å…¥åŠ›ã—ã¦ã­ï¼",
                "ğŸ”„"
                )

    else:   # choice == "2"
        step3(mysql_url)
        step4()
        cute_box(
        "Step-3/4 å…¨éƒ¨å®Œæˆï¼",
        "Step-3/4 å…¨ã¦å®Œäº†ã—ã¾ã—ãŸï¼",
         "ğŸ‰"
        )


if __name__ == "__main__":
    main()

# åŸä½œè€…ï¼šæ¨å¤©ä¹@å…³è¥¿å¤§å­¦ / Author: Shiame Yeung@Kansai University / ä½œæˆè€…ï¼šæ¥Šã€€å¤©æ¥½@é–¢è¥¿å¤§å­¦
#!/usr/bin/env python3
# coding: utf-8
"""
na_pipeline.py  â€”â€”  å•æ–‡ä»¶ç‰ˆï¼ˆStepâ€‘1 å¯¹é½ + æ‰©å±•å…¬å¸è¯†åˆ«ï¼‰
2025â€‘07â€‘08  revâ€‘C
"""

def cute_box(cn: str, jp: str, icon: str = "ğŸŒ¸") -> None:
    """
    å¤šè¡Œä¹Ÿèƒ½å¯¹é½çš„å¯çˆ±ä¸­/æ—¥åŒè¯­æ¡†
    cn: ä¸­æ–‡æç¤ºï¼ˆå¯ä»¥å¤šè¡Œï¼Œç”¨ '\\n' åˆ†éš”ï¼‰
    jp: æ—¥æ–‡æç¤ºï¼ˆå¯ä»¥å¤šè¡Œï¼‰
    icon: æ¯è¡Œå¼€å¤´å’Œç»“å°¾çš„å°è¡¨æƒ…
    """
    # æŠŠä¸­/æ—¥å„è‡ªçš„å¤šè¡Œæ‹†å¼€ï¼Œæ‹¼æˆç»Ÿä¸€åˆ—è¡¨
    lines = []
    for segment in (cn, jp):
        for ln in segment.split("\n"):
            ln = ln.strip()
            # ç”¨ "icon + ç©ºæ ¼ + æ–‡æœ¬ + ç©ºæ ¼ + icon" æ„é€ æ¯ä¸€è¡Œ
            lines.append(f"{icon} {ln} {icon}")

    # æ‰¾åˆ°æœ€é•¿é‚£è¡Œï¼Œåšä¸ºæ¡†å®½
    width = max(len(ln) for ln in lines)
    border = "â”€" * width

    # æ‰“å°ä¸Šè¾¹æ¡†
    print(f"â•­{border}â•®")
    # æ‰“å°æ¯ä¸€è¡Œï¼Œå³ä¾§å¡«å……ç©ºæ ¼åˆ° width
    for ln in lines:
        print("â”‚" + ln.ljust(width) + "â”‚")
    # æ‰“å°ä¸‹è¾¹æ¡†
    print(f"â•°{border}â•¯")

import sys, subprocess, os

def ensure_env() -> None:
    """
    ç¯å¢ƒè‡ªæ£€ä¸è‡ªåŠ¨ä¿®å¤ç¨‹åº
    1. æ£€æŸ¥æ‰€æœ‰å¿…è¦çš„åº“ (åŒ…æ‹¬ OpenAI, GLiNER, RapidFuzz ç­‰)
    2. ç¼ºå¤±åˆ™è‡ªåŠ¨è°ƒç”¨ pip å®‰è£…
    3. å®‰è£…å®Œæˆåè‡ªåŠ¨é‡å¯è„šæœ¬ï¼Œå®ç°æ— ç¼ä½“éªŒ
    """
    import sys
    import subprocess
    import pkg_resources
    from pkg_resources import DistributionNotFound, VersionConflict

    # --- å®šä¹‰é¡¹ç›®æ‰€éœ€çš„å…¨éƒ¨ä¾èµ– ---
    # æ ¼å¼éµå¾ª pip requirements.txt æ ‡å‡†
    REQUIRED_PACKAGES = [
        # åŸºç¡€å·¥å…·
        "pandas", 
        "tqdm", 
        "requests",
        "packaging",
        
        # æ•°æ®åº“
        "sqlalchemy", 
        "pymysql",
        
        # æ–‡æœ¬å¤„ç†
        "python-docx", 
        "rapidfuzz",  # æ¨¡ç³ŠåŒ¹é…
        
        # AI ä¸ æ¨¡å‹ (æ ¸å¿ƒ)
        "openai>=1.0.0",          # å¿…é¡» 1.0 ä»¥ä¸Šç‰ˆæœ¬
        "gliner",                 # æ–°å¢ï¼šå®ä½“æå–
        "sentence-transformers",  # è¯­ä¹‰å‘é‡
        "torch",                  # æ·±åº¦å­¦ä¹ åç«¯
        "transformers",           # HuggingFace å·¥å…·
        
        # æ—§ç‰ˆå…¼å®¹ (å¦‚æœè¿˜ç”¨ spaCy)
        "spacy",
    ]

    # æ£€æŸ¥å½“å‰ Python ç‰ˆæœ¬ä»¥å†³å®šç‰¹å®šä¾èµ– (å¯é€‰)
    py_major, py_minor = sys.version_info[:2]
    if (py_major, py_minor) >= (3, 13):
        # Python 3.13+ å¯èƒ½éœ€è¦ç‰¹å®šç‰ˆæœ¬çš„ numpy æˆ–å…¶ä»–åº“ï¼Œè¿™é‡Œæš‚ä¸”ä¿ç•™é€šç”¨
        pass

    missing = []
    
    # --- 1. æ£€æŸ¥ç¼ºå¤±åŒ… ---
    for pkg in REQUIRED_PACKAGES:
        try:
            pkg_resources.require(pkg)
        except (DistributionNotFound, VersionConflict):
            missing.append(pkg)

    # --- 2. æ£€æŸ¥ spaCy æ¨¡å‹ (ç‰¹ä¾‹) ---
    try:
        import spacy
        if not spacy.util.is_package("en_core_web_sm"):
            missing.append("spacy_model:en_core_web_sm")
    except ImportError:
        pass # spacy æœ¬èº«ç¼ºå¤±ä¼šåœ¨ä¸Šé¢è¢«æ•è·

    # --- 3. æ‰§è¡Œå®‰è£… ---
    if missing:
        cute_box(
            f"æ£€æµ‹åˆ°ç¼ºå¤±ä¾èµ–ï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…...\nç¼ºå¤±é¡¹: {', '.join(missing)}",
            f"ä¸è¶³ã—ã¦ã„ã‚‹ä¾å­˜é–¢ä¿‚ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ä¸­...\nå¯¾è±¡: {', '.join(missing)}",
            "ğŸ“¦"
        )
        
        # åˆ†ç¦»æ™®é€šåŒ…å’Œ spaCy æ¨¡å‹
        pip_pkgs = [p for p in missing if not p.startswith("spacy_model:")]
        spacy_models = [p for p in missing if p.startswith("spacy_model:")]

        # å®‰è£… pip åŒ…
        if pip_pkgs:
            try:
                # æ³¨æ„ï¼šè¿™é‡Œå»æ‰äº† stdout=subprocess.DEVNULLï¼Œè®©ç”¨æˆ·çœ‹åˆ°è¿›åº¦æ¡
                subprocess.check_call([sys.executable, "-m", "pip", "install"] + pip_pkgs)
            except subprocess.CalledProcessError as e:
                cute_box(f"å®‰è£…å¤±è´¥: {e}\nè¯·å°è¯•æ‰‹åŠ¨è¿è¡Œ: pip install {' '.join(pip_pkgs)}", 
                         "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ‰‹å‹•ã§å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚", "âŒ")
                sys.exit(1)

        # å®‰è£… spaCy æ¨¡å‹
        for model in spacy_models:
            model_name = model.split(":")[1]
            print(f"â¬‡ï¸ Downloading spaCy model: {model_name}...")
            subprocess.check_call([sys.executable, "-m", "spacy", "download", model_name])

        cute_box(
            "ä¾èµ–å®‰è£…å®Œæˆï¼æ­£åœ¨è‡ªåŠ¨é‡å¯ç¨‹åº...",
            "ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†ï¼ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’è‡ªå‹•å†èµ·å‹•ã—ã¾ã™...",
            "ğŸ”„"
        )

        # --- 4. è‡ªåŠ¨é‡å¯è„šæœ¬ (é»‘ç§‘æŠ€) ---
        # ä½¿ç”¨ os.execv é‡æ–°åŠ è½½å½“å‰è„šæœ¬ï¼Œç»§æ‰¿å½“å‰çš„è¿›ç¨‹ ID
        # è¿™æ ·ç”¨æˆ·å°±ä¸éœ€è¦æ‰‹åŠ¨å†è¾“ä¸€æ¬¡å‘½ä»¤äº†
        os.execv(sys.executable, [sys.executable] + sys.argv)

# â€”â€”â€”â€”â€”â€” åœ¨è„šæœ¬ä¸€å¯åŠ¨å°±å…ˆç¡®ä¿ç¯å¢ƒ â€”â€”â€”â€”â€”â€”
ensure_env()

import os, re, sys, unicodedata, string
from pathlib import Path
from typing import List, Dict, Set

from datetime import datetime
import random

import itertools

import json
from openai import OpenAI

import pandas as pd
from tqdm import tqdm
from sqlalchemy import create_engine, text
from rapidfuzz import fuzz, process
import numpy as np
import torch
from sentence_transformers import SentenceTransformer

try:
    from docx import Document
    import spacy
    nlp = spacy.load("en_core_web_sm", disable=["parser", "lemmatizer"])
    model_emb = SentenceTransformer(
        "sentence-transformers/all-MiniLM-L6-v2",
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
except Exception:
    cute_box(
      "ç¼ºå°‘ä¾èµ–ï¼šè¯·è¿è¡Œ pip install python-docx spacy",
      "ä¾å­˜é–¢ä¿‚ãŒè¶³ã‚Šã¾ã›ã‚“ï¼špip install python-docx spacy ã‚’å®Ÿè¡Œã—ã¦ã­",
      "âš ï¸"
    )
    sys.exit(1)

# ---------------- å¸¸é‡ ----------------
STOPWORDS = {"the","and","for","with","from","that","this","have","will","are","you","not","but","all","any","one","our","their"}
# é¢„è®¾çš„å…³é”®è¯åˆ—è¡¨ (é€‰é¡¹1)
PRESET_KEYWORDS_2025 = [
    'partner','alliance','collaborat','cooper','cooperat','join','merger','acquisiti',
    'outsourc','invest','licens','integrat','coordinat','synergiz','associat',
    'confedera','federa','union','unit','amalgamat','conglomerat','combin',
    'buyout','companion','concur','concert','comply','complement','assist',
    'takeover','accession','procure','suppl','conjoint','support','adjust',
    'adjunct','patronag','subsid','affiliat','endors'
]
# å…¨å±€ä½¿ç”¨çš„å…³é”®è¯åˆ—è¡¨ (åˆå§‹ä¸ºç©ºï¼Œç¨ååœ¨ configure_keywords ä¸­èµ‹å€¼)
KEYWORD_ROOTS = []

# åŒ¹é…: "April 28, 2025" æˆ– "21 May 2025"
DATE_FINDER = re.compile(
    r'\b(?:(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:t(?:ember)?)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\s+\d{1,2},?\s+\d{4}|\d{1,2}\s+(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:t(?:ember)?)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\s+\d{4})\b',
    re.IGNORECASE
)
# ---------------- Bad-Rate è§„åˆ™ ----------------
ORG_SUFFIX  = re.compile(
    r'\b(Inc\.?|Corp\.?|Corporation|Ltd\.?|LLC|PLC|AG|NV|SA|GmbH|S\.p\.A|Co\.?|Company|'
    r'Group|Holdings?|Partners?|Capital|Ventures?|Bank|Trust|Software|'
    r'Technolog(?:y|ies)|Pharma(?:ceuticals)?|Systems?|Services?|'
    r'Industr(?:y|ies)|Foundation|Laborator(?:y|ies)|'
    r'University|College|Institute|School|Hospital|Center|Centre)\b',
    re.I)

TIME_QTY    = re.compile(
    r'\b(year|month|week|day|decade|centur(?:y|ies)|quarter|q[1-4]|'
    r'ago|last|next|few|couple|several|dozen|half|around|approximately)s?\b',
    re.I)

# â”€â”€ é‡‘èæŠ¥è¡¨ / ä¸šç»©å…¬å‘Šç±» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FIN_REPORT = re.compile(
    r'\b(results?|earnings?|revenues?|turnover|profit(?:s)?|loss(?:es)?|guidance|forecast|'
    r'financial statements?|balance sheets?|cash flows?|income statements?)\b',
    re.I)

# â”€â”€ åˆ†å­£ / åˆ†åŠæœŸ / åˆ†å¹´æè¿° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ORDINAL_PERIOD = re.compile(
    r'\b(first|second|third|fourth|fifth|sixth|seventh|eighth|ninth|tenth)\b.*?\b(quarter|half|year)\b',
    re.I)

# â”€â”€ å…¸å‹â€œå…¬å‘Š/æŠ¥å‘Š/æ›´æ–°â€è§¦å‘è¯ï¼ˆå¤šè§äº ban_listï¼‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ANNOUNCE_VERB = re.compile(
    r'\b(reports?|announces?|updates?|revises?|publishes?|files?|issues?|unveils?)\b',
    re.I)
# ===== åœ¨å¸¸é‡åŒºï¼ˆTIME_QTY ä¹‹åï¼‰æ–°å¢å‡ æ¡é€šç”¨ regex =====
GENERIC_NOUN = re.compile(
    r'\b(services?|solutions?|systems?|platforms?|programs?|projects?|'
    r'statements?|reports?|targets?|technologies?|operations?|activities|'
    r'strategies?|plans?)\b', re.I)

MONTH_NAME = re.compile(
    r'\b(jan(?:uary)?|feb(?:ruary)?|mar(?:ch)?|apr(?:il)?|may|jun(?:e)?|'
    r'jul(?:y)?|aug(?:ust)?|sep(?:t(?:ember)?)?|oct(?:ober)?|nov(?:ember)?|'
    r'dec(?:ember)?)\b', re.I)

NEW_GENERIC_TIME = re.compile(
    r'\b(?:end|beginning|middle|start|first|second|third|fourth|prior|previous|'
    r'current|next)\s+(?:of\s+)?(?:the\s+)?(?:year|quarter|month|week)s?\b',
    re.I)

#  çº¯å¤§å†™ 2â€“4 ä½ç¼©å†™ï¼ˆPBM / ESG â€¦ï¼‰
ALLCAP_SHORT = re.compile(r'^[A-Z]{2,4}$')

#  %ã€ç™¾ä¸‡/åäº¿ã€ç¾å…ƒç¬¦å·ä¹‹ç±»
NUMERIC = re.compile(r'[%\$]\s*\d|\d[\d,\.]+\s*(?:million|billion|thousand)', re.I)


ALL_UPPER  = re.compile(r'^[A-Z]{2,}$')
ALL_LOWER  = re.compile(r'^[a-z]{4,}$')

SHORT_TOKEN = re.compile(r'^[A-Za-z]{1,4}$')
ART_LOWER   = re.compile(r'^\s*(a|an|about|approximately|the|this|that|those)\s+[a-z]')
GENERIC_END = re.compile(
    r'\b(plan|plans?|programs?|systems?|platforms?|services?|solutions?|operations?|'
    r'agreements?|strategies?|reports?|statements?)$', re.I)

def _lower_ratio(text: str) -> float:
    w = text.split()
    return sum(t[0].islower() for t in w) / len(w) if w else 0

# --- ã€æ–°å¢ã€‘é¢„å®šä¹‰ä¸€äº›â€œéå…¬å¸â€çš„åƒåœ¾æ¦‚å¿µå‘é‡ ---
# è¿™äº›è¯ä»£è¡¨äº†æˆ‘ä»¬æƒ³è¿‡æ»¤æ‰çš„â€œåƒåœ¾ç±»å‹â€
NOISE_CONCEPTS = [
    "financial report results",   # è´¢æŠ¥ç±»
    "fiscal year quarter",        # æ—¶é—´ç±»
    "forward looking statements", # æ³•å¾‹å£°æ˜ç±»
    "January February March",     # æœˆä»½
    "global market growth",       # æ³›æŒ‡å¸‚åœº
    "conference call webcast",    # ä¼šè®®
    "operating expenses",         # ä¼šè®¡æœ¯è¯­
    "agreement partnership"       # æ³›æŒ‡åˆä½œ
]

# é¢„è®¡ç®—åƒåœ¾æ¦‚å¿µçš„å‘é‡ï¼ˆä¸ºäº†é€Ÿåº¦ï¼Œåªç®—ä¸€æ¬¡ï¼‰
# æ³¨æ„ï¼šè¿™è¡Œä»£ç è¦æ”¾åœ¨ model_emb åŠ è½½ä¹‹å
print("â³ æ­£åœ¨é¢„è®¡ç®—åƒåœ¾è¯å‘é‡...")
noise_vecs = model_emb.encode(NOISE_CONCEPTS, normalize_embeddings=True)

def calc_Bad_Score(text: str) -> int:
    """
    ã€å‡çº§ç‰ˆã€‘è§„åˆ™ + AI æ··åˆè¯„åˆ†
    """
    score = 0
    
    # === â‘  è§„åˆ™åˆ¤æ–­ (ä¿ç•™åŸæœ‰çš„å¿«é€Ÿç­›é€‰ï¼Œé€Ÿåº¦æå¿«) ===
    if ORG_SUFFIX.search(text): return 0       # åƒå…¬å¸åï¼Œç›´æ¥æ”¾è¡Œ
    
    # åŸºç¡€è§„åˆ™æ‰£åˆ†
    if TIME_QTY.search(text): score += 30
    if FIN_REPORT.search(text): score += 30
    if len(text.split()) <= 2: score += 10
    if _lower_ratio(text) > 0.30: score += 10
    
    # === â‘¡ AI è¯­ä¹‰åˆ¤æ–­ (æ–°å¢æ ¸å¿ƒåŠŸèƒ½) ===
    # åªæœ‰å½“ text æ¯”è¾ƒé•¿ï¼Œæˆ–è€…è§„åˆ™æ²¡åˆ¤ 0 åˆ†æ—¶ï¼Œæ‰åŠ¨ç”¨ AI (çœç®—åŠ›)
    if score > 0 or len(text.split()) > 2:
        # 1. è®¡ç®—å½“å‰è¯çš„å‘é‡
        text_vec = model_emb.encode([text], normalize_embeddings=True)[0]
        
        # 2. è®¡ç®—å®ƒå’Œâ€œåƒåœ¾æ¦‚å¿µâ€çš„æœ€å¤§ç›¸ä¼¼åº¦
        # np.dot è®¡ç®—ç‚¹ç§¯ (å› ä¸ºå·²ç»normalizeäº†ï¼Œæ‰€ä»¥ç­‰åŒäºä½™å¼¦ç›¸ä¼¼åº¦)
        sims = np.dot(noise_vecs, text_vec)
        max_sim = float(np.max(sims))
        
        # 3. æ ¹æ®ç›¸ä¼¼åº¦æ‰£åˆ†
        if max_sim > 0.4:  # ç¨å¾®æœ‰ç‚¹åƒåƒåœ¾
            score += 20
        if max_sim > 0.6:  # å¾ˆåƒåƒåœ¾
            score += 40
        if max_sim > 0.8:  # å‡ ä¹ç¡®å®šæ˜¯åƒåœ¾
            score += 100

    return score
# ---------------- å…¨å±€å˜é‡ ----------------
BASE_DIR = Path(__file__).resolve().parent
MAX_COMP_COLS = 50
SENTENCE_RECORDS: List[Dict] = []

# ---------------- å…±ç”¨ ----------------



# ------- æ–°ç‰ˆæœ¬ï¼šé¦–æ¬¡è¾“å…¥åå†™ .db_keyï¼Œåç»­è‡ªåŠ¨è¯»å– -------
def ask_mysql_url() -> str:
    key_file = Path(__file__).with_name(".db_key")   # è„šæœ¬åŒç›®å½• .db_key
    if key_file.exists():
        key = key_file.read_text().strip()
    else:
        key = input("è¯·è¾“å…¥ç§˜é’¥/ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼šuser:pass@host\n>>>>>> ").strip()
        key_file.write_text(key)                     # ç¼“å­˜ä¸‹æ¬¡ç”¨
    return f"mysql+pymysql://{key}:3306/na_data?charset=utf8mb4" 

def choose() -> str:
    """
    æ˜¾ç¤ºä¸»èœå•ï¼Œè¿”å›ç”¨æˆ·é€‰æ‹©
    """
    cute_box(
        "CorpLink-AI è‡ªåŠ¨åŒ–å¤„ç†ç³»ç»Ÿ\n"
        "------------------------------------------------\n"
        "â‘  [å¼€å§‹] æå–æ•°æ® (Step 1-2)\n"
        "   - ä»æ–‡æ¡£æå–å¥å­ -> åˆæ­¥è¯†åˆ« -> ç”Ÿæˆå¾…æ¸…æ´—è¡¨\n\n"
        "â‘¡ [æ¸…æ´—] AI è‡ªåŠ¨åå¯„ã› (Step 2.5)\n"
        "   - è°ƒç”¨ GPT API è‡ªåŠ¨æ¸…æ´—/æ ‡å‡†åŒ– result_mapping_todo.csv\n\n"
        "â‘¢ [å®Œæˆ] å…¥åº“ä¸åˆ†æ (Step 3-4)\n"
        "   - è¯»å–æ¸…æ´—åçš„è¡¨ -> å­˜å…¥æ•°æ®åº“ -> ç”Ÿæˆç½‘ç»œåˆ†æè¡¨\n"
        "------------------------------------------------\n"
        "ä½œè€…ï¼šæ¨å¤©ä¹ @ å…³è¥¿å¤§å­¦ ä¼Šä½ç”°ç ”ç©¶å®¤",
        
        "CorpLink-AI è‡ªå‹•åŒ–å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ \n"
        "------------------------------------------------\n"
        "â‘  [é–‹å§‹] ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºãƒ»ä¸€æ¬¡å‡¦ç† (Step 1-2)\n"
        "   - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè§£æ -> ä¼æ¥­åæŠ½å‡º -> å€™è£œãƒªã‚¹ãƒˆç”Ÿæˆ\n\n"
        "â‘¡ [æµ„åŒ–] AIã«ã‚ˆã‚‹è‡ªå‹•åå¯„ã› (Step 2.5)\n"
        "   - GPT APIã‚’åˆ©ç”¨ã—ã¦ã€è¡¨è¨˜ã‚†ã‚Œã‚„ãƒã‚¤ã‚ºã‚’è‡ªå‹•ä¿®æ­£\n\n"
        "â‘¢ [å®Œäº†] DBç™»éŒ²ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æ (Step 3-4)\n"
        "   - ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’DBã¸ç™»éŒ² -> åˆ†æç”¨ãƒ†ãƒ¼ãƒ–ãƒ«å‡ºåŠ›\n"
        "------------------------------------------------\n"
        "ä½œæˆè€…ï¼šæ¥Š å¤©æ¥½ã€€å”åŠ›ï¼šæ å®—æ˜Š æ ä½³ç’‡ @é–¢è¥¿å¤§å­¦",
        
        "ğŸ¤–"
    )
    
    while True:
        c = input("ğŸ‘‰ è¯·è¾“å…¥åŠŸèƒ½åºå· / ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (1/2/3): ").strip()
        if c in {"1", "2", "3"}:
            return c
        print("âŒ è¾“å…¥æ— æ•ˆï¼Œè¯·é‡æ–°è¾“å…¥ / ç„¡åŠ¹ãªå…¥åŠ›ã§ã™")

# ---------- ã€æ–°å¢åŠŸèƒ½ã€‘å…³é”®è¯é…ç½®å‡½æ•° ----------
# å…¨å±€å˜é‡
KEYWORD_ROOTS = []
USE_SEMANTIC_FILTER = False  # æ–°å¢ï¼šæ ‡è®°æ˜¯å¦ä½¿ç”¨è¯­ä¹‰ç­›é€‰

# è¯­ä¹‰ç­›é€‰çš„â€œæ ‡æ†å¥å­â€
ANCHOR_TEXT = "Companies announce strategic partnership, joint venture, merger, acquisition, investment, or business collaboration."

def configure_keywords():
    """
    é…ç½®ç­›é€‰æ¨¡å¼ï¼šé¢„è®¾å…³é”®è¯ã€è‡ªå®šä¹‰å…³é”®è¯ã€æˆ– AI è¯­ä¹‰ç­›é€‰
    """
    global KEYWORD_ROOTS, USE_SEMANTIC_FILTER
    
    cute_box(
        "ã€é…ç½®ã€‘è¯·é€‰æ‹©ä¿¡æ¯æŠ½å–çš„æ¨¡å¼ï¼š\n"
        "1. å…³é”®è¯æ¨¡å¼: 2025 AI x Healthcare (é»˜è®¤)\n"
        "2. å…³é”®è¯æ¨¡å¼: è‡ªå®šä¹‰è¾“å…¥\n"
        "3. AIè¯­ä¹‰æ¨¡å¼: è¯­ä¹‰å‘é‡åŒ¹é… (Beta)(sentence-transformers/all-MiniLM-L6-v2)",
        
        "ã€è¨­å®šã€‘æƒ…å ±æŠ½å‡ºãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼š\n"
        "1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰: 2025 AI x ãƒ˜ãƒ«ã‚¹ã‚±ã‚¢ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)\n"
        "2. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ¢ãƒ¼ãƒ‰: ã‚«ã‚¹ã‚¿ãƒ å…¥åŠ› (ãã®ä»–)\n"
        "3. AIãƒ¢ãƒ¼ãƒ‰: ãƒ™ã‚¯ãƒˆãƒ«é¡ä¼¼åº¦ãƒãƒƒãƒãƒ³ã‚° (Beta)(sentence-transformers/all-MiniLM-L6-v2)",
        "âš™ï¸"
    )
    
    choice = input("ğŸ‘‰ è¯·è¾“å…¥ / ç•ªå·ã‚’å…¥åŠ› (1/2/3) [Default: 1]: ").strip()
    
    if choice == "3":
        USE_SEMANTIC_FILTER = True
        print("\nâœ… [System] AIè¯­ä¹‰ç­›é€‰å·²å¯ç”¨ (Model: sentence-transformers/all-MiniLM-L6-v2)")
        print("   [System] AIãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒæœ‰åŠ¹ã«ãªã‚Šã¾ã—ãŸ")
        
    elif choice == "2":
        print("\nğŸ‘‰ è¯·è¾“å…¥è‡ªå®šä¹‰å…³é”®è¯ (é€—å·åˆ†éš”) / ã‚«ã‚¹ã‚¿ãƒ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ› (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š):")
        raw_input = input(">>>>>> ").strip()
        try:
            custom_keys = [k.strip().strip("'").strip('"') for k in raw_input.split(',') if k.strip()]
            if not custom_keys: raise ValueError
            KEYWORD_ROOTS = custom_keys
            print(f"âœ… [System] å·²åŠ è½½ {len(KEYWORD_ROOTS)} ä¸ªè‡ªå®šä¹‰å…³é”®è¯")
        except:
            print("âŒ [Error] æ ¼å¼é”™è¯¯ï¼Œå·²å›é€€åˆ°é»˜è®¤æ¨¡å¼ / ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã«æˆ»ã‚Šã¾ã™")
            KEYWORD_ROOTS = PRESET_KEYWORDS_2025
    else:
        KEYWORD_ROOTS = PRESET_KEYWORDS_2025
        print("âœ… [System] å·²åŠ è½½é»˜è®¤å…³é”®è¯é›† / ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")

def dedup_company_cols(df: pd.DataFrame) -> pd.DataFrame:
    comp_cols = [c for c in df.columns if c.startswith("company_")]
    for ridx in df.index:
        seen: Set[str] = set()
        for col in comp_cols:
            val = str(df.at[ridx, col]).strip()
            if val in seen:
                df.at[ridx, col] = ""
            else:
                seen.add(val)
    return df

# ---------------- Stepâ€‘1 ----------------

def _normalize(text: str) -> str:
    t = re.sub(r"\s+", " ", text.lower().strip())
    t = re.sub(r"^[\-:\"']+|[\-:\"']+$", "", t)
    t = re.sub(r"[,.;/()]+", "", t)
    return t.strip()

def clean_text(t: str) -> str:
    return ''.join(c for c in t if unicodedata.category(c)[0] != 'C' or c in ('\n', '\t'))

def extract_sentences(path: Path) -> List[str]:
    doc = Document(path)
    collecting, current, articles = False, "", []
    for p in doc.paragraphs:
        txt = p.text.strip()
        if not txt: continue
        tag = txt.lower()
        if tag == "body": collecting, current = True, ""; continue
        if tag in ("notes", "classification") and collecting:
            collecting = False; articles.append(current.strip()); continue
        if collecting: current += " " + txt
    sents = []
    for art in articles:
        for s in re.split(r"\.\s*", art):
            s = s.strip();
            if len(s) >= 5: sents.append(s)
    return sents

def extract_index_titles(paragraphs):
    paras_text = [p.text.strip() for p in paragraphs]
    
    # 1. å°è¯•è·å–æ€»ç¯‡æ•°
    m = re.search(r'Documents?\s*\(\s*(\d+)\s*\)', '\n'.join(paras_text[:50]), re.I)
    if not m: return []
    total = int(m.group(1))
    
    pat = re.compile(r'^(\d+)\.\s+(.*)$')
    seen = set()
    titles = []
    
    for i, line in enumerate(paras_text):
        m2 = pat.match(line)
        if m2:
            # === ã€æ ¸å¿ƒä¿®å¤ã€‘éªŒè¯æœºåˆ¶ ===
            # çœŸæ­£çš„ç›®å½•æ ‡é¢˜ï¼Œåé¢ 6 è¡Œå†…ä¸€å®šæœ‰ "Client/Matter"
            is_valid_toc = False
            for offset in range(1, 7): 
                if i + offset >= len(paras_text): break
                next_line = paras_text[i + offset].lower()
                if "client/matter" in next_line or "search terms" in next_line:
                    is_valid_toc = True
                    break
            
            # å¦‚æœåé¢æ²¡æœ‰å…ƒæ•°æ®ï¼Œè¯´æ˜è¿™æ˜¯æ­£æ–‡é‡Œçš„æ™®é€šåˆ—è¡¨ï¼ˆå¦‚ "3. REVENUE"ï¼‰ï¼Œè·³è¿‡ï¼
            if not is_valid_toc:
                continue 
            
            # === éªŒè¯é€šè¿‡ ===
            raw = m2.group(2).strip()
            norm = _normalize(raw)
            
            if norm in seen: continue
            seen.add(norm)
            titles.append((int(m2.group(1)), raw, norm))
            
            if len(titles) >= total: break
                
    return sorted(titles, key=lambda x: x[0])

def extract_sentences_by_titles(filepath: str) -> List[Dict]:
    """
    ã€ä¿®å¤ç‰ˆ v4ã€‘
    1. åŒ…å«æ—¥æœŸéªŒè¯æœºåˆ¶ï¼šé˜²æ­¢åŒ¹é…åˆ°æ­£æ–‡é‡Œçš„é‡å¤æ ‡é¢˜ï¼Œè§£å†³ Publisher æŠ“é”™é—®é¢˜ã€‚
    2. åŒ…å«è¯­ä¹‰ç­›é€‰åŠŸèƒ½ï¼šæ”¯æŒå…³é”®è¯/AIè¯­ä¹‰æ¨¡å¼åˆ‡æ¢ã€‚
    """
    doc = Document(filepath); paras = doc.paragraphs
    index_titles = extract_index_titles(paras); recs = []
    
    if index_titles:
        paras_norm = [_normalize(p.text) for p in paras]
        last_article_end_idx = 0

        for i_title, (doc_idx, title_raw, title_norm) in enumerate(index_titles):
            match_idx = -1
            date_line_idx = -1
            
            # ä»ä¸Šæ¬¡ç»“æŸä½ç½®å¼€å§‹ï¼Œå¯»æ‰¾æ‰€æœ‰åŒ¹é…çš„æ ‡é¢˜è¡Œ
            candidates = [i for i, n in enumerate(paras_norm) 
                          if i >= last_article_end_idx and n == title_norm]
            
            for idx in candidates:
                # 1. ç›®å½•æ£€æŸ¥ (Client/Matter) - è·³è¿‡ç›®å½•
                if idx + 1 < len(paras):
                    next_line = paras[idx+1].text.strip().lower()
                    if next_line.startswith("client/matter") or next_line.startswith("search terms"):
                        continue 

                # 2. ã€å…³é”®ä¿®å¤ã€‘æ—¥æœŸéªŒè¯æœºåˆ¶
                # çœŸæ­£çš„æ–‡ç« æ ‡é¢˜ï¼Œå…¶å 1-3 è¡Œå†…ä¸€å®šåŒ…å«æ—¥æœŸã€‚
                # å¦‚æœæ‰¾ä¸åˆ°æ—¥æœŸï¼Œè¯´æ˜è¿™æ˜¯æ­£æ–‡é‡Œçš„å‡æ ‡é¢˜ï¼ˆæ‘˜è¦æˆ–å¼•ç”¨ï¼‰ï¼Œè·³è¿‡ï¼
                found_date = False
                temp_date_idx = -1
                
                # å¾€åçœ‹ 3 è¡Œå¯»æ‰¾æ—¥æœŸ
                for offset in range(1, 4):
                    if idx + offset >= len(paras): break
                    txt = paras[idx + offset].text.strip()
                    if DATE_FINDER.search(txt):
                        found_date = True
                        temp_date_idx = idx + offset
                        break
                
                if found_date:
                    match_idx = idx
                    date_line_idx = temp_date_idx
                    break
            
            # å¦‚æœæ²¡æ‰¾åˆ°å¸¦æ—¥æœŸçš„æ ‡é¢˜ï¼Œå°±æ”¾å¼ƒè¿™ç¯‡æ–‡ç« ï¼ˆé˜²æ­¢æŠ“é”™ï¼‰
            if match_idx == -1: 
                continue

            # ----------------------------------

            # 3. æ™ºèƒ½æå– Publisher
            # Publisher é€šå¸¸åœ¨ Title å’Œ Date ä¹‹é—´
            # å¦‚æœ Date åœ¨ Title ä¸‹é¢ç¬¬ 2 è¡Œæˆ–æ›´è¿œï¼Œä¸­é—´é‚£å°±æ˜¯ Publisher
            if date_line_idx > match_idx + 1:
                publisher = paras[match_idx + 1].text.strip()
            else:
                publisher = "" # åªæœ‰æ—¥æœŸï¼Œæ²¡å†™å‡ºç‰ˆç¤¾
            
            # 4. æå–æ—¥æœŸ
            news_date = ""
            m = DATE_FINDER.search(paras[date_line_idx].text.strip())
            if m: news_date = m.group(0)

            # 5. ç¡®å®šæ­£æ–‡èŒƒå›´
            # Body åº”è¯¥åœ¨ Date ä¹‹åå¼€å§‹ï¼Œæ›´æ–° pub_idx ä¸ºæ—¥æœŸè¡Œï¼Œæ–¹ä¾¿åç»­å®šä½
            pub_idx = date_line_idx 
            
            search_end_limit = len(paras)
            if i_title + 1 < len(index_titles):
                next_title_norm = index_titles[i_title+1][2]
                try:
                    next_candidates = [i for i, n in enumerate(paras_norm) 
                                       if i > match_idx + 20 and n == next_title_norm]
                    if next_candidates: search_end_limit = next_candidates[0]
                except Exception: pass

            body_start = next((i+1 for i in range(pub_idx+1, search_end_limit) if paras[i].text.strip().lower() == "body"), None)
            if body_start is None: 
                body_start = pub_idx + 1 # å¦‚æœæ²¡ Body æ ‡ç­¾ï¼Œå°±ä»æ—¥æœŸä¸‹ä¸€è¡Œå¼€å§‹
            
            body_end = len(paras)
            for i in range(body_start, search_end_limit):
                t_low = paras[i].text.strip().lower()
                if t_low.startswith("notes") or t_low.startswith("classification") or "(end) dow jones" in t_low:
                    body_end = i
                    break
            last_article_end_idx = body_end

            # 6. æå–å¥å­ (ä¿ç•™äº†è¯­ä¹‰ç­›é€‰é€»è¾‘)
            article = " ".join(clean_text(paras[i].text) for i in range(body_start, body_end))
            
            # åˆ‡åˆ†å¥å­
            raw_sents = [s.strip() for s in re.split(r"\.\s*", article) if len(s.strip())>=20]

            # --- å¦‚æœæ˜¯è¯­ä¹‰æ¨¡å¼ï¼Œå…ˆæ‰¹é‡è®¡ç®—å‘é‡ ---
            if 'USE_SEMANTIC_FILTER' in globals() and USE_SEMANTIC_FILTER and raw_sents:
                # è®¡ç®—æ ‡æ†å‘é‡ (å¦‚æœè¿˜æ²¡ç®—è¿‡)
                if not hasattr(extract_sentences_by_titles, "anchor_vec"):
                     extract_sentences_by_titles.anchor_vec = model_emb.encode([ANCHOR_TEXT], normalize_embeddings=True)[0]
                
                sent_vecs = model_emb.encode(raw_sents, normalize_embeddings=True)
                sim_scores = np.dot(sent_vecs, extract_sentences_by_titles.anchor_vec)
            else:
                sim_scores = [0.0] * len(raw_sents)

            # --- éå†å¥å­è¿›è¡Œç­›é€‰ ---
            for i, sent in enumerate(raw_sents):
                is_hit = False
                match_reason = ""
                hit_count = 0

                if 'USE_SEMANTIC_FILTER' in globals() and USE_SEMANTIC_FILTER:
                    # === æ¨¡å¼ A: AI è¯­ä¹‰ç­›é€‰ ===
                    score = float(sim_scores[i])
                    if score > 0.45: # é˜ˆå€¼å¯è°ƒ
                        is_hit = True
                        hit_count = 1 # è¯­ä¹‰å‘½ä¸­ç®— 1 åˆ†
                        match_reason = f"Semantic({score:.2f})"
                else:
                    # === æ¨¡å¼ B: å…³é”®è¯ç­›é€‰ ===
                    hits = [k for k in KEYWORD_ROOTS if k in sent.lower()]
                    if hits:
                        is_hit = True
                        hit_count = len(hits)
                        match_reason = "; ".join(hits)
                
                if is_hit:
                    recs.append({
                        "Title": title_raw,
                        "Publisher": publisher,
                        "Date": news_date,
                        "Country": "",
                        "Sentence": sent,
                        "Hit_Count": hit_count,
                        "Matched_Keywords": match_reason
                    })
        
        if recs: return recs

    # Fallback (æ— ç´¢å¼•æƒ…å†µ)
    # è¿™é‡Œä¸éœ€è¦å¤ªå¤æ‚çš„æ—¥æœŸéªŒè¯ï¼Œå› ä¸ºå•ç¯‡æ–‡ç« é€šå¸¸ç»“æ„ç®€å•
    for sent in extract_sentences(Path(filepath)):
        # è¿™é‡Œåªåšäº†ç®€å•çš„å…³é”®è¯ç­›é€‰å…¼å®¹ï¼Œå¦‚æœéœ€è¦Fallbackä¹Ÿæ”¯æŒè¯­ä¹‰ï¼Œé€»è¾‘åŒä¸Š
        hits = [k for k in KEYWORD_ROOTS if k in sent.lower()]
        if hits:
             recs.append({
                "Title": "", "Publisher": "", "Date": "", "Country": "", 
                "Sentence": sent, "Hit_Count": len(hits), 
                "Matched_Keywords": "; ".join(hits)
            })
    return recs

def step1():
    cute_box(
        "Step-1ï¼šæå– Word å¥å­ ä¸­â€¦",
        "Step-1ï¼šæ–‡æŠ½å‡ºä¸­â€¦",
        "ğŸ“„"
    )
    all_recs: List[Dict] = []

    # 1) æ”¶é›†æ‰€æœ‰ .docx è·¯å¾„
    docx_files = []
    for root, _, files in os.walk(BASE_DIR):
        for fname in files:
            if not fname.endswith(".docx") or fname.startswith("~$"):
                continue
            full = Path(root) / fname
            rel = full.relative_to(BASE_DIR).parts
            tier1 = rel[0] if len(rel) >= 1 else ""
            tier2 = rel[1] if len(rel) >= 2 else ""
            docx_files.append((str(full), tier1, tier2, fname))

    # 2) é€æ–‡ä»¶æå–å¥å­
    for fp, t1, t2, fname in tqdm(docx_files, desc="ğŸ“„ å¤„ç† Word æ–‡ä»¶"):
        for r in extract_sentences_by_titles(fp):
            if not r["Title"]:
                r["Title"] = Path(fname).stem
            r.update({"Tier_1": t1, "Tier_2": t2, "Filename": fname})
            all_recs.append(r)

    global SENTENCE_RECORDS
    SENTENCE_RECORDS = all_recs
    cute_box(
        f"Step-1 å®Œæˆï¼Œå…± {len(all_recs)} æ¡è®°å½•",
        f"Step-1 å®Œäº†ã—ã¾ã—ãŸï¼šå…¨{len(all_recs)}ä»¶",
        "âœ…"
    )

# ----------------â€”â€” Stepâ€‘2 â€”â€”----------------

def is_valid_token(token: str) -> bool:
    token = token.strip()
    if "@" in token or token.startswith("http"):    # â‘  å«é‚®ç®± / URL ç‰¹å¾
        return False
    if not token or all(c in "-â€“â€”ãƒ».ã€ã€‚ï¼ï¼Ÿï¼ãƒ¼" for c in token):
        return False
    if re.search(r"\d", token) and not re.search(r"[A-Za-z]", token):
        return False
    if "  " in token:
        return False
    return True


# â€”â€” 4. åŸå§‹ä¼ä¸šåæå– â€”â€”
# â€”â€” 4. åŸå§‹ä¼ä¸šåæå– â€”â€”
def extract_companies(text: str,
                      company_db: List[str],
                      ner_model,
                      fuzzy_threshold: int = 95) -> List[str]:
    """
    Â· **ä»…è´Ÿè´£â€œæŠŠå¥å­é‡Œå¯èƒ½æ˜¯å…¬å¸åçš„ç‰‡æ®µå…¨éƒ¨æŠ“å‡ºæ¥â€**ï¼Œ
      ä¸åšä»»ä½• ban/æ˜ å°„/å»é‡å¤„ç†â€”â€”è¿™äº›ç•™ç»™åç»­æ•°æ®åº“æ¯”å¯¹é˜¶æ®µå®Œæˆã€‚
    Â· è¯†åˆ«é€»è¾‘å®Œå…¨æ²¿ç”¨å•ä½“ç‰ˆï¼ˆspaCy NER + â€œIBMersâ€æ­£åˆ™ + ä¸¥æ ¼æ¨¡ç³ŠåŒ¹é…ï¼‰ã€‚
    """
    comps: Set[str] = set()

    # 1) å»æ‰æ—¥æœŸï¼ˆæ’é™¤ â€˜xx/xx/xxxx ä¹‹åæ•´æ®µâ€™ çš„å™ªéŸ³ï¼‰
    text_clean = re.sub(r"\s*\d{1,2}/\d{1,2}/\d{2,4}.*$", "", text).strip()
    # --- æ–°å¢æ¸…æ´— ---
    # 1) å»æ‰ Â® â„¢ Â©
    text_clean = re.sub(r"[Â®â„¢Â©]", "", text_clean)
    # 2) å»æ‰ç®€å†™å•†æ ‡æ‹¬å·ï¼Œå¦‚ â€œWeight Doctors(R)â€
    text_clean = re.sub(r"\(\s*[A-Z]{1,3}\s*\)", "", text_clean)
    # 3) æ•´å¥é‡Œå¸¦é‚®ç®±çš„ç›´æ¥å‰ªæ‰é‚®ç®±
    text_clean = re.sub(r"\b\S+@\S+\b", "", text_clean)

    # 2) spaCy NER
    doc = ner_model(text_clean)
    for ent in doc.ents:
        ent_text = ent.text.strip()

        # â€”â€” åŸºç¡€å™ªéŸ³è¿‡æ»¤ï¼ˆå’Œå•ä½“ç‰ˆä¸€è‡´ï¼‰
        if "  " in ent_text or re.search(r"[\d/%+]|[^\x00-\x7F]", ent_text):
            continue
        valid_ent = True
        for w in ent_text.split():
            if (not w[0].isalpha()
                or w in {"The","And","For","With","From","That","This"}
                or not is_valid_token(w)):
                valid_ent = False
                break
        if valid_ent:
            comps.add(ent_text)

    # 3) â€œIBMersâ€ ä¸€ç±»å†™æ³•
    for m in re.findall(r"\b([A-Z]{2,})ers\b", text_clean):
        comps.add(m)

    # 4) ä»…ç”¨äºâ€œç¡®è®¤æ˜¯å·²çŸ¥å…¬å¸â€ï¼Œä½†ä¾æ—§è¿”å›åŸè¯
    STOPWORDS = {"The","And","For","With","From","That","This","Have","Will",
                "Are","You","Not","But","All","Any","One","Our","Their"}

    tokens = re.findall(r"\b\S+\b", text_clean)
    for pos, token in enumerate(tokens):
        # â€”â€” å™ªéŸ³ä¸æ ¼å¼è¿‡æ»¤ï¼ˆåŒåŸå…ˆé€»è¾‘ï¼‰ â€”â€”
        if (pos == 0 or token in STOPWORDS
            or any(ch in token for ch in "/%+") or "  " in token
            or len(token) < 5 or not token[0].isupper() or token.isupper()
            or re.search(r"\d|[^\x00-\x7F]", token)
            or not is_valid_token(token)):
            continue

        # è‹¥æ•°æ®åº“é‡Œå­˜åœ¨â€œå®Œå…¨åŒåï¼ˆå¤§å°å†™ä¸åŒè§†åŒï¼‰â€çš„æ¡ç›®ï¼Œå°±ä¿ç•™ï¼›å¦åˆ™å¿½ç•¥
        if any(token.lower() == db.lower() for db in company_db):
            comps.add(token)

    return list(comps)


def step2(mysql_url: str):
    cute_box(
        "Step-2ï¼šå…¬å¸è¯†åˆ«ï¼‹BAN è¿‡æ»¤ ä¸­â€¦",
        "Step-2ï¼šä¼æ¥­åèªè­˜ï¼‹BAN ãƒ•ã‚£ãƒ«ã‚¿ä¸­â€¦",
        "ğŸ·ï¸"
    )
    # å•ç‹¬å¯¼å‡º canonical è¡¨ï¼ˆengine_tmpï¼‰
    engine_tmp = create_engine(mysql_url)            # â† æ–°å»º
    df_canon = pd.read_sql("SELECT id, canonical_name FROM company_canonical", engine_tmp)
    df_canon.to_csv(BASE_DIR / "canonical_list.csv", index=False, encoding="utf-8-sig")
    cute_box(
        f"å·²å†™å‡º canonical_list.csvï¼Œå…± {len(df_canon)} è¡Œ",
        f"canonical_list.csv ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼š{len(df_canon)} è¡Œ",
        "ğŸ—‚ï¸"
    )
    # ---- è¿æ¥æ•°æ®åº“ ----
    engine = create_engine(mysql_url)
    with engine.begin() as conn:
        ban_set = {r[0] for r in conn.execute(text("SELECT alias FROM ban_list"))}
        rows = conn.execute(text("""
            SELECT a.alias, c.canonical_name FROM company_alias a
            JOIN company_canonical c ON a.canonical_id = c.id
        """))
        alias_map = {alias: canon for alias, canon in rows}
        canon_set = {r[0] for r in conn.execute(text("SELECT canonical_name FROM company_canonical"))}
        # â€”â€” é¢„ç¼–ç å…¨éƒ¨ canonicalï¼Œä¸€æ¬¡æå®š â€”â€”
        canon_names = list(canon_set)
        canon_vecs  = model_emb.encode(canon_names, batch_size=64, normalize_embeddings=True)
        # â†“â†“â†“ æ–°å¢ï¼šåå­—â†’ID çš„å­—å…¸ï¼Œç”¨äº Advice å¯¹åº”çš„ ID
        rows2 = conn.execute(text(
            "SELECT id, canonical_name FROM company_canonical"
        ))
        canon_name2id = {name: cid for cid, name in rows2}      # â† æ–°å¢
    
    cute_box(
    f"ban_list={len(ban_set)}ï¼Œalias_map={len(alias_map)}ï¼Œcanon_set={len(canon_set)}",
    f"ban_listï¼š{len(ban_set)}ä»¶ï¼alias_mapï¼š{len(alias_map)}ä»¶ï¼canon_setï¼š{len(canon_set)}ä»¶",
    "ğŸ”"
    )

    df = pd.DataFrame(SENTENCE_RECORDS)
    df_hit = df[df["Hit_Count"].astype(int) >= 1].reset_index(drop=True)
    if df_hit.empty:
        cute_box(
        "Step-1 æ²¡æå–åˆ°ä»»ä½•å¥å­ï¼Œè¯·å…ˆè·‘ Step-1ï¼",
        "Step-1 ã§æ–‡ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã¾ãš Step-1 ã‚’å®Ÿè¡Œã—ã¦ã­",
        "ğŸš«"
        )
        return

    company_db = list(canon_set) + list(alias_map.keys())   # canonical + alias
    comp_cols: List[List[str]] = []
    for sent in tqdm(df_hit["Sentence"].tolist(), desc="å…¬å¸è¯†åˆ«"):
        names_raw = extract_companies(sent, company_db, nlp)
        uniq: List[str] = []
        for alias in names_raw:
            if alias in uniq:     
                continue
            uniq.append(alias)                         # ä¿ç•™å¥é¢åŸè¯ï¼Œä¸åšä»»ä½•æ›¿æ¢
        comp_cols.append(uniq[:MAX_COMP_COLS])

    for i in range(MAX_COMP_COLS):
        df_hit[f"company_{i+1}"] = [lst[i] if i < len(lst) else "" for lst in comp_cols]
        
# === â‘¢ å…ˆæŒ‰æ•°æ®åº“è§„åˆ™å¤„ç†æ¯è¡Œ company_n åˆ— ===
    ban_lower     = {b.lower() for b in ban_set}
    canon_lower   = {c.lower() for c in canon_set}
    alias_lower   = {a.lower(): canon for a, canon in alias_map.items()}
    canon_lower2orig = {c.lower(): c for c in canon_set}
    # â€”â€” æ ‡å‡†åŒ–ï¼šå»æ‰æ‰€æœ‰éå­—æ¯æ•°å­—ï¼Œå†å°å†™
    def _norm_key(s: str) -> str:
        return re.sub(r"[^A-Za-z0-9]", "", s).lower()

    comp_cols = [f"company_{i+1}" for i in range(MAX_COMP_COLS)]

    for ridx in df_hit.index:
        orig_names = [df_hit.at[ridx, c].strip() for c in comp_cols if df_hit.at[ridx, c].strip()]
        new_names  = []
        for nm in orig_names:
            nm_l = nm.lower()
            # â‘  ban â†’ ä¸¢å¼ƒ
            if nm_l in ban_lower:
                continue
            # â‘¡ å·²æ˜¯æ ‡å‡†å â†’ ä¿ç•™åŸæ ·
            if nm_l in canon_lower:
                new_names.append(canon_lower2orig[nm_l])
                continue
            # â‘¢ åˆ«å â†’ æ›¿æ¢ä¸ºå¯¹åº” canonical
            if nm_l in alias_lower:
                new_names.append(alias_lower[nm_l])
                continue
            # â‘£ æœªçŸ¥ â†’ åŸæ ·
            new_names.append(nm)

        # â‘¤ é¡ºä½å·¦ç§» + â€œåŒæ ¹â€ å»é‡
        cleaned = []
        seen_keys = set()
        for nm in sorted(new_names, key=len, reverse=True):           # å…ˆé•¿åçŸ­
            key = _norm_key(nm)
            # 1) ä¸å·²é€‰ä»»ä½•åç§° key å‰ç¼€ / åç¼€ ç›¸åŒ â†’ è§†ä¸ºé‡å¤
            if any(key in k or k in key for k in seen_keys):
                continue
            cleaned.append(nm)
            seen_keys.add(key)
        # â‘¥ å†™å›è¡Œï¼ˆä¸è¶³è¡¥ç©ºï¼Œç”¨ .atï¼‰
        for i, col in enumerate(comp_cols):
            df_hit.at[ridx, col] = cleaned[i] if i < len(cleaned) else ""


    # === â‘£ ç»§ç»­åŸæµç¨‹å†™ result.csvï¼ˆä¸‹æ–¹åŸä»£ç ä¿æŒä¸å˜ï¼‰ ===

    # ---- ç»„è£…ç²¾ç®€ç‰ˆ result.csv ----
    meta_cols = ["Tier_1", "Tier_2", "Filename", "Date",
                 "Title", "Publisher", "Sentence",
                 "Hit_Count", "Matched_Keywords"]

    df_final = (df_hit[meta_cols +
                [c for c in df_hit.columns if c.startswith("company_")]]
                .fillna(""))
    df_final = dedup_company_cols(df_final)

    df_final.to_csv(BASE_DIR / "result.csv",
                    index=False, encoding="utf-8-sig")
    cute_box(
        f"å·²ç”Ÿæˆ result.csvï¼Œå…± {len(df_final)} æ¡è®°å½•",
        f"result.csv ã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼šå…¨{len(df_final)}ä»¶",
        "ğŸ“‘"
    )
    
    # ---- ç”Ÿæˆ result_mapping_todo.csv ï¼ˆç©ºè¡¨å®‰å…¨ + ç»Ÿè®¡ï¼‰----
    # 1) nameâ†’id å­—å…¸ï¼ˆAdvice éœ€è¦ï¼‰
    canon_name2id = {row.canonical_name: row.id for row in df_canon.itertuples()}

    todo_rows: List[Dict] = []

    # ç»Ÿè®¡ï¼šå“ªäº›è¢«è¿‡æ»¤/è·³è¿‡
    ban_hits = alias_hits = canon_hits = 0
    rows_skipped_not_enough_companies = 0  # åŒè¡Œå…¬å¸æ€»æ•° < 2 çš„è¡Œ

    comp_cols = [c for c in df_final.columns if c.startswith("company_")]

    for _, row in df_final.iterrows():
        # å–å‡ºè¯¥è¡Œæ‰€æœ‰éç©ºä¼ä¸šåï¼ˆå·²åšè¿‡ban/alias/canonicalä¸€æ¬¡æ¸…æ´—ä¸åŒæ ¹å»é‡ï¼‰
        names = [row[c].strip() for c in comp_cols if row[c].strip()]

        # â€”â€” åˆ†ç±»ç»Ÿè®¡ï¼ˆå…ˆæ›´æ–° ban/alias/canonical è®¡æ•°ï¼Œå†æ”¶é›† unknownsï¼‰
        unknowns: List[str] = []
        for alias in names:
            alias_l = alias.lower()
            if alias_l in ban_lower:
                ban_hits += 1
                continue
            if alias_l in alias_lower:
                alias_hits += 1
                continue
            if alias_l in canon_lower:
                canon_hits += 1
                continue
            unknowns.append(alias)
        # âœ… è¡Œçº§é—¨æ§›ï¼šåŒè¡Œå…¬å¸æ€»æ•°ï¼ˆnamesï¼‰< 2 â†’ ä¸å…¥ todoï¼Œä½†ä¸Šé¢çš„ç»Ÿè®¡å·²è®¡å…¥
        if len(names) < 2:
            rows_skipped_not_enough_companies += 1
            continue

        # è¯¥è¡Œè¾¾åˆ°â€œæœ‰2+å®¶å…¬å¸â€çš„é—¨æ§›ï¼Œå³ä½¿ unknowns åªæœ‰1ä¸ªæˆ–æ›´å¤šï¼Œéƒ½è¿›å…¥ todo
        # ---------------- ä¼˜åŒ–åçš„åŒ¹é…é€»è¾‘ (Fuzzy + AI) ----------------
        
        # 1. æ‰¹é‡è®¡ç®—æœ¬è¡Œ unknowns çš„å‘é‡ (æ€§èƒ½ä¼˜åŒ–ï¼šä¸€æ¬¡ç®—å®Œï¼Œæ¯”å¾ªç¯é‡Œä¸€ä¸ªä¸ªç®—å¿«å¾—å¤š)
        if len(canon_vecs) > 0 and unknowns:
            unknown_vecs = model_emb.encode(unknowns, normalize_embeddings=True)
        else:
            unknown_vecs = []

        for i, alias in enumerate(unknowns):
            advice = ""
            adviced_id = ""
            match_info = "" # è°ƒè¯•ç”¨ï¼Œçœ‹çœ‹æ˜¯è°ç«‹åŠŸäº†
            
            # --- ç­–ç•¥ A: é«˜ç²¾åº¦æ¨¡ç³ŠåŒ¹é… (RapidFuzz) ---
            # ä¸“æ²»ï¼šæ‹¼å†™é”™è¯¯ã€åç¼€å·®å¼‚ (e.g. "Apple Incc" vs "Apple Inc.")
            # token_sort_ratio å¯ä»¥å¿½ç•¥å•è¯é¡ºåº (e.g. "Motors General" vs "General Motors")
            fuzzy_res = process.extractOne(alias, canon_names, scorer=fuzz.token_sort_ratio)
            
            if fuzzy_res:
                candidate, score, _ = fuzzy_res
                # è®¾å®šä¸€ä¸ªè¾ƒé«˜çš„é—¨æ§›ï¼Œæ¯”å¦‚ 90 åˆ†ï¼Œç¡®ä¿å­—é¢éå¸¸åƒæ‰ç›´æ¥é‡‡çº³
                if score >= 90:
                    advice = candidate
                    adviced_id = canon_name2id.get(advice, "")
                    match_info = f"Fuzzy({score:.0f})"
            
            # --- ç­–ç•¥ B: AI å‘é‡è¯­ä¹‰åŒ¹é… ---
            # åªæœ‰å½“ Fuzzy æ²¡æå®š (advice ä¸ºç©º) æ—¶ï¼Œæ‰è¯· AI å‡ºå±±
            if not advice and len(canon_vecs) > 0:
                # å–å‡ºåˆšåˆšæ‰¹é‡ç®—å¥½çš„å‘é‡
                curr_vec = unknown_vecs[i]
                
                # è®¡ç®—ä¸æ‰€æœ‰æ ‡å‡†åçš„ç›¸ä¼¼åº¦
                sims = np.dot(canon_vecs, curr_vec)
                best_idx = int(np.argmax(sims))
                vector_score = float(sims[best_idx])
                
                # é˜ˆå€¼ï¼š0.82 (ç¨å¾®æé«˜ä¸€ç‚¹é—¨æ§›ï¼Œå‡å°‘å¹»è§‰)
                if vector_score >= 0.82:
                    advice = canon_names[best_idx]
                    adviced_id = canon_name2id.get(advice, "")
                    match_info = f"AI({vector_score:.2f})"

            # --- å­˜å…¥ Todo ---
            todo_rows.append({
                "Sentence": row["Sentence"],
                "Alias":    alias,
                "Bad_Score": calc_Bad_Score(alias),
                "Advice":   advice,           # æ¨èç»“æœ
                "Adviced_ID": adviced_id,     # æ¨èID
                # "Match_Info": match_info,   # (å¯é€‰) å¦‚æœä½ æƒ³åœ¨ CSV é‡Œçœ‹åˆ°æ˜¯ AI è¿˜æ˜¯ Fuzzy åŒ¹é…çš„ï¼Œå¯ä»¥æŠŠè¿™è¡ŒåŠ åˆ° csv åˆ—é‡Œ
                "Canonical_Name": "",
                "Std_Result": ""
            })

    # 2) ç»„è£… DataFrameï¼ˆç©ºè¡¨å®‰å…¨ï¼‰
    todo_cols = [
        "Sentence", "Alias", "Bad_Score",
        "Advice", "Adviced_ID",
        "Canonical_Name", "Std_Result"
    ]

    if not todo_rows:
        # â€”â€” æ²¡æœ‰æ–°çš„åˆ«åéœ€è¦æ˜ å°„ï¼šå†™å‡ºåªæœ‰è¡¨å¤´çš„ç©ºè¡¨ï¼Œå¹¶å‹å¥½æç¤º
        todo_df = pd.DataFrame(columns=todo_cols)
        todo_df.to_csv(BASE_DIR / "result_mapping_todo.csv",
                       index=False, encoding="utf-8-sig")

        cute_box(
            "æœ¬æ‰¹æ²¡æœ‰äº§ç”Ÿæ–°çš„åˆ«åéœ€è¦æ˜ å°„ï¼›å·²è¢«è§„åˆ™è¯†åˆ«/è¿‡æ»¤ï¼Œæˆ–å› â€œåŒè¡Œå…¬å¸ä¸è¶³ï¼ˆ<2ï¼‰â€è§„åˆ™è€Œè·³è¿‡ã€‚\n"
            f"ï¼ˆban å‘½ä¸­ï¼š{ban_hits}ï¼Œå·²æœ‰ aliasï¼š{alias_hits}ï¼Œå·²æœ‰ canonicalï¼š{canon_hits}ï¼ŒåŒè¡Œå…¬å¸ä¸è¶³è·³è¿‡ï¼š{rows_skipped_not_enough_companies}ï¼‰",
            "ä»Šå›ã®ãƒãƒƒãƒã§ã¯æ–°ã—ã„åˆ¥åã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã«ä¸€è‡´ï¼é™¤å¤–ã€ã¾ãŸã¯ã€ŒåŒä¸€è¡Œã®ä¼æ¥­æ•°ãŒ2æœªæº€ã€è¦å‰‡ã§ã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸã€‚\n"
            f"ban ä¸€è‡´ï¼š{ban_hits}ï¼æ—¢å­˜ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼š{alias_hits}ï¼æ—¢å­˜ã‚«ãƒãƒ‹ã‚«ãƒ«ï¼š{canon_hits}ï¼åŒä¸€è¡Œã®ä¼æ¥­æ•°ä¸è¶³ã‚¹ã‚­ãƒƒãƒ—ï¼š{rows_skipped_not_enough_companies}",
            "â„¹ï¸"
        )
    else:
        # â€”â€” æ­£å¸¸å»é‡ï¼ˆæŒ‰åˆ«åå°å†™ï¼‰
        todo_df = pd.DataFrame(todo_rows)
        todo_df["__alias_l"] = todo_df["Alias"].str.lower()
        todo_df = todo_df.drop_duplicates("__alias_l").drop(columns="__alias_l")

        # åˆ†ç»„æ’åº
        todo_df["__grp"] = todo_df["Bad_Score"].apply(lambda x: 0 if x >= 50 else (1 if x >= 10 else 2))
        todo_df = (todo_df
                   .sort_values(["__grp", "Sentence"], ascending=[True, True])
                   .drop(columns="__grp"))

        # å›ºå®šåˆ—é¡ºåº
        for col in todo_cols:
            if col not in todo_df.columns:
                todo_df[col] = ""   # å…œåº•ï¼Œä¿è¯åˆ—é½å…¨
        todo_df = todo_df[todo_cols]

        # å†™æ–‡ä»¶
        todo_df["Bad_Score"] = todo_df["Bad_Score"].astype(int).astype(str)
        todo_df['Sentence'] = todo_df['Sentence'].apply(
            lambda s: "'" + s if isinstance(s, str) and s.startswith('=') else s
        )
        todo_df.to_csv(BASE_DIR / "result_mapping_todo.csv",
                       index=False, encoding="utf-8-sig")

        cute_box(
            f"å·²ç”Ÿæˆ result_mapping_todo.csvï¼Œå…± {len(todo_df)} æ¡å¾…å¤„ç†åˆ«åã€‚\n"
            f"ï¼ˆban å‘½ä¸­ï¼š{ban_hits}ï¼Œå·²æœ‰ aliasï¼š{alias_hits}ï¼Œå·²æœ‰ canonicalï¼š{canon_hits}ï¼ŒåŒè¡Œå…¬å¸ä¸è¶³è·³è¿‡ï¼š{rows_skipped_not_enough_companies}ï¼‰",
            f"result_mapping_todo.csv ã‚’ä½œæˆï¼š{len(todo_df)} ä»¶ã®å€™è£œã€‚\n"
            f"ï¼ˆban ä¸€è‡´ï¼š{ban_hits}ï¼æ—¢å­˜ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼š{alias_hits}ï¼æ—¢å­˜ã‚«ãƒãƒ‹ã‚«ãƒ«ï¼š{canon_hits}ï¼åŒä¸€è¡Œã®ä¼æ¥­æ•°ä¸è¶³ã‚¹ã‚­ãƒƒãƒ—ï¼š{rows_skipped_not_enough_companies}ï¼‰",
            "ğŸ“"
        )
        
    cute_box(
    "Step-2 å®Œæˆï¼è¯·ç¼–è¾‘ result_mapping_todo.csv ç„¶åè¿è¡Œ Step-3",
    "Step-2 å®Œäº†ï¼result_mapping_todo.csv ã‚’ç·¨é›†ã—ã¦ã‹ã‚‰ Step-3 ã‚’å®Ÿè¡Œã—ã¦ã­",
    "âœ…"
    )
    cute_box(
        "result_mapping_todo.csv å¿«é€Ÿå¡«å†™æŒ‡å—ï¼š\n"
        "1) ç©ºç™½â†’è·³è¿‡\n"
        "2) 0â†’åŠ  ban_list\n"
        "3) nâ†’è§†ä¸º canonical_id\n"
        "4) å…¶ä»–â†’æ–°/å·²æœ‰æ ‡å‡†å",
        "result_mapping_todo.csv ç°¡æ˜“å…¥åŠ›ã‚¬ã‚¤ãƒ‰ï¼š\n"
        "1) ãƒ–ãƒ©ãƒ³ã‚¯â†’ã‚¹ã‚­ãƒƒãƒ—\n"
        "2) 0â†’ban_listç™»éŒ²\n"
        "3) nâ†’canonical_id ã¨è¦‹ãªã™\n"
        "4) ãã®ä»–â†’æ–°è¦/æ—¢å­˜æ¨™æº–å",
        "ğŸ“‹"
    )

# ================ Step-3 ==============

def step3(mysql_url: str):
    """
    Step-3 æ ‡å‡†åŒ– + å†™åº“ï¼ˆä¸æ—§ NA_step3_standardize.py ç­‰ä»·ï¼‰
    - Canonical_Name == ''  â†’ Std_Result = 'No input'
    - Canonical_Name == '0' â†’ å†™ ban_list,  Std_Result = 'Banned'
    - å…¶å®ƒ:
        â€¢ è‹¥å·²å­˜åœ¨ alias â†’ Std_Result = 'Exists'
        â€¢ å¦åˆ™æ’å…¥/è¡¥å…¨ canonical & alias, Std_Result = 'Added'
    åŒæ—¶æŠŠæœ€æ–°æ˜ å°„åº”ç”¨å› result.csv
    """
    # æœ¬è½®æ‰¹æ¬¡å·ï¼šYYYYMMDD + 8ä½éšæœºæ•°
    process_id = datetime.now().strftime("%Y%m%d") + f"{random.randint(0, 99999999):08d}"
    res_f  = BASE_DIR / "result.csv"
    todo_f = BASE_DIR / "result_mapping_todo.csv"
    if not (res_f.exists() and todo_f.exists()):
        cute_box(
            "æ‰¾ä¸åˆ° result.csv æˆ– result_mapping_todo.csvï¼Œè¯·å…ˆç”Ÿæˆå®ƒä»¬",
            "result.csv ã¾ãŸã¯ result_mapping_todo.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ä½œæˆã—ã¦ã­",
            "â—"
        )
        sys.exit(1)

    # è¯»å–ç°æœ‰æ–‡ä»¶
    df_res = pd.read_csv(res_f,  dtype=str).fillna("")
    df_map = pd.read_csv(todo_f, dtype=str).fillna("")
    if "Process_ID" not in df_map.columns:
        df_map["Process_ID"] = ""

    engine = create_engine(mysql_url)
    with engine.begin() as conn:
        # æ‹‰å–ä¸‰è¡¨åˆ°å†…å­˜
        ban_set    = {r[0] for r in conn.execute(text("SELECT alias FROM ban_list"))}
        canon_map  = {r[0]: r[1] for r in conn.execute(text("SELECT id, canonical_name FROM company_canonical"))}
        alias_map  = {r[0]: r[1] for r in conn.execute(text(
            "SELECT a.alias, c.canonical_name FROM company_alias a "
            "JOIN company_canonical c ON a.canonical_id=c.id"
        ))}
        # æ— è§†å¤§å°å†™çš„é•œåƒ
        ban_lower       = {b.lower() for b in ban_set}
        alias_lower_map = {a.lower(): c for a, c in alias_map.items()}
        canon_lower2id  = {name.lower(): cid for cid, name in canon_map.items()}

    # â€”â€” å¤„ç† todo æ˜ å°„ â€”â€”  
    for idx, row in df_map.iterrows():
        alias_raw   = row["Alias"].strip()
        alias_raw_l = alias_raw.lower()
        canon_input = row["Canonical_Name"].strip()
        if not canon_input:
            df_map.at[idx, "Std_Result"] = "No input"
            continue

        # Case A: Ban (0)
        if canon_input == "0":
            if alias_raw_l not in ban_lower:
                try:
                    with engine.begin() as conn:
                        conn.execute(text(
                            "INSERT IGNORE INTO ban_list(alias, process_id) VALUES (:a, :pid)"
                        ), {"a": alias_raw, "pid": process_id})
                    ban_lower.add(alias_raw_l)
                except Exception as e:
                    print(f"âš ï¸ Ban insert skip: {e}")
            df_map.at[idx, "Std_Result"]   = "Banned"
            df_map.at[idx, "Process_ID"] = f"'{process_id}"
            continue

        # Case B: Existing ID (æ•°å­—)
        if canon_input.isdigit():
            cid = int(canon_input)
            if cid not in canon_map:
                df_map.at[idx, "Std_Result"] = "Bad ID"
                continue
            canon_name = canon_map[cid]
            
        # Case C: New/Text Canonical
        else:
            ci_l = canon_input.lower()
            # å¦‚æœå†…å­˜é‡Œæ²¡è¿™ä¸ªå…¬å¸ï¼Œå°è¯•æ’å…¥
            if ci_l not in canon_lower2id:
                try:
                    # --- å°è¯•æ’å…¥æ–°å…¬å¸ ---
                    with engine.begin() as conn:
                        res = conn.execute(text(
                            "INSERT INTO company_canonical(canonical_name, process_id) VALUES (:c, :pid)"
                        ), {"c": canon_input, "pid": process_id})
                    new_id = res.lastrowid
                    
                except Exception as e:
                    # --- ã€å…³é”®ä¿®å¤ã€‘å¦‚æœæŠ¥é”™(é‡å¤)ï¼Œè¯´æ˜æ•°æ®åº“é‡Œå…¶å®å·²ç»æœ‰äº† ---
                    # å¯èƒ½æ˜¯å› ä¸ºé‡éŸ³ç¬¦å·(Ã‰ vs E)å¯¼è‡´ Python æ²¡è®¤å‡ºæ¥ï¼Œä½†æ•°æ®åº“è®¤å‡ºæ¥äº†
                    print(f"âš ï¸ å‘ç°æ½œåœ¨é‡å¤å…¬å¸: {canon_input}ï¼Œå°è¯•ä»æ•°æ®åº“è·å– ID...")
                    with engine.begin() as conn:
                        # å°è¯•ç›´æ¥ç”¨åå­—æŸ¥ ID
                        rows = conn.execute(text(
                            "SELECT id FROM company_canonical WHERE canonical_name = :c"
                        ), {"c": canon_input}).fetchall()
                        
                        if rows:
                            new_id = rows[0][0]
                            print(f"   -> å·²æ‰¾å›ç°æœ‰ ID: {new_id}")
                        else:
                            # æå…¶ç½•è§çš„æƒ…å†µï¼šæ’å…¥æŠ¥é”™ä½†åˆæŸ¥ä¸åˆ°ï¼Œè®°å½•é”™è¯¯è·³è¿‡
                            print(f"âŒ æ— æ³•è§£å†³çš„å†²çªï¼Œè·³è¿‡æ­¤æ¡: {e}")
                            df_map.at[idx, "Std_Result"] = "DB Error"
                            continue

                # æ›´æ–°å†…å­˜å­—å…¸
                canon_map[new_id]        = canon_input
                canon_lower2id[ci_l]     = new_id
                df_map.at[idx, "Process_ID"] = f"'{process_id}"
                canon_name = canon_input
            else:
                # å†…å­˜é‡Œå·²ç»æœ‰äº†ï¼Œç›´æ¥ç”¨
                canon_name = canon_map[canon_lower2id[ci_l]]

        # Case D: Insert Alias
        if alias_raw_l in alias_lower_map or alias_raw_l in canon_lower2id:
            df_map.at[idx, "Std_Result"] = "Exists"
            continue
            
        try:
            with engine.begin() as conn:
                conn.execute(text(
                    "INSERT IGNORE INTO company_alias(alias, canonical_id, process_id) "
                    "VALUES (:a, :cid, :pid)"
                ), {"a": alias_raw, "cid": canon_lower2id[canon_name.lower()], "pid": process_id})
            alias_lower_map[alias_raw_l] = canon_name
            df_map.at[idx, "Std_Result"]   = "Added"
            df_map.at[idx, "Process_ID"] = f"'{process_id}"
        except Exception as e:
            print(f"âš ï¸ Alias insert error: {e}")

    # å…ˆå†™å› todoï¼Œå†åšå›å†™ result.csv
    df_map.to_csv(todo_f, index=False, encoding="utf-8-sig")

    # ====== å°†æœ€æ–°æ˜ å°„åº”ç”¨å› result.csv ======
    # é‡æ–°æ‹‰å– ban/alias/canonical å‡†å¤‡æ˜ å°„
    with engine.begin() as conn2:
        ban_set2    = {r[0] for r in conn2.execute(text("SELECT alias FROM ban_list"))}
        rows2       = conn2.execute(text(
            "SELECT a.alias, c.canonical_name FROM company_alias a "
            "JOIN company_canonical c ON a.canonical_id=c.id"
        ))
        alias_map2  = {a: c for a, c in rows2}
        canon_set2  = {r[0] for r in conn2.execute(text("SELECT canonical_name FROM company_canonical"))}

    ban_lower2        = {b.lower() for b in ban_set2}
    alias_lower_map2  = {a.lower(): c for a, c in alias_map2.items()}
    canon_lower2orig2 = {c.lower(): c for c in canon_set2}

    comp_cols = [c for c in df_res.columns if c.startswith("company_")]

    def _norm_key(s: str) -> str:
        return re.sub(r"[^A-Za-z0-9]", "", str(s)).lower()

    changed_cells = 0
    for ridx in df_res.index:
        # è¯»å‡ºåŸå€¼
        orig = df_res.loc[ridx, comp_cols].astype(str).tolist()
        vals_in = [v.strip() for v in orig if v.strip()]
        vals_out = []
        for nm in vals_in:
            key = nm.lower()
            if key in ban_lower2:
                continue
            if key in alias_lower_map2:
                nm = alias_lower_map2[key]
                changed_cells += 1
                key = nm.lower()
            elif key in canon_lower2orig2:
                corrected = canon_lower2orig2[key]
                if corrected != nm:
                    changed_cells += 1
                nm = corrected
            vals_out.append(nm)
        # åŒæ ¹å»é‡ + å·¦ç§»
        cleaned, seen = [], set()
        for nm in sorted(vals_out, key=len, reverse=True):
            k = _norm_key(nm)
            if any(k in kk or kk in k for kk in seen):
                continue
            cleaned.append(nm)
            seen.add(k)
        # å›å†™
        for i, col in enumerate(comp_cols):
            new_val = cleaned[i] if i < len(cleaned) else ""
            if str(df_res.at[ridx, col]) != new_val:
                changed_cells += 1
            df_res.at[ridx, col] = new_val

    # å†æ¸…ä¸€æ¬¡åˆ—å†…é‡å¤
    df_res = dedup_company_cols(df_res)

    cute_box(
        f"å·²å°†æœ€æ–°æ˜ å°„åº”ç”¨åˆ° result.csvï¼ˆå˜æ›´å•å…ƒæ ¼çº¦ {changed_cells} ä¸ªï¼‰",
        f"æœ€æ–°ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ result.csv ã«é©ç”¨ã—ã¾ã—ãŸï¼ˆå¤‰æ›´ã‚»ãƒ«æ•° ç´„ {changed_cells}ï¼‰",
        "ğŸ› ï¸"
    )

    # æœ€åä¿å­˜
    df_res.to_csv(res_f, index=False, encoding="utf-8-sig")

    cute_box(
        f"Step-3 å®Œæˆï¼Œå¤„ç† {len(df_map)} æ¡æ˜ å°„ï¼Œresult.csv å·²æ›´æ–°",
        f"Step-3 å®Œäº†ï¼š{len(df_map)}ä»¶ å‡¦ç†æ¸ˆã¿ï¼Œresult.csv æ›´æ–°å®Œäº†",
        "ğŸš€"
    )
    cute_box(
        f"æœ¬æ‰¹æ¬¡ Process IDï¼š{process_id}",
        f"ä»Šå›ã® Process IDï¼š{process_id}",
        "ğŸ“Œ"
    )
               
# ---------- ã€æ–°å¢ã€‘é€‰é¡¹ 3ï¼šGPT è‡ªåŠ¨å¡«å……åŠŸèƒ½ ----------

def ask_gpt_batch(batch_data: List[Dict], api_key: str) -> Dict:
    client = OpenAI(api_key=api_key)
    prompt = f"""
    You are a data cleaning expert for business strategy research. 
    Analyze the list of "alias" strings.

    Task: Determine the [Organizational Entity] behind the alias.
    
    [Allowed Categories] -> Set "is_company": true
    1. Commercial Companies (e.g., Toyota, Google, OpenAI)
    2. Educational Institutions (e.g., Harvard University, Tokyo High School)
    3. Government Bodies & Municipalities (e.g., Osaka Prefecture, Ministry of Economy)
    4. NGOs, NPOs, Associations (e.g., Red Cross, IEEE)
    
    [Special Mapping Rules for Products & IPs] -> Set "is_company": true
    If the 'alias' is a Product, Service, or Fictional Character/IP, DO NOT reject it. instead, map it to its OWNER Company.
    Examples:
    - "iPhone" -> is_company: true, clean_name: "Apple"
    - "ChatGPT" -> is_company: true, clean_name: "OpenAI"
    - "Mickey Mouse" -> is_company: true, clean_name: "Disney"
    - "Mario" -> is_company: true, clean_name: "Nintendo"
    - "Barbie" -> is_company: true, clean_name: "Mattel"

    [Forbidden Categories] -> Set "is_company": false
    1. General Nouns / Not Proper Nouns (e.g., "external researchers", "local governments", "our partners", "the committee", "anime", "video games")
    2. Job Titles / Departments (e.g., "CEO", "Sales Department")
    3. Individuals (unless the name refers to a sole proprietorship/studio)

    Rules for "clean_name":
    - Remove legal suffixes (Inc., Ltd., Corp., K.K., etc.).
    - If it is a Product/IP, use the OWNER Company Name.
    - Keep the full proper name (e.g., "University of Tokyo" -> "University of Tokyo").
    
    Input: {json.dumps(batch_data, ensure_ascii=False)}
    
    Output JSON format:
    {{
        "alias_original_text": {{ 
            "is_company": bool, 
            "clean_name": str, 
            "matches_advice": bool // If the mapped company matches the provided 'advice' entity
        }}
    }}
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0
        )
        return json.loads(response.choices[0].message.content)
    except:
        return {}

def step_ai_autofill():
    """
    è¯»å– result_mapping_todo.csvï¼Œåˆ©ç”¨ GPT è‡ªåŠ¨å¡«å†™ Canonical_Name åˆ—
    åŒ…å«è‡ªåŠ¨ä¿å­˜ API Key çš„åŠŸèƒ½
    """
    csv_path = BASE_DIR / "result_mapping_todo.csv"
    if not csv_path.exists():
        cute_box("æ‰¾ä¸åˆ° result_mapping_todo.csvï¼", "ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", "âŒ")
        return

    # --- ã€æ–°å¢ã€‘è‡ªåŠ¨è¯»å–/ä¿å­˜ Key çš„é€»è¾‘ ---
    key_file = BASE_DIR / ".openai_key"
    api_key = ""
    
    if key_file.exists():
        api_key = key_file.read_text().strip()
        print(f"ğŸ”‘ å·²è‡ªåŠ¨åŠ è½½ä¿å­˜çš„ API Key: {api_key[:8]}...")
    
    if not api_key:
        api_key = input("è¯·è¾“å…¥ OpenAI API Key (sk-...) / APIã‚­ãƒ¼ã‚’å…¥åŠ›: ").strip()
        if api_key:
            # ä¿å­˜åˆ°æ–‡ä»¶
            key_file.write_text(api_key)
            print("ğŸ’¾ API Key å·²ä¿å­˜ï¼Œä¸‹æ¬¡æ— éœ€è¾“å…¥ã€‚")
    # ----------------------------------------

    if not api_key:
        print("âŒ æœªè¾“å…¥ Keyï¼Œæ“ä½œå–æ¶ˆã€‚")
        return

    print("â³ æ­£åœ¨è¯»å– CSV...")
    df = pd.read_csv(csv_path, dtype=str).fillna("")
    
    # ... (åç»­é€»è¾‘ä¿æŒä¸å˜) ...
    
    # 2. ç­›é€‰å‡ºéœ€è¦å¤„ç†çš„è¡Œ (Canonical_Name ä¸ºç©ºçš„è¡Œ)
    mask = df["Canonical_Name"] == ""
    rows_to_process = df[mask]
    
    if rows_to_process.empty:
        print("âœ¨ æ‰€æœ‰è¡Œçš„ Canonical_Name éƒ½å·²å¡«å¥½ï¼Œæ— éœ€å¤„ç†ï¼")
        return

    print(f"ğŸ¤– å‡†å¤‡å¤„ç† {len(rows_to_process)} æ¡æ•°æ®...")
    
    # 3. åˆ†æ‰¹å¤„ç† (æ¯æ‰¹ 30 æ¡)
    batch_size = 30
    updates = {} # æš‚å­˜ç»“æœ {index: canonical_value}
    
    data_list = []
    for idx, row in rows_to_process.iterrows():
        data_list.append({
            "index": idx, # è®°ä½åŸå§‹è¡Œå·
            "alias": row["Alias"],
            "advice": row["Advice"]
        })

    for i in tqdm(range(0, len(data_list), batch_size), desc="GPT Cleaning"):
        batch = data_list[i : i + batch_size]
        
        gpt_input = [{"alias": item["alias"], "advice": item["advice"]} for item in batch]
        
        # è°ƒç”¨ API
        gpt_res = ask_gpt_batch(gpt_input, api_key)
        
        # è§£æç»“æœå¹¶å†³å®šå¡«ä»€ä¹ˆ
        for item in batch:
            alias = item["alias"]
            idx = item["index"]
            
            adv_id = df.at[idx, "Adviced_ID"]
            
            if alias in gpt_res:
                res = gpt_res[alias]
                
                if not res.get("is_company", False):
                    updates[idx] = "0"
                else:
                    if df.at[idx, "Advice"] and df.at[idx, "Adviced_ID"] and res.get("matches_advice", False):
                        updates[idx] = df.at[idx, "Adviced_ID"]
                    else:
                        updates[idx] = res.get("clean_name", alias)

    print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœ...")
    for idx, val in updates.items():
        df.at[idx, "Canonical_Name"] = val
        
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    cute_box(
        f"âœ… è‡ªåŠ¨å¡«å†™å®Œæˆï¼å·²æ›´æ–° {len(updates)} è¡Œ", 
        f"è‡ªå‹•å…¥åŠ›å®Œäº†ï¼{len(updates)} è¡Œã‚’æ›´æ–°ã—ã¾ã—ãŸ", 
        "ğŸ‰"
    )

def step4():
    import pandas as _pd

    # 1) è¯» CSV
    df = _pd.read_csv(BASE_DIR / "result.csv", dtype=str).fillna("")

    # 2) å‡†å¤‡è¾“å‡ºè¡Œï¼šæ³¨æ„è¿™é‡Œç»™æ¯ä¸€æ¡éƒ½åŠ ä¸Š value=1
    rows = []
    for _, r in tqdm(df.iterrows(), desc="ç”Ÿæˆé‚»æ¥è¡¨", total=len(df)):
        comps = [r[f"company_{i}"] 
                 for i in range(1, MAX_COMP_COLS+1) 
                 if r[f"company_{i}"].strip()]
        for a, b in itertools.permutations(comps, 2):
            rows.append({
                "company_a": a,
                "company_b": b,
                "value": 1,
            })

    # 3) æ„å»ºå®Œæ•´ DataFrame
    out = _pd.DataFrame(rows)

    # 4) å†™ adjacency list ï¼ˆåªä¿ç•™ a/b ä¸¤åˆ—ï¼‰
    out[['company_a','company_b']].to_csv(
        BASE_DIR / "result_adjacency_list.csv",
        index=False, encoding="utf-8-sig"
    )
    cute_box(
        "Step4 å·²ç”Ÿæˆé‚»æ¥è¡¨ï¼šresult_adjacency_list.csv",
        "Step4 éš£æ¥ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼šresult_adjacency_list.csv",
        "ğŸ“‹"
    )

    # â€”â€”â€” ç”Ÿæˆå¸¦è¡Œåˆ—æ ‡é¢˜çš„ Pivot Table â€”â€”â€”
    pivot = out.pivot_table(
        index="company_a",      # è¡Œæ ‡ç­¾
        columns="company_b",    # åˆ—æ ‡ç­¾
        values="value",         # èšåˆå­—æ®µ
        aggfunc="sum",          # æŠŠæ‰€æœ‰ value=1 ç´¯åŠ 
        fill_value=""           # 0 æˆ– NaN éƒ½æ˜¾ç¤ºç©ºç™½
    )

    # 5) å¯¼å‡ºå¸¦è¡Œ/åˆ—æ ‡é¢˜çš„çŸ©é˜µ
    pivot.to_csv(
        BASE_DIR / "pivot_table.csv",
        encoding="utf-8-sig"
    )
    cute_box(
        "Step4 å·²ç”Ÿæˆé€è§†è¡¨ï¼špivot_table.csv",
        "Step4 ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸï¼špivot_table.csv",
        "ğŸ“Š"
    )
def main():
    # 1. è¿æ¥æ•°æ®åº“
    mysql_url = ask_mysql_url()
    try:
        create_engine(mysql_url).connect().close()
        print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ / ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ")
    except Exception as e:
        cute_box(f"æ•°æ®åº“è¿æ¥å¤±è´¥ï¼š{e}", f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š å¤±æ•—ï¼š{e}", "âŒ")
        sys.exit(1)
        
    # 2. é…ç½®å…³é”®è¯æ¨¡å¼
    configure_keywords()
    
    # 3. ä¸»èœå•å¾ªç¯
    while True:
        choice = choose()

        if choice == "1":
            # --- é˜¶æ®µä¸€ï¼šæå– ---
            step1()
            step2(mysql_url)
            
            # æ ‡è®°ï¼šæ˜¯å¦å·²ç»è·‘è¿‡ AI æ¸…æ´—
            ai_cleaned_done = False

            # è·‘å®Œ Step 1-2 åï¼Œè¿›å…¥å­èœå•
            while True:
                print("\n" + "="*60)
                
                if not ai_cleaned_done:
                    # --- çŠ¶æ€ Aï¼šåˆšè·‘å®Œæå–ï¼Œè¿˜æ²¡æ¸…æ´— ---
                    print("ğŸ‰ [Step 1-2] å®Œæˆ / å®Œäº†")
                    print("   æ–‡ä»¶å·²ç”Ÿæˆ: result_mapping_todo.csv")
                    print("   ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†: result_mapping_todo.csv")
                    print("-" * 60)
                    print("ğŸ‘‰ æ¥ä¸‹æ¥å»ºè®®åšä»€ä¹ˆï¼Ÿ/ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼š")
                    print("   [a] ğŸ¤– è¿è¡Œ AI è‡ªåŠ¨åå¯„ã› (å¼ºçƒˆæ¨è) / AIè‡ªå‹•åå¯„ã›ã‚’å®Ÿè¡Œ [æ¨å¥¨]")
                    print("   [b] âš ï¸ è·³è¿‡æ¸…æ´—ï¼Œç›´æ¥å…¥åº“ãƒ»åˆ†æãƒ»çµæœå‡ºåŠ› / ãã®ã¾ã¾DBç™»éŒ²ã¸é€²ã‚€ãƒ»åˆ†æãƒ»çµæœå‡ºåŠ›")
                else:
                    # --- çŠ¶æ€ Bï¼šå·²ç»è·‘å®Œ AIåå¯„ã› ---
                    print("âœ¨ [Step 2.5] AIåå¯„ã›å·²å®Œæˆ / AIåå¯„ã›å®Œäº†")
                    print("   è¯·æ‰“å¼€ result_mapping_todo.csv ç®€å•æ£€æŸ¥ä¸€ä¸‹ï¼Œç¡®è®¤æ— è¯¯åç»§ç»­ã€‚")
                    print("   åå¯„ã›å®Œäº†ã®result_mapping_todo.csvã‚’ç¢ºèªã—ã€å•é¡Œãªã‘ã‚Œã°æ¬¡ã¸é€²ã‚“ã§ãã ã•ã„ã€‚")
                    print("-" * 60)
                    print("ğŸ‘‰ ä¸‹ä¸€æ­¥ / Next Stepï¼š")
                    print("   [b] ğŸš€ ç¡®è®¤æ— è¯¯ï¼Œæ‰§è¡Œå…¥åº“ãƒ»åˆ†æãƒ»çµæœå‡ºåŠ› / ç¢ºèªOKã€DBç™»éŒ²ãƒ»åˆ†æãƒ»çµæœå‡ºåŠ›")
                    print("   [a] ğŸ”„ ä¸æ»¡æ„ï¼Œé‡è·‘ AI æ¸…æ´— / ã‚‚ã†ä¸€åº¦AIã‚’å®Ÿè¡Œ")

                print("   [e] ğŸ‘‹ é€€å‡ºç¨‹åº / ä¸€æ—¦çµ‚äº†")
                print("="*60)
                
                sub_c = input("Input [a/b/e]: ").strip().lower()
                
                if sub_c == "a":
                    step_ai_autofill()
                    ai_cleaned_done = True # æ ‡è®°ä¸ºå·²æ¸…æ´—
                    
                elif sub_c == "b":
                    step3(mysql_url)
                    step4()
                    print("ğŸ‰ å®Œæˆï¼ result_adjacency_list.csvã‚„pivot_table.csvã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€œ")
                    sys.exit(0) # å…¨éƒ¨å®Œæˆï¼Œé€€å‡º
                    
                elif sub_c == "e":
                    print("ğŸ‘‹ Bye!")
                    sys.exit(0)

        elif choice == "2":
            # --- é˜¶æ®µäºŒï¼šå•ç‹¬è¿è¡Œ AI æ¸…æ´— ---
            step_ai_autofill()
            print("\nâœ… å®Œæˆã€‚æ‚¨å¯ä»¥é€‰æ‹© [3] è¿›è¡Œå…¥åº“ï¼Œæˆ–è¾“å…¥ [e] é€€å‡ºã€‚\nâœ… å®Œæˆã€‚ [3] ã§DBç™»éŒ²ãƒ»åˆ†æãƒ»çµæœå‡ºåŠ›ã€ã‚‚ã—ãã¯ [e] ã§çµ‚äº†ã€‚")
            # è¿™é‡Œå¯ä»¥ä¸å¼ºåˆ¶è·³è½¬ï¼Œè®©ç”¨æˆ·è‡ªå·±é€‰

        elif choice == "3":
            # --- é˜¶æ®µä¸‰ï¼šå…¥åº“ä¸åˆ†æ ---
            step3(mysql_url)
            step4()
            print("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å·²å®Œæˆ / å…¨ã¦ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¾ã—ãŸ")
            sys.exit(0)

if __name__ == "__main__":
    main()

# åŸä½œè€…ï¼šæ¨å¤©ä¹@å…³è¥¿å¤§å­¦ / Author: Shiame Yeung@Kansai University / ä½œæˆè€…ï¼šæ¥Šã€€å¤©æ¥½@é–¢è¥¿å¤§å­¦
#!/usr/bin/env python3
# coding: utf-8
"""
na_pipeline.py  â€”â€”  å•æ–‡ä»¶ç‰ˆï¼ˆStepâ€‘1 å¯¹é½ + æ‰©å±•å…¬å¸è¯†åˆ«ï¼‰
2025â€‘07â€‘08  revâ€‘C
"""

import os, re, sys, unicodedata, string
from pathlib import Path
from typing import List, Dict, Set

from datetime import datetime
import random

import pandas as pd
from tqdm import tqdm
from sqlalchemy import create_engine, text
from rapidfuzz import fuzz, process

try:
    from docx import Document
    import spacy
    nlp = spacy.load("en_core_web_sm", disable=["parser", "lemmatizer"])
except Exception:
    print("âŒ ç¼ºå°‘ä¾èµ–ï¼špip install python-docx spacy && python -m spacy download en_core_web_sm"); sys.exit(1)

# ---------------- å¸¸é‡ ----------------
STOPWORDS = {"the","and","for","with","from","that","this","have","will","are","you","not","but","all","any","one","our","their"}
KEYWORD_ROOTS = [
    'partner','alliance','collaborat','cooper','cooperat','join','merger','acquisiti',
    'outsourc','invest','licens','integrat','coordinat','synergiz','associat',
    'confedera','federa','union','unit','amalgamat','conglomerat','combin',
    'buyout','companion','concur','concert','comply','complement','assist',
    'takeover','accession','procure','suppl','conjoint','support','adjust',
    'adjunct','patronag','subsid','affiliat','endors'
]
# ---------------- Bad-Rate è§„åˆ™ ----------------
ORG_SUFFIX  = re.compile(
    r'\b(Inc\.?|Corp\.?|Corporation|Ltd\.?|LLC|PLC|AG|NV|SA|GmbH|S\.p\.A|Co\.?|Company|'
    r'Group|Holdings?|Partners?|Capital|Ventures?|Bank|Trust|Software|'
    r'Technolog(?:y|ies)|Pharma(?:ceuticals)?|Systems?|Services?|'
    r'Industr(?:y|ies)|Foundation|Laborator(?:y|ies)|'
    r'University|College|Institute|School|Hospital|Center|Centre)\b',
    re.I)

TIME_QTY    = re.compile(
    r'\b(year|month|week|day|decade|centur(?:y|ies)|quarter|q[1-4]|'
    r'ago|last|next|few|couple|several|dozen|half|around|approximately)s?\b',
    re.I)
ART_LOWER   = re.compile(r'^\s*(a|an|about|approximately|the|this|that|those)\s+[a-z]')
GENERIC_END = re.compile(
    r'\b(plan|plans?|programs?|systems?|platforms?|services?|solutions?|operations?|'
    r'agreements?|strategies?|reports?|statements?)$', re.I)

def _lower_ratio(text: str) -> float:
    w = text.split()
    return sum(t[0].islower() for t in w) / len(w) if w else 0

def calc_bad_rate(text: str) -> int:
    """0â€“100ï¼šè¶Šé«˜è¶Šå¯èƒ½æ˜¯ bad"""
    if ORG_SUFFIX.search(text):
        return 0
    score = 0
    if TIME_QTY.search(text):              score += 40
    if ART_LOWER.match(text):              score += 25
    if ' of the ' in text.lower():         score += 20
    if GENERIC_END.search(text):           score += 15
    if len(text.split()) <= 2:             score += 20
    if _lower_ratio(text) > 0.30:          score += 20
    return min(score, 100)
# ---------------- å…¨å±€å˜é‡ ----------------
BASE_DIR = Path(__file__).resolve().parent
MAX_COMP_COLS = 50
SENTENCE_RECORDS: List[Dict] = []

# ---------------- å…±ç”¨ ----------------



# ------- æ–°ç‰ˆæœ¬ï¼šé¦–æ¬¡è¾“å…¥åå†™ .db_keyï¼Œåç»­è‡ªåŠ¨è¯»å– -------
def ask_mysql_url() -> str:
    key_file = Path(__file__).with_name(".db_key")   # è„šæœ¬åŒç›®å½• .db_key
    if key_file.exists():
        key = key_file.read_text().strip()
    else:
        key = input("è¯·è¾“å…¥ç§˜é’¥/ã‚­ãƒ¼ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ï¼šuser:pass@host\n>>>>>> ").strip()
        key_file.write_text(key)                     # ç¼“å­˜ä¸‹æ¬¡ç”¨
    return f"mysql+pymysql://{key}.mysql.rds.aliyuncs.com:3306/na_data?charset=utf8mb4"

def choose() -> str:
    # â”€â”€ 1. é€‰é¡¹æ¡† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(r"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â‘   åˆæ¬¡è¿è¡Œï¼ˆStep-1 âœ Step-2ï¼‰ / åˆå›å®Ÿè¡Œ      â”‚
â”‚  â‘¡  å·²æœ‰ mappingï¼ˆStep-3ï¼‰ / mapping é©ç”¨ã®ã¿  â”‚
â”‚  ä½œè€…ï¼šæ¥Šã€€å¤©æ¥½ï¼ é–¢è¥¿å¤§å­¦ã€€ä¼Šä½ç”°ç ”ç©¶å®¤             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
    c = input("è¯·è¾“å…¥ 1 æˆ– 2 / 1 ã‹ 2 ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()

    # â”€â”€ 2. æ ¡éªŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if c not in {"1", "2"}:
        print(r"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âŒ æ— æ•ˆé€‰æ‹© / ç„¡åŠ¹ãªé¸æŠã§ã™  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")
        sys.exit(1)

    return c

def dedup_company_cols(df: pd.DataFrame) -> pd.DataFrame:
    comp_cols = [c for c in df.columns if c.startswith("company_")]
    for ridx in df.index:
        seen: Set[str] = set()
        for col in comp_cols:
            val = str(df.at[ridx, col]).strip()
            if val in seen:
                df.at[ridx, col] = ""
            else:
                seen.add(val)
    return df

# ---------------- Stepâ€‘1 ----------------

def _normalize(text: str) -> str:
    t = re.sub(r"\s+", " ", text.lower().strip())
    t = re.sub(r"^[\-:\"']+|[\-:\"']+$", "", t)
    t = re.sub(r"[,.;/()]+", "", t)
    return t.strip()

def clean_text(t: str) -> str:
    return ''.join(c for c in t if unicodedata.category(c)[0] != 'C' or c in ('\n', '\t'))

def extract_sentences(path: Path) -> List[str]:
    doc = Document(path)
    collecting, current, articles = False, "", []
    for p in doc.paragraphs:
        txt = p.text.strip()
        if not txt: continue
        tag = txt.lower()
        if tag == "body": collecting, current = True, ""; continue
        if tag in ("notes", "classification") and collecting:
            collecting = False; articles.append(current.strip()); continue
        if collecting: current += " " + txt
    sents = []
    for art in articles:
        for s in re.split(r"\.\s*", art):
            s = s.strip();
            if len(s) >= 5: sents.append(s)
    return sents

def extract_index_titles(paragraphs):
    paras_text = [p.text.strip() for p in paragraphs]
    m = re.search(r'Documents?\s*\(\s*(\d+)\s*\)', '\n'.join(paras_text), re.I)
    if not m: return []
    total = int(m.group(1)); pat = re.compile(r'^(\d+)\.\s+(.*)$'); seen, titles = set(), []
    for line in paras_text:
        m2 = pat.match(line)
        if m2:
            raw = m2.group(2).strip(); norm = _normalize(raw)
            if norm in seen: continue
            seen.add(norm); titles.append((int(m2.group(1)), raw, norm))
            if len(titles) >= total: break
    return sorted(titles, key=lambda x: x[0])

def extract_sentences_by_titles(filepath: str) -> List[Dict]:
    doc = Document(filepath); paras = doc.paragraphs
    index_titles = extract_index_titles(paras); recs = []
    if index_titles:
        paras_norm = [_normalize(p.text) for p in paras]
        for _, title_raw, title_norm in index_titles:
            try: match_idx = next(i for i, n in enumerate(paras_norm) if n == title_norm)
            except StopIteration: continue
            pub_idx = match_idx + 1; publisher = paras[pub_idx].text.strip() if pub_idx < len(paras) else ""
            body_start = next((i+1 for i in range(match_idx+1,len(paras)) if paras[i].text.strip().lower()=="body"), None)
            if body_start is None: body_start = pub_idx + 1
            body_end = next((i for i in range(body_start, len(paras)) if paras[i].text.strip().lower() in ("notes","classification")), len(paras))
            article = " ".join(clean_text(paras[i].text) for i in range(body_start, body_end))
            for sent in [s.strip() for s in re.split(r"\.\s*", article) if len(s.strip())>=5]:
                hits = [k for k in KEYWORD_ROOTS if k in sent.lower()]
                recs.append({"Title":title_raw,"Publisher":publisher,"Country":"","Sentence":sent,"Hit_Count":len(hits),"Matched_Keywords":"; ".join(hits)})
        if recs: return recs
    # æ— ç´¢å¼•
    for sent in extract_sentences(Path(filepath)):
        hits=[k for k in KEYWORD_ROOTS if k in sent.lower()]
        recs.append({"Title":"","Publisher":"","Country":"","Sentence":sent,"Hit_Count":len(hits),"Matched_Keywords":"; ".join(hits)})
    return recs

def step1():
    print("\nâ–¶ Step-1ï¼šæå– Word å¥å­ / Word æ–‡ã‹ã‚‰æ–‡æŠ½å‡ºä¸­â€¦")
    all_recs: List[Dict] = []

    # 1) æ”¶é›†æ‰€æœ‰ .docx è·¯å¾„
    docx_files = []
    for root, _, files in os.walk(BASE_DIR):
        for fname in files:
            if not fname.endswith(".docx") or fname.startswith("~$"):
                continue
            full = Path(root) / fname
            rel = full.relative_to(BASE_DIR).parts
            tier1 = rel[0] if len(rel) >= 1 else ""
            tier2 = rel[1] if len(rel) >= 2 else ""
            docx_files.append((str(full), tier1, tier2, fname))

    # 2) é€æ–‡ä»¶æå–å¥å­
    for fp, t1, t2, fname in tqdm(docx_files, desc="ğŸ“„ å¤„ç† Word æ–‡ä»¶"):
        for r in extract_sentences_by_titles(fp):
            if not r["Title"]:
                r["Title"] = Path(fname).stem
            r.update({"Tier_1": t1, "Tier_2": t2, "Filename": fname})
            all_recs.append(r)

    global SENTENCE_RECORDS
    SENTENCE_RECORDS = all_recs
    print(f"âœ” Step-1 å®Œæˆ / å®Œäº†ï¼šå…± {len(all_recs)} æ¡è®°å½• / ä»¶ï¼ˆå·²ç¼“å­˜ / ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ï¼‰")

# ----------------â€”â€” Stepâ€‘2 â€”â€”----------------

def is_valid_token(token: str) -> bool:
    token = token.strip()
    if "@" in token or token.startswith("http"):    # â‘  å«é‚®ç®± / URL ç‰¹å¾
        return False
    if not token or all(c in "-â€“â€”ãƒ».ã€ã€‚ï¼ï¼Ÿï¼ãƒ¼" for c in token):
        return False
    if re.search(r"\d", token) and not re.search(r"[A-Za-z]", token):
        return False
    if "  " in token:
        return False
    return True


# â€”â€” 4. åŸå§‹ä¼ä¸šåæå– â€”â€”
# â€”â€” 4. åŸå§‹ä¼ä¸šåæå– â€”â€”
def extract_companies(text: str,
                      company_db: List[str],
                      ner_model,
                      fuzzy_threshold: int = 95) -> List[str]:
    """
    Â· **ä»…è´Ÿè´£â€œæŠŠå¥å­é‡Œå¯èƒ½æ˜¯å…¬å¸åçš„ç‰‡æ®µå…¨éƒ¨æŠ“å‡ºæ¥â€**ï¼Œ
      ä¸åšä»»ä½• ban/æ˜ å°„/å»é‡å¤„ç†â€”â€”è¿™äº›ç•™ç»™åç»­æ•°æ®åº“æ¯”å¯¹é˜¶æ®µå®Œæˆã€‚
    Â· è¯†åˆ«é€»è¾‘å®Œå…¨æ²¿ç”¨å•ä½“ç‰ˆï¼ˆspaCy NER + â€œIBMersâ€æ­£åˆ™ + ä¸¥æ ¼æ¨¡ç³ŠåŒ¹é…ï¼‰ã€‚
    """
    comps: Set[str] = set()

    # 1) å»æ‰æ—¥æœŸï¼ˆæ’é™¤ â€˜xx/xx/xxxx ä¹‹åæ•´æ®µâ€™ çš„å™ªéŸ³ï¼‰
    text_clean = re.sub(r"\s*\d{1,2}/\d{1,2}/\d{2,4}.*$", "", text).strip()
    # --- æ–°å¢æ¸…æ´— ---
    # 1) å»æ‰ Â® â„¢ Â©
    text_clean = re.sub(r"[Â®â„¢Â©]", "", text_clean)
    # 2) å»æ‰ç®€å†™å•†æ ‡æ‹¬å·ï¼Œå¦‚ â€œWeight Doctors(R)â€
    text_clean = re.sub(r"\(\s*[A-Z]{1,3}\s*\)", "", text_clean)
    # 3) æ•´å¥é‡Œå¸¦é‚®ç®±çš„ç›´æ¥å‰ªæ‰é‚®ç®±
    text_clean = re.sub(r"\b\S+@\S+\b", "", text_clean)

    # 2) spaCy NER
    doc = ner_model(text_clean)
    for ent in doc.ents:
        ent_text = ent.text.strip()

        # â€”â€” åŸºç¡€å™ªéŸ³è¿‡æ»¤ï¼ˆå’Œå•ä½“ç‰ˆä¸€è‡´ï¼‰
        if "  " in ent_text or re.search(r"[\d/%+]|[^\x00-\x7F]", ent_text):
            continue
        valid_ent = True
        for w in ent_text.split():
            if (not w[0].isalpha()
                or w in {"The","And","For","With","From","That","This"}
                or not is_valid_token(w)):
                valid_ent = False
                break
        if valid_ent:
            comps.add(ent_text)

    # 3) â€œIBMersâ€ ä¸€ç±»å†™æ³•
    for m in re.findall(r"\b([A-Z]{2,})ers\b", text_clean):
        comps.add(m)

    # 4) ä»…ç”¨äºâ€œç¡®è®¤æ˜¯å·²çŸ¥å…¬å¸â€ï¼Œä½†ä¾æ—§è¿”å›åŸè¯
    STOPWORDS = {"The","And","For","With","From","That","This","Have","Will",
                "Are","You","Not","But","All","Any","One","Our","Their"}

    tokens = re.findall(r"\b\S+\b", text_clean)
    for pos, token in enumerate(tokens):
        # â€”â€” å™ªéŸ³ä¸æ ¼å¼è¿‡æ»¤ï¼ˆåŒåŸå…ˆé€»è¾‘ï¼‰ â€”â€”
        if (pos == 0 or token in STOPWORDS
            or any(ch in token for ch in "/%+") or "  " in token
            or len(token) < 5 or not token[0].isupper() or token.isupper()
            or re.search(r"\d|[^\x00-\x7F]", token)
            or not is_valid_token(token)):
            continue

        # è‹¥æ•°æ®åº“é‡Œå­˜åœ¨â€œå®Œå…¨åŒåï¼ˆå¤§å°å†™ä¸åŒè§†åŒï¼‰â€çš„æ¡ç›®ï¼Œå°±ä¿ç•™ï¼›å¦åˆ™å¿½ç•¥
        if any(token.lower() == db.lower() for db in company_db):
            comps.add(token)

    return list(comps)


def step2(mysql_url: str):
    print("\\nâ–¶ Step-2ï¼šå…¬å¸è¯†åˆ« + ban è¿‡æ»¤ / ä¼æ¥­åèªè­˜ï¼‹BAN ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°â€¦")
    # å•ç‹¬å¯¼å‡º canonical è¡¨ï¼ˆengine_tmpï¼‰
    engine_tmp = create_engine(mysql_url)            # â† æ–°å»º
    df_canon = pd.read_sql("SELECT id, canonical_name FROM company_canonical", engine_tmp)
    df_canon.to_csv(BASE_DIR / "canonical_list.csv", index=False, encoding="utf-8-sig")
    print(f"  Â· canonical_list.csv å·²å†™ / ä¿å­˜ {len(df_canon)} è¡Œ / è¡Œ")
    # ---- è¿æ¥æ•°æ®åº“ ----
    engine = create_engine(mysql_url)
    with engine.begin() as conn:
        ban_set = {r[0] for r in conn.execute(text("SELECT alias FROM ban_list"))}
        rows = conn.execute(text("""
            SELECT a.alias, c.canonical_name FROM company_alias a
            JOIN company_canonical c ON a.canonical_id = c.id
        """))
        alias_map = {alias: canon for alias, canon in rows}
        canon_set = {r[0] for r in conn.execute(text("SELECT canonical_name FROM company_canonical"))}
        # â†“â†“â†“ æ–°å¢ï¼šåå­—â†’ID çš„å­—å…¸ï¼Œç”¨äº Advice å¯¹åº”çš„ ID
        rows2 = conn.execute(text(
            "SELECT id, canonical_name FROM company_canonical"
        ))
        canon_name2id = {name: cid for cid, name in rows2}      # â† æ–°å¢
    print(f"  Â· ban_list {len(ban_set)} æ¡ / ä»¶ï¼Œalias_map {len(alias_map)} æ¡ / ä»¶ï¼Œcanon_set {len(canon_set)} æ¡ / ä»¶")

    df = pd.DataFrame(SENTENCE_RECORDS)
    df_hit = df[df["Hit_Count"].astype(int) >= 1].reset_index(drop=True)
    if df_hit.empty:
        print("âŒ Step-1 æœªæå–åˆ°å¥å­ï¼Œæ— æ³•ç»§ç»­ Step-2 / Step-1 ã§æ–‡ãŒå–å¾—ã§ããšã€Step-2 ã‚’ç¶šè¡Œã§ãã¾ã›ã‚“"); return

    company_db = list(canon_set) + list(alias_map.keys())   # canonical + alias
    comp_cols: List[List[str]] = []
    for sent in tqdm(df_hit["Sentence"].tolist(), desc="å…¬å¸è¯†åˆ«"):
        names_raw = extract_companies(sent, company_db, nlp)
        uniq: List[str] = []
        for alias in names_raw:
            if alias in uniq:     
                continue
            uniq.append(alias)                         # ä¿ç•™å¥é¢åŸè¯ï¼Œä¸åšä»»ä½•æ›¿æ¢
        comp_cols.append(uniq[:MAX_COMP_COLS])

    for i in range(MAX_COMP_COLS):
        df_hit[f"company_{i+1}"] = [lst[i] if i < len(lst) else "" for lst in comp_cols]
        
# === â‘¢ å…ˆæŒ‰æ•°æ®åº“è§„åˆ™å¤„ç†æ¯è¡Œ company_n åˆ— ===
    ban_lower     = {b.lower() for b in ban_set}
    canon_lower   = {c.lower() for c in canon_set}
    alias_lower   = {a.lower(): canon for a, canon in alias_map.items()}
    canon_lower2orig = {c.lower(): c for c in canon_set}
    # â€”â€” æ ‡å‡†åŒ–ï¼šå»æ‰æ‰€æœ‰éå­—æ¯æ•°å­—ï¼Œå†å°å†™
    def _norm_key(s: str) -> str:
        return re.sub(r"[^A-Za-z0-9]", "", s).lower()

    comp_cols = [f"company_{i+1}" for i in range(MAX_COMP_COLS)]

    for ridx in df_hit.index:
        orig_names = [df_hit.at[ridx, c].strip() for c in comp_cols if df_hit.at[ridx, c].strip()]
        new_names  = []
        for nm in orig_names:
            nm_l = nm.lower()
            # â‘  ban â†’ ä¸¢å¼ƒ
            if nm_l in ban_lower:
                continue
            # â‘¡ å·²æ˜¯æ ‡å‡†å â†’ ä¿ç•™åŸæ ·
            if nm_l in canon_lower:
                new_names.append(canon_lower2orig[nm_l])
                continue
            # â‘¢ åˆ«å â†’ æ›¿æ¢ä¸ºå¯¹åº” canonical
            if nm_l in alias_lower:
                new_names.append(alias_lower[nm_l])
                continue
            # â‘£ æœªçŸ¥ â†’ åŸæ ·
            new_names.append(nm)

        # â‘¤ é¡ºä½å·¦ç§» + â€œåŒæ ¹â€ å»é‡
        cleaned = []
        seen_keys = set()
        for nm in sorted(new_names, key=len, reverse=True):           # å…ˆé•¿åçŸ­
            key = _norm_key(nm)
            # 1) ä¸å·²é€‰ä»»ä½•åç§° key å‰ç¼€ / åç¼€ ç›¸åŒ â†’ è§†ä¸ºé‡å¤
            if any(key in k or k in key for k in seen_keys):
                continue
            cleaned.append(nm)
            seen_keys.add(key)
        # â‘¥ å†™å›è¡Œï¼ˆä¸è¶³è¡¥ç©ºï¼Œç”¨ .atï¼‰
        for i, col in enumerate(comp_cols):
            df_hit.at[ridx, col] = cleaned[i] if i < len(cleaned) else ""


    # === â‘£ ç»§ç»­åŸæµç¨‹å†™ result.csvï¼ˆä¸‹æ–¹åŸä»£ç ä¿æŒä¸å˜ï¼‰ ===

    # ---- ç»„è£…ç²¾ç®€ç‰ˆ result.csv ----
    meta_cols = ["Tier_1", "Tier_2", "Filename",
                 "Title", "Publisher", "Sentence",
                 "Hit_Count", "Matched_Keywords"]

    df_final = (df_hit[meta_cols +
                [c for c in df_hit.columns if c.startswith("company_")]]
                .fillna(""))
    df_final = dedup_company_cols(df_final)

    df_final.to_csv(BASE_DIR / "result.csv",
                    index=False, encoding="utf-8-sig")
    print(f"   Â· result.csv å·²å†™ / ä¿å­˜ï¼Œå…± {len(df_final)} æ¡è®°å½• / è¡Œ")

       # ---- ç”Ÿæˆ mapping_todo.csv ----
        # ---- ç”Ÿæˆ mapping_todo.csv ----
    # 1) ä¸ºäº†èƒ½æŸ¥åˆ° canonical çš„ idï¼Œå…ˆåšä¸€ä¸ª nameâ†’id çš„å­—å…¸
    canon_name2id = {row.canonical_name: row.id for row in df_canon.itertuples()}

    todo_rows: List[Dict] = []
    for _, row in df_final.iterrows():
        for alias in (
            row[c].strip()
            for c in df_final.columns if c.startswith("company_")
            if row[c].strip()
        ):
            # å·²åœ¨ä¸‰å¼ è¡¨é‡Œå‡ºç°è¿‡çš„ alias ä¸å†è¿›å…¥ todo
            if alias in ban_set or alias in alias_map or alias in canon_set:
                continue

            # ---------- è®¡ç®— Advice å’Œ Adviced_ID ----------
            if canon_set:
                # å¿…é¡»ä¼ å…¥ query + choices ä¸¤ä¸ªå‚æ•°
                advice, score, _ = process.extractOne(
                    alias,                    # query
                    list(canon_set),          # choices
                    scorer=fuzz.WRatio
                )
                if score < 85:               # ç›¸ä¼¼åº¦é˜ˆå€¼
                    advice = ""
            else:
                advice = ""

            adviced_id = canon_name2id.get(advice, "")

            # ---------- å†™å…¥ todo_rows ----------
            todo_rows.append({
                "Sentence":       row["Sentence"],
                "Alias":          alias,
                "Bad_Rate":       calc_bad_rate(alias),
                "Advice":         advice,
                "Adviced_ID":     adviced_id,
                "Canonical_Name": "",
                "Std_Result":     ""
            })

    # â‘  ç»„è£… DataFrame
    
    for r in todo_rows:
        r["Alias_lower"] = r["Alias"].lower()
    todo_df = (pd.DataFrame(todo_rows)
            .drop_duplicates("Alias_lower")
            .drop(columns="Alias_lower"))

    # â‘¡ åˆ†ç»„æ’åºï¼š0=High(â‰¥60) â†’ 1=Mid(30-59) â†’ 2=Low(<30)
    todo_df["__grp"] = todo_df["Bad_Rate"].apply(
        lambda x: 0 if x >= 60 else (1 if x >= 30 else 2)
    )
    todo_df = (todo_df
               .sort_values(["__grp", "Sentence"], ascending=[True, True])
               .drop(columns="__grp"))

    # â‘¢ å›ºå®šåˆ—é¡ºåºï¼ˆå¯é€‰ï¼‰
    todo_df = todo_df[[
        "Sentence", "Alias", "Bad_Rate",
        "Advice", "Adviced_ID",          # â† æ–°å¢
        "Canonical_Name", "Std_Result"
    ]]

    # â‘£ æ˜¾ç¤ºæˆç™¾åˆ†æ¯”åå†™æ–‡ä»¶ï¼ˆæ’åºå·²å®Œæˆï¼Œå®‰å…¨ï¼‰
    todo_df["Bad_Rate"] = todo_df["Bad_Rate"].astype(int).astype(str) + "%"

    todo_df.to_csv(BASE_DIR / "mapping_todo.csv",
                   index=False, encoding="utf-8-sig")
    print(f" mapping_todo.csv ç”Ÿæˆ / ä½œæˆ {len(todo_df)} æ¡è®°å½• / è¡Œ")
    print("âœ” Step-2 å®Œæˆ / å®Œäº†ï¼Œè¯·ç¼–è¾‘ mapping_todo.csv åè¿è¡Œ Step-3 / mapping_todo.csv ã‚’ç·¨é›†ã—ã¦ã‹ã‚‰ Step-3 ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„\n")
    print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“„  mapping_todo.csv ç®€æ˜“å¡«å†™æŒ‡å— / mapping_todo.csv ç°¡æ˜“å…¥åŠ›ã‚¬ã‚¤ãƒ‰ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1) ç©ºç™½ â†’ è·³è¿‡ / ã‚¹ã‚­ãƒƒãƒ—                                   â”‚
â”‚ 2) 0  â†’ åŠ å…¥ ban_list / ban_list ã«ç™»éŒ²                     â”‚
â”‚ 3) æ•°å­— n â†’ è§†ä¸º canonical_id = n / æ•°å­— n ã¯ ID ã¨ã—ã¦å‡¦ç† â”‚
â”‚ 4) å…¶ä»–æ–‡æœ¬ â†’ æ–°æˆ–å·²æœ‰æ ‡å‡†å / ãã‚Œä»¥å¤–ã®æ–‡å­—åˆ— = æ¨™æº–å     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ä¿å­˜åè¿è¡Œ Step-3 / ä¿å­˜ã—ã¦ Step-3 ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„        â”‚
â”‚      â€¢ è¾“å…¥ 2  â†’ è¿è¡Œ Step-3ï¼ˆå†™å…¥æ•°æ®åº“ï¼‰                    â”‚
â”‚      â€¢ è¾“å…¥ e  â†’ é€€å‡ºç¨‹åº                                     â”‚
â”‚                                                              â”‚
â”‚  â€» è‹±èªã‚¬ã‚¤ãƒ‰                                                â”‚
â”‚      â€¢ 2 ã‚’å…¥åŠ› â†’ Step-3 å®Ÿè¡Œ                                 â”‚
â”‚      â€¢ e ã‚’å…¥åŠ› â†’ çµ‚äº†       
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# ================ Step-3 ==============

def step3(mysql_url: str):
    """
    Step-3 æ ‡å‡†åŒ– + å†™åº“ï¼ˆä¸æ—§ NA_step3_standardize.py ç­‰ä»·ï¼‰
    - Canonical_Name == ''  â†’ Std_Result = 'No input'
    - Canonical_Name == '0' â†’ å†™ ban_list,  Std_Result = 'Banned'
    - å…¶å®ƒ:
        â€¢ è‹¥å·²å­˜åœ¨ alias â†’ Std_Result = 'Exists'
        â€¢ å¦åˆ™æ’å…¥/è¡¥å…¨ canonical & alias, Std_Result = 'Added'
    åŒæ—¶æŠŠæœ€æ–°æ˜ å°„åº”ç”¨å› result.csv
    """
    # æœ¬è½®æ‰¹æ¬¡å·ï¼šYYYYMMDD + 8ä½éšæœºæ•°
    process_id = datetime.now().strftime("%Y%m%d") + f"{random.randint(0, 99999999):08d}"
    res_f  = BASE_DIR / "result.csv"
    todo_f = BASE_DIR / "mapping_todo.csv"
    if not (res_f.exists() and todo_f.exists()):
        print("âŒ ç¼ºå°‘ result.csv æˆ– mapping_todo.csv / result.csv ã¾ãŸã¯ mapping_todo.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"); sys.exit(1)

    # è¯»å–
    df_res  = pd.read_csv(res_f,  dtype=str).fillna("")
    df_map  = pd.read_csv(todo_f, dtype=str).fillna("")
    
    if "Process_ID" not in df_map.columns:
        df_map["Process_ID"] = ""

    engine = create_engine(mysql_url)
    with engine.begin() as conn:

        # 1) æ‹‰å–ä¸‰è¡¨åˆ°å†…å­˜
        ban_set = {r[0] for r in conn.execute(text(
            "SELECT alias FROM ban_list"
        ))}
        canon_map = {r[0]: r[1] for r in conn.execute(text("SELECT id, canonical_name FROM company_canonical"))}  # idâ†’name
        canon_rev = {v: k for k, v in canon_map.items()}  # nameâ†’id
        alias_map = {r[0]: r[1] for r in conn.execute(text("""
            SELECT a.alias, c.canonical_name
            FROM company_alias a
            JOIN company_canonical c ON a.canonical_id = c.id
        """))}

        # 2) é€è¡Œå¤„ç† mapping
        for idx, row in df_map.iterrows():
            alias_raw = row["Alias"].strip()
            canon_input = row["Canonical_Name"].strip()
            
            did_write = False

            if not canon_input:                       # â€”â€” ç©ºç™½
                df_map.at[idx, "Std_Result"] = "No input"
                continue

           # === â‘  Banï¼ˆè¾“å…¥ 0ï¼‰ ===
            if canon_input == "0":
                if alias_raw not in ban_set:          # åªåœ¨ç¬¬ä¸€æ¬¡æ‰å†™åº“ï¼‹æ‰“æ‰¹æ¬¡å·
                    conn.execute(text(
                        "INSERT INTO ban_list(alias, process_id) "
                        "VALUES (:a, :pid)"
                    ), {"a": alias_raw, "pid": process_id})
                    ban_set.add(alias_raw)            # åˆ«å¿˜äº†åŒæ­¥åˆ°æœ¬åœ°é›†åˆ
                    df_map.at[idx, "Process_ID"] = f"'{process_id}"
                # å·²å­˜åœ¨å°±ä»€ä¹ˆéƒ½ä¸æ›´æ”¹æ‰¹æ¬¡å·
                df_map.at[idx, "Std_Result"] = "Banned"
                continue

            # === â‘¡ ç”¨æˆ·è¾“å…¥æ•°å­— â†’ è§†ä¸º canonical_id ===
            if canon_input.isdigit():
                cid = int(canon_input)
                if cid not in canon_map:              # id ä¸å­˜åœ¨
                    df_map.at[idx, "Std_Result"] = "Bad ID"
                    continue
                canon_name = canon_map[cid]           # â† id â†’ name
                canon_id   = cid
            else:
                canon_name = canon_input              # ç”¨æˆ·ç›´æ¥å†™çš„æ–‡æœ¬
                # è‹¥æ–‡æœ¬ä¸å­˜åœ¨äº canonical è¡¨ â†’ æ–°å»º
                if canon_name not in canon_rev:
                    res = conn.execute(text(
                        "INSERT INTO company_canonical(canonical_name, process_id) VALUES (:c, :pid)"
                    ), {"c": canon_name, "pid": process_id})
                    did_write = True
                    
                    canon_id = res.lastrowid
                    canon_rev[canon_name] = canon_id
                    df_map.at[idx, "Process_ID"] = f"'{process_id}"
                else:
                    canon_id = canon_rev[canon_name]

            # === â‘¢ å†™ aliasï¼ˆå¦‚å·²å­˜åœ¨åˆ™è·³è¿‡ï¼‰ ===
            if alias_raw in alias_map:
                df_map.at[idx, "Std_Result"] = "Exists"
                continue

            conn.execute(text(
                "INSERT IGNORE INTO company_alias(alias, canonical_id, process_id) VALUES (:a, :cid, :pid)"
            ), {"a": alias_raw, "cid": canon_id, "pid": process_id})
            did_write = True
            alias_map[alias_raw] = canon_name
            df_map.at[idx, "Std_Result"] = "Added"
            if did_write:
                df_map.at[idx, "Process_ID"]  = f"'{process_id}"

    # 3) åº”ç”¨æœ€æ–°æ˜ å°„åˆ° result.csv
    for col in [c for c in df_res.columns if c.startswith("company_")]:
        df_res[col] = df_res[col].apply(lambda x: alias_map.get(x, x))

    df_res = dedup_company_cols(df_res)
    # df_map["Process_ID"] = "'" + process_id   # â† å‰é¢åŠ å•å¼•å·ï¼ŒExcel ä¼šå½“æ–‡æœ¬
    df_res.to_csv(res_f, index=False, encoding="utf-8-sig")
    df_map.to_csv(todo_f, index=False, encoding="utf-8-sig")

    print(f"âœ” Step-3 å®Œæˆ / å®Œäº†ï¼š{len(df_map)} æ¡æ˜ å°„ / ä»¶ã‚’å‡¦ç†ï¼Œresult.csv å·²æ›´æ–° / æ›´æ–°å®Œäº†")
    print(f"ğŸ“Œ æœ¬æ‰¹æ¬¡/ä»Šå›ã® Process ID : {process_id}")
# ================ ä¸»å…¥å£ ==============

def main():
    mysql_url = ask_mysql_url()
    try:
        create_engine(mysql_url).connect().close()
        print("âœ… æ•°æ®åº“è¿é€š / ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ / ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šå¤±æ•—: {e}"); sys.exit(1)

    choice = choose()

    if choice == "1":
        step1()
        step2(mysql_url)

        # â€”â€” æ–°å¢ï¼šè·‘å®Œ Step-2 åç­‰å¾…ç”¨æˆ·æŒ‡ä»¤ â€”â€”
        while True:
            nxt = input("ğŸ‘‰ è¾“å…¥ 2 ç»§ç»­ Step-3ï¼Œæˆ–è¾“å…¥ e é€€å‡º / 2 ã§Step-3ã‚’ç¶šè¡Œ, e ã§çµ‚äº†: ").strip().lower()
            if nxt == "2":
                step3(mysql_url)
                break
            elif nxt == "e":
                print("å·²é€€å‡º / çµ‚äº†ã—ã¾ã—ãŸã€‚")
                return
            else:
                print("æ— æ•ˆè¾“å…¥ / ç„¡åŠ¹ãªå…¥åŠ›ã§ã™ï¼Œè¯·é‡æ–°è¾“å…¥ / ã‚‚ã†ä¸€åº¦å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    else:   # choice == "2"
        step3(mysql_url)


if __name__ == "__main__":
    main()

# ä¾èµ– / Dependencies / ä¾å­˜é–¢ä¿‚ï¼špython-docx, pandas, openpyxl, unicodedata, tqdm, spacy, fuzzywuzzy, python-Levenshtein
# å®‰è£… / Install / ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼špip install python-docx pandas openpyxl tqdm spacy fuzzywuzzy python-Levenshtein
# æ¨¡å‹ä¸‹è½½ / Model Download / ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼špython -m spacy download en_core_web_sm
# åŸä½œè€…ï¼šæ¨å¤©ä¹@å…³è¥¿å¤§å­¦ / Author: Shiame Yeung@Kansai University / ä½œæˆè€…ï¼šæ¥Šã€€å¤©æ¥½@é–¢è¥¿å¤§å­¦
# ç‰ˆæœ¬ / Version / ãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼š2025.06.05


import os
import re
import unicodedata
from docx import Document
import pandas as pd
from tqdm import tqdm

# ------------------ ç”¨æˆ·å¯é…ç½® ------------------
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = SCRIPT_DIR  # æ ¹ç›®å½•å°±æ˜¯è„šæœ¬æ‰€åœ¨ä½ç½®
OUTPUT_CSV = os.path.join(SCRIPT_DIR, 'keyword_hit.csv')  # è¾“å‡ºä¸º CSVï¼Œä¸ Step2/Step3 ä¿æŒä¸€è‡´
PUB2COUNTRY_CSV = os.path.join(SCRIPT_DIR, 'Publisher to Country List.csv')
# -----------------------------------------------

# ------------------ 1. è¯»å– Publisher â†’ Country æ˜ å°„ ------------------
try:
    pub2country_df = pd.read_csv(PUB2COUNTRY_CSV, dtype=str, encoding='utf-8-sig')
    pub2country_map = {
        pub.strip(): country.strip()
        for pub, country in zip(pub2country_df['Publisher'], pub2country_df['Country'])
        if isinstance(pub, str)
    }
except Exception as e:
    print(f"â— æ— æ³•è¯»å– Publisher to Country List.csv: {e}")
    pub2country_map = {}
# -------------------------------------------------------------------

# ------------------ 2. å®šä¹‰å…³é”®è¯æ ¹åˆ—è¡¨ ------------------
KEYWORD_ROOTS = [
    'partner','alliance','collaborat','cooper','cooperat','join','merger','acquisiti',
    'outsourc','invest','licens','integrat','coordinat','synergiz','associat',
    'confedera','federa','union','unit','amalgamat','conglomerat','combin',
    'buyout','companion','concur','concert','comply','complement','assist',
    'takeover','accession','procure','suppl','conjoint','support','adjust',
    'adjunct','patronag','subsid','affiliat','endors'
]
# --------------------------------------------------------

def clean_text(text: str) -> str:
    """
    å»é™¤ Unicode æ§åˆ¶å­—ç¬¦ï¼ˆC ç±»åˆ«ï¼‰å¹¶ä¿ç•™æ¢è¡Œã€åˆ¶è¡¨ç¬¦
    """
    return ''.join(c for c in text if unicodedata.category(c)[0] != 'C' or c in ('\n','\t'))

def _normalize(text: str) -> str:
    """
    å½’ä¸€åŒ–å¤„ç†ï¼š
      1. è½¬å°å†™
      2. å»æ‰ä¸¤ç«¯ç©ºç™½
      3. åˆå¹¶è¿ç»­ç©ºç™½ä¸ºå•ä¸ªç©ºæ ¼
      4. å»æ‰é¦–å°¾çš„ç ´æŠ˜å·ã€å†’å·ã€å¼•å·
      5. å»æ‰å¸¸è§æ ‡ç‚¹ç¬¦å·ï¼ˆé€—å·ã€å¥å·ã€åˆ†å·ã€æ–œæ ã€æ‹¬å·ç­‰ï¼‰
    """
    if not text:
        return ''
    t = text.lower().strip()
    t = re.sub(r'\s+', ' ', t)
    t = re.sub(r'^[\-:\"\']+', '', t)
    t = re.sub(r'[\-:\"\']+$', '', t)
    t = re.sub(r'[,.;/()]+', '', t)
    t = re.sub(r'\s+', ' ', t)
    return t.strip()

# ------------------ 3. ä»ç´¢å¼•åŒºæå–æ‰€æœ‰æ ‡é¢˜ ------------------
def extract_index_titles(paragraphs):
    """
    è§£ææ–‡æ¡£å¼€å¤´çš„ç´¢å¼•ï¼Œæ‰¾åˆ° Documents(n) æˆ– Document(n) è¡Œï¼Œ
    ç„¶ååœ¨å…¶åè¿ç»­æŠ“å–æ ¼å¼ä¸º "æ•°å­—. ç©ºæ ¼ + æ ‡é¢˜" çš„è¡Œã€‚
    å¯¹æŠ“åˆ°çš„æ ‡é¢˜åšå½’ä¸€åŒ–å¹¶å»é‡ï¼Œè¿”å› [(num, title_raw, title_norm), ...] åˆ—è¡¨ã€‚
    """
    paras_text = [p.text.strip() for p in paragraphs]
    titles_list = []

    # åŒ¹é… "Document(n)" æˆ– "Documents(n)"
    doc_pattern = re.compile(r'Documents?\s*\(\s*(\d+)\s*\)', re.IGNORECASE)
    doc_count = None
    start_idx = None
    for idx, txt in enumerate(paras_text):
        m = doc_pattern.match(txt)
        if m:
            try:
                doc_count = int(m.group(1))
            except:
                pass
            start_idx = idx + 1
            break

    if doc_count is None or start_idx is None:
        return []

    # ä» start_idx å¼€å§‹ï¼ŒåŒ¹é… "^(\d+)\.\s+(.*)$"
    pattern = re.compile(r'^(\d+)\.\s+(.*)$')
    seen_norm = set()
    for j in range(start_idx, len(paras_text)):
        txt = paras_text[j]
        if not txt:
            continue
        m2 = pattern.match(txt)
        if m2:
            num = int(m2.group(1))
            title_raw = m2.group(2).strip()
            title_norm = _normalize(title_raw)
            if title_norm in seen_norm:
                continue
            seen_norm.add(title_norm)
            titles_list.append((num, title_raw, title_norm))
            if len(titles_list) >= doc_count:
                break

    return sorted(titles_list, key=lambda x: x[0])

# ------------------ 4. æå–æ­£æ–‡å¥å­ï¼ˆåŒ…å« fallback é€»è¾‘ï¼‰ ------------------
def extract_sentences_by_titles(filepath):
    """
    å¯¹å•ä¸ª .docx æ–‡ä»¶ï¼Œå…ˆæå–ç´¢å¼•åŒºæ ‡é¢˜ï¼Œå†é€ç¯‡å®šä½æ­£æ–‡ï¼š
      1. å¦‚æœèƒ½æŠ“åˆ° index_titlesï¼Œåˆ™ç”¨å½’ä¸€åŒ–åŒ¹é…ç´¢å¼•æ ‡é¢˜åˆ°æ­£æ–‡åŒºï¼Œ
         æŒ‰é¡ºåºæ‰¾åˆ° Publisherã€Bodyã€Classification/Notes åŒºé—´å¹¶æå–å¥å­ï¼Œ
         ä¿å­˜ Title/Publisher/Countryã€‚
      2. å¦‚æœ index_titles ä¸ºç©ºï¼Œåˆ™å›é€€åˆ°æ—§é€»è¾‘ï¼šæå–æ‰€æœ‰ Bodyâ†’(Notes/Classification) åŒºå¯¹ï¼Œ
         å¥å­è®°å½•çš„ Title/Publisher/Country å‡ç½®ä¸ºç©ºã€‚
    è¿”å›ä¸€ä¸ª records åˆ—è¡¨ï¼Œæ¯æ¡ä¸º dictï¼š
      { 'Title','Publisher','Country','Sentence','Hit_Count','Matched_Keywords' }
    """
    records = []
    try:
        doc = Document(filepath)
    except Exception as e:
        print(f"â— æ— æ³•è¯»å–æ–‡ä»¶ {filepath}: {e}")
        return []

    paras = doc.paragraphs
    paras_text = [p.text.strip() for p in paras]
    paras_norm = [_normalize(t) for t in paras_text]

    # 4.1 æå–ç´¢å¼•æ ‡é¢˜
    index_titles = extract_index_titles(paras)

    # 4.2 å¦‚æœæ²¡æœ‰ç´¢å¼•æ ‡é¢˜ï¼Œåˆ™è¿›å…¥å›é€€é€»è¾‘ï¼šæå–æ‰€æœ‰ Bodyâ†’Notes/Classification å¯¹
    if not index_titles:
        # æ‰¾åˆ°æ‰€æœ‰ (body_start, body_end) å¯¹
        pairs = []
        for idx, txt in enumerate(paras_text):
            if txt == 'Body':
                for k in range(idx + 1, len(paras_text)):
                    if paras_text[k] in ('Notes', 'Classification'):
                        pairs.append((idx + 1, k))
                        break
        # å¦‚æœæ²¡æœ‰ä»»ä½• Bodyâ†’Notes/Classification åŒºé—´ï¼Œç›´æ¥è¿”å›ç©º
        if not pairs:
            return []
        for (bs, be) in pairs:
            full_text = []
            for k in range(bs, be):
                line = paras_text[k]
                if not line:
                    continue
                cleaned = clean_text(line)
                if cleaned:
                    full_text.append(cleaned)
            article_text = ' '.join(full_text)
            sentences = [s.strip() for s in re.split(r'\.\s*', article_text) if len(s.strip()) >= 5]
            for sent in sentences:
                hits = [root for root in KEYWORD_ROOTS if root in sent.lower()]
                records.append({
                    'Title': '',
                    'Publisher': '',
                    'Country': '',
                    'Sentence': sent,
                    'Hit_Count': len(hits),
                    'Matched_Keywords': '; '.join(hits)
                })
        return records

    # 4.3 å¦‚æœæœ‰ç´¢å¼•æ ‡é¢˜ï¼Œåˆ™é€æ¡åŒ¹é…æ­£æ–‡åŒº
    for num, title_raw, title_norm in index_titles:
        match_idx = None
        for idx, norm_txt in enumerate(paras_norm):
            if norm_txt == title_norm:
                match_idx = idx
                break
        if match_idx is None:
            continue

        # ä¸‹ä¸€è¡Œæ˜¯ Publisher
        pub_idx = match_idx + 1
        if pub_idx >= len(paras_text):
            continue
        publisher = paras_text[pub_idx].strip()
        country = pub2country_map.get(publisher, '')

        # ä» match_idx+2 å¼€å§‹æ‰¾ Body
        body_start = None
        for k in range(match_idx + 2, len(paras_text)):
            if paras_text[k] == 'Body':
                body_start = k + 1
                break
        if body_start is None:
            continue

        # ä» body_start å¼€å§‹æ‰¾ç¬¬ä¸€å¤„ Notes æˆ– Classification
        body_end = None
        for k in range(body_start, len(paras_text)):
            if paras_text[k] in ('Classification', 'Notes'):
                body_end = k
                break
        if body_end is None:
            continue

        # æå–æ­£æ–‡ [body_start, body_end)
        full_text = []
        for k in range(body_start, body_end):
            line = paras_text[k]
            if not line:
                continue
            cleaned = clean_text(line)
            if cleaned:
                full_text.append(cleaned)
        article_text = ' '.join(full_text)
        sentences = [s.strip() for s in re.split(r'\.\s*', article_text) if len(s.strip()) >= 5]
        for sent in sentences:
            hits = [root for root in KEYWORD_ROOTS if root in sent.lower()]
            records.append({
                'Title': title_raw,
                'Publisher': publisher,
                'Country': country,
                'Sentence': sent,
                'Hit_Count': len(hits),
                'Matched_Keywords': '; '.join(hits)
            })

    return records

# ------------------ 5. ä¸»æµç¨‹ï¼šéå†æ‰€æœ‰ docx å¹¶æ±‡æ€» ------------------
records_all = []
docx_files = []

for root, dirs, files in os.walk(ROOT_DIR):
    for fname in files:
        if not fname.endswith('.docx') or fname.startswith('~$'):
            continue
        path = os.path.join(root, fname)
        rel = os.path.relpath(path, ROOT_DIR).split(os.sep)
        if len(rel) >= 3:
            tier1, tier2, filename = rel[0], rel[1], rel[2]
        elif len(rel) == 2:
            tier1, tier2, filename = rel[0], '', rel[1]
        else:
            tier1, tier2, filename = '', '', rel[0]
        docx_files.append((path, tier1, tier2, filename))

for path, tier1, tier2, filename in tqdm(docx_files, desc="ğŸ“„ å¤„ç† Word æ–‡ä»¶", ncols=100):
    recs = extract_sentences_by_titles(path)
    for r in recs:
        r['Tier_1'] = tier1
        r['Tier_2'] = tier2
        r['Filename'] = filename
        records_all.append(r)

# 5.1 æ„å»º DataFrame å¹¶æ’åºã€è¾“å‡º CSV
if records_all:
    df = pd.DataFrame(records_all, columns=[
        'Tier_1', 'Tier_2', 'Filename',
        'Title', 'Publisher', 'Country',
        'Sentence', 'Hit_Count', 'Matched_Keywords'
    ])
    # åŸæœ‰æ’åºï¼šå…ˆæŒ‰æ˜¯å¦å‘½ä¸­ï¼ˆHit_Count>0ï¼‰ï¼Œå†æŒ‰ Tier_1/Tier_2/Filename
    df['Hit_Flag'] = df['Hit_Count'].apply(lambda x: 1 if x > 0 else 0)
    df = df.sort_values(['Hit_Flag', 'Tier_1', 'Tier_2', 'Filename'],
                        ascending=[False, True, True, True]).reset_index(drop=True)
    df = df.drop(columns=['Hit_Flag'])

    df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8-sig')
    print(f"âœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶ï¼š{OUTPUT_CSV}")
else:
    print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„å¥å­æˆ– docx æ–‡ä»¶ä¸ºç©ºã€‚")
